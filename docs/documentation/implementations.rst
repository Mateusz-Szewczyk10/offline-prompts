==========
Problem Formulation and Implementations
==========

Problem Formulation
----------
We start by formulating prompt optimization as contextual bandits with *auxiliary outputs*. Let :math:`u \in \mathcal{U} \subseteq \mathbb{R}^{d_u}` be a :math:`d_u`-dimensional user feature vector (e.g., demographic profile or user id), sampled from an unknown distribution :math:`p(u)`. 
Let :math:`q \in \mathcal{Q} \subseteq \mathbb{R}^{d_q}` be a *query* (e.g., query to a frozen LLM), sampled from a conditional distribution :math:`p(q | u)`. Let :math:`a \in \mathcal{A}` be a (discrete) *prompt*, where each prompt is associated with some vectorial embedding, :math:`e_a \in \mathbb{R}^{d_e}`, where :math:`d_e` is the dimension of the embeddings. 
The prompt is used to generate a sentence via a frozen LLM. 

This process can be formulated as a procedure of sampling sentence :math:`s \in \mathcal{S}` as an auxiliary output from the stochastic output distribution of the LLM: :math:`p_{\text{LLM}}(s | q, a)`. 
A user will respond to the output sentence and provide some reward :math:`r \in \mathbb{R}` (e.g., click or purchase), where :math:`r` follows :math:`p(r | u, q, s)`. 
Let :math:`\pi \in \Pi` be a *prompt policy* where :math:`\pi(a | u, q)` is the probability of choosing *prompt* :math:`a` for *context* :math:`x := (u, q) \in \mathcal{X}`.
The goal of policy learning is to optimize the prompt policy to maximize the expected reward, defined as

.. math::

    V(\pi) 
    := \mathbb{E}_{p(u)p(q|u) \pi(a|u,q) p_{\text{LLM}}(s|q, a)p(r | u,q,s)}[r] 
    = \mathbb{E}_{p(x)\pi(a | x) p(r, s | x, a)}[r].

where :math:`p(x) = p(u, q) = p(u)p(q|u)`, :math:`\pi(a|x) = \pi(a|u,q)`, and :math:`p(r,s|x,a) = p_{\text{LLM}}(s|q, a)p(r|u,q,s)`.

Now, we consider the case where we naturally collect logged data through the past operation of the systems as follows.

.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/personalized_sentence_generation.png
   :text-align: center

   User interaction and data collection process from prompt-guided language generation. Examples are generated by ChatGPT-3.5 :cite:`brown2020language`.

.. raw:: html

    <div class="white-space-20px"></div>

The key point is that, for each coming user, a policy chooses which prompt to use to generate sentences with a frozen LLM. Therefore, each user observes only the sentence generated by the chosen prompt and provides the reward for the corresponding sentence. 
This means that we cannot observe rewards for the sentences generated by prompts *not* chosen by the logging policy.

Off-policy evaluation and learning (OPE/L) aims to evaluate or learn a new policy using the logged data :math:`\mathcal{D}` collected by a *logging* policy :math:`\pi_0` as follows.

.. math::

    \mathcal{D} 
    := \{ x_i, a_i, s_i, r_i \}_{i=1}^n \, \sim \prod_{i=1}^n p(x) \pi_0(a | x) p_{\text{LLM}}(s | x, a) p(r | x, s),

where :math:`n` is the data size and :math:`i` is its index.
The logged data informs us whether the prompt (:math:`a_i`) results in a high reward or not (:math:`r_i`) for a particular context (:math:`x_i`). However, a difficult aspect of using the logged data is that the reward observation is *partial*,
i.e., it is observed only for the prompt chosen by the logging policy (:math:`\pi_0`) but not for all the other actions. This can be particularly challenging when training a new policy :math:`\pi` on the logged data, as :math:`\pi` may choose actions that are not chosen by :math:`\pi_0`. 
Thus, we need to address such *counterfactuals* and *distribution shift* between the logging and learning policies when using logged data for a reliable policy optimization.

In the following sections, we describe the OPE/L methods implemented in *OfflinePrompts* for optimizing a prompt policy for a successful (and personalized) language generation.

OPL methods
----------
In this section, we introduce the following OPL methods:

* Regression :cite:``
* Importance Sampling (IS) :cite:``
* Doubly Robust (DR) :cite:``
* POTEC :cite:``
* Direct Sentence (DSO, ours) :cite:``

Policy gradient (PG)
^^^^^^^^^^
In OPL, we often aim to unbiasedly estimate the following PG (estimation target) with a small variance, using logged data.

.. math::

    \nabla_{\theta} V(\pi_{\theta}) = \mathbb{E}_{p(x) \pi_{\theta}(a | x) p(r | x, a)} [\nabla_{\theta} \log \pi_{\theta}(a | x) r ].

In the following, we denote :math:`z \sim p(z)` as sampling a *single* random variable :math:`z` from the probability distribution :math:`p(z)`.


Regression :cite:`konda1999actor`. 
^^^^^^^^^^
A typical way of using logged data is to train a reward predictor :math:`\hat{q}` :cite:`stiennon2020learning, jaques2017sequence, snell2022offline`, and then use the predicted reward to estimate the policy gradient (PG).

.. math::

    \nabla_{\theta} V(\pi_{\theta}) \approx \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{a \sim \pi_{\theta}(a | x_i)}\left[ \nabla_{\theta} \log \pi_{\theta}(a | x_i) \hat{q}(x_i, a) \right],

where :math:`\hat{q}(x, a) \approx \mathbb{E}[r|x,a]`.
Oftentimes, an accurate regression for OPL is difficult under complex relations between prompts and rewards. 
This is because the reward observation is partial and covariate shifts arise between the logging policy (:math:`\pi_0`) and the target policy (:math:`\pi_{\theta}`). 
If the learned regression model :math:`\hat{q}` is inaccurate, the estimated PG can have a high bias :cite:`swaminathan2015batch`.

Importance sampling (IS) :cite:`swaminathan2015batch`
^^^^^^^^^^
Instead of using potentially inaccurate regression, IS corrects the distribution shift between :math:`\pi_0` and :math:`\pi_{\theta}` by reweighing the observations:

.. math::

    \nabla_{\theta} V(\pi_{\theta}) \approx \frac{1}{n} \sum_{i=1}^n \frac{\pi_{\theta}(a_i | x_i)}{\pi_0(a_i | x_i)} \nabla_{\theta} \log \pi_{\theta}(a_i | x_i) r_i.

IS is unbiased under the \textit{action support} condition, i.e., :math:`\forall (x, a) \in \mathcal{X} \times \mathcal{A}, \, \pi_{\theta}(a | x) > 0 \implies \pi_0(a | x) > 0`. 
However, IS produces considerable bias due to the violation of the condition called deficient support :cite:`sachdeva2020off` and extremely high variance due to large importance weight :cite:`saito2023off, saito2024potec,sachdeva2024off`, which are likely when the action space is large. 

Doubly Robust (DR) :cite:`dudik2011doubly`
^^^^^^^^^^
DR is a hybrid approach, which effectively combines the regression and IS to exploit the benefits of the two.

.. math::

    \nabla_{\theta} V(\pi_{\theta}) 
    &\approx \frac{1}{n} \sum_{i=1}^n \frac{\pi_{\theta}(a_i | x_i)}{\pi_0(a_i | x_i)} \nabla_{\theta} \log \pi_{\theta}(a_i | x_i) (r_i - \hat{q}(x_i, a_i)) \\
    & + \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{a \sim \pi_{\theta}(a|x_i)}[\nabla_{\theta} \log \pi_{\theta}(a_i | x_i) \hat{q}(x_i, a)].

By using the regressed reward as a control variate, DR often reduces the variance of IS, while remaining unbiased under the same condition as IS. 
However, when the regression is inaccurate, the variance reduction is limited and DR often suffers from high variance when the action space is large :cite:`saito2022off`. 
The shortcoming of IS and DR is that these typical methods treat each prompt independently and discard the similarity among prompts and sentences.

POTEC :cite:`saito2024potec`
^^^^^^^^^^
To deal with the variance issue of DR, POTEC considers the clustering in the action space and decomposes the policy into two stages as follows.

.. math::

    \pi_{\theta}(a|x) = \sum_{c \in \mathcal{C}} \pi_{\theta}^{\text{1st}}(c|x) \pi^{\text{2nd}}(a|x,c),

where :math:`c` indicates the cluster of the action :math:`a`, which can be learned by applying an off-the-shelf clustering method to action (i.e., prompt) embeddings. 
Using this decomposition, POTEC chooses clusters via a DR-style approach as follows, and chooses actions within a cluster via regression.

.. math::

    \nabla_{\theta} V(\pi_{\theta}) 
    &\approx \frac{1}{n} \sum_{i=1}^n \frac{\pi_{\theta}^{\text{1st}}(c(a_i)|x_i)}{\pi_0^{\text{1st}}(c(a_i)|x_i)} \nabla_{\theta} \log \pi_{\theta}^{\text{1st}}(c(a_i) | x_i) (r_i - \hat{q}(x_i, a_i)) \\
    &+ \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{a \sim \pi_{\theta}(a|x_i)}[\nabla_{\theta} \log \pi_{\theta}^{\text{1st}}(c(a_i) | x_i) \hat{q}(x_i, a)],

where :math:`\pi_0^{\text{1st}}(c(a)|x) = \sum_{a' \in \mathcal{A}, c(a')=c(a)} \pi_0(a|x)`. The second-stage policy greedily chooses action as :math:`\pi^{\text{2nd}}(a | x, c) = \mathbb{I} \{ \hat{q}(x, a) = {\arg\max}_{a' \in \mathcal{A}, c(a') = c(a)} \hat{q}(x,a') \}`.
By applying IS on the clustered action space, POTEC reduces the variance of naive IS, leveraging the similarity among prompts. 
POTEC is also able to convert regression to a pair-wise regression within a cluster. 
However, especially when the relation between actions and rewards is complex, a good clustering is often hard to identify, and POTEC cannot take the rich information about generated sentences into account.

Direct Sentence (DSO) :cite:`kiyohara2024prompt`
^^^^^^^^^^
To take the similarity among generated sentences into account, DSO calculates the policy gradient in the (marginalized) sentence space as follows.

.. math::

    \nabla_{\theta} V(\pi_{\theta}) 
    \approx \frac{1}{n} \sum_{i=1}^n \underbrace{\frac{\pi_{\theta}(\phi(s_i)|x_i)}{\pi_0(\phi(s_i)|x_i)}}_{:=w(\phi(s_i), x_i)} \nabla_{\theta} \log \pi_{\theta}(\phi(s_i)|x_i) \, r_i,

where :math:`\phi(s) \in \Phi(\mathcal{S})` is the kernel-based neighbors of sentence $s$, whose probability density is defined as :math:`\mathbb{P}(\phi(s)|\cdot):= \int_{s' \in \mathcal{S}} K(s', s; \, x, \tau) \mathbb{P}(s'|\cdot) ds', \, \forall \mathbb{P}`. 
:math:`K(\cdot, \cdot)` is a kernel function, which must satisfy :math:`\int_{s' \in \mathcal{S}} K(s', s; x, \tau) = 1` and :math:`\tau` is a bandwidth hyperparameter. 
Then, using the *``re-sampling''* technique to estimate the weighted score function (i.e., :math:`w(\phi(s), x) \nabla_{\theta} \log \pi_{\theta}(\phi(s)|x)`), DSO estimates the policy gradient as follows:

.. math::

    \nabla_{\theta} V(\pi_{\theta}) 
    \approx \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{(a, s') \sim \pi_{\theta}(a|x_i)p_{\text{LLM}}(s'|x_i,a)} \biggl[ \frac{K(s_i, s'; \, x_i, \tau) \nabla_{\theta} \log \pi_{\theta}(a | x_i)}{\pi_{0}(\phi(s_i)|x_i)} \biggr] \, r_i.

This means that DSO performs soft rejection sampling on the data :math:`(a, s')` augmented by :math:`\pi_{\theta}`, while correcting the bias in the logged data by applying the inverse propensity of :math:`\pi_0` in the marginalized sentence space.
The logging marginal density :math:`\pi_0(\phi(s)|x) = \mathbb{E}_{\pi_0(s'|x)}[K(s, s'; \, x, \tau)]` can be estimated by either the Monte-Carlo estimation or function approximation with the following loss.

.. math::

    \ell(f_{\psi}) \approx \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{(s,s') \sim \pi_0(s|x_i)\pi_0(s'|x_i)}[ (f_{\psi}(x_i,s) - K(s,s' ; \, x_i,\tau))^2].

DSO reduces the variance of action-based IS by avoiding large importance weights and implicitly augmenting the data using similarity among sentences. 
DSO also keeps the bias small by using similarity-based weighting on neighbors via kernels. 


OPE estimators
----------
In this section, we also introduce the following OPE estimators corresponding to the aforementioned OPL methods:

* Direct Method (DM) :cite:``
* Importance Sampling (IS) :cite:``
* Doubly Robust (DR) :cite:``
* OffCEM :cite:``
* Kernel IS (ours) :cite:``

Direct Method (DM) :cite:`beygelzimer2009offset`
^^^^^^^^^^
To enable the offline evaluation of a new policy, one can consider applying off-policy evaluation (OPE). 
DM is a prevalent method in OPE, which estimates the policy value using the regressed reward.

.. math::

    V(\pi_{\theta}) \approx \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{a \sim \pi_{\theta}(a | x_i)}\left[ \hat{q}(x_i, a) \right].

Similar to the REINFORCE policy gradient, the benefit of this estimator is not incurring high variance. However, when the regression is inaccurate due to covariate shifts, the estimation has a serious bias.

Inverse Propensity Scoring (IPS) :citep:`strehl2010learning`
^^^^^^^^^^
IPS applies importance sampling to correct the reward observation probability as follows.

.. math::

    V(\pi_{\theta}) \approx \frac{1}{n} \sum_{i=1}^n \frac{\pi_{\theta}(a_i | x_i)}{\pi_0(a_i | x_i)} r_i.

As seen in the discussion of OPL, IPS suffers from high variance and deficient support, especially when the number of candidate actions (i.e., prompts) is large :cite:`sachdeva2020off, saito2022off, saito2023off`.

Doubly Robust (DR) :cite:`dudik2011doubly`
^^^^^^^^^^
DR uses the regressed reward as a control variate when applying importance weights as follows.

.. math::

    V(\pi_{\theta}) 
    \approx \frac{1}{n} \sum_{i=1}^n \left\{ \frac{\pi_{\theta}(a_i | x_i)}{\pi_0(a_i | x_i)}  (r_i - \hat{q}(x_i, a_i)) + \mathbb{E}_{a \sim \pi_{\theta}(a|x_i)}[\hat{q}(x_i, a)] \right\}.

DR reduces the variance of IPS when the regression is reasonably accurate (i.e., $|q(x, a) - \hat{q}(x, a)| \leq |q(x, a)|$). Also, DR is \textit{``doubly'' robust} in that it is unbiased either when the regression is accurate for all $(x, a) \in \mathcal{X} \times \mathcal{A}$ or $\pi_0$ is accessible and do not have deficient support. However, this condition is hard to satisfy when the action space is large, and empirically, DR is known to perform similarly to IS in large action settings~\citep{saito2022off, saito2023off}.

OffCEM :cite:`saito2023off`
^^^^^^^^^^
OffCEM aggregates the reward observation among similar prompts using the clusters in the action space as follows.

.. math::

    V(\pi_{\theta}) 
    \approx \frac{1}{n} \sum_{i=1}^n \left \{ \frac{\pi_{\theta}(c(a_i)|x_i)}{\pi_0(c(a_i)|x_i)} (r_i - \hat{q}(x_i, a_i)) 
    + \mathbb{E}_{a \sim \pi_{\theta}(a|x_i)}[\hat{q}(x_i, a)] \right \},

where :math:`\pi(c(a)|x) = \sum_{a' \in \mathcal{A}, c(a')=c(a)} \pi(a|x)` is the probability of choosing cluster :math:`c` under policy :math:`\pi` (note that, in contrast to POTEC, this estimator is applicable regardless :math:`\pi` and :math:`\pi_0` being two-stage policies or not). 
OffCEM reduces the variance of DR by avoiding extreme importance weights. However, when there is a complex relation between prompts and rewards and a cluster contains prompts that produce different rewards, OffCEM may incur high bias in estimation.

Kernel IS :cite:`kiyohara2024prompt`
^^^^^^^^^^
Kernel IS applies importance sampling on the marginalized sentence space as follows.

.. math::

    V(\pi_{\theta}) 
    \approx \frac{1}{n} \sum_{i=1}^n \frac{\pi_{\theta}(\phi(s_i)|x_i)}{\pi_0(\phi(s_i)|x_i)} r_i,

where :math:`\pi(\phi(s)|x) = \mathbb{E}_{\pi(s'|x)}[K(s, s'; \, x, \tau)]` is the marginalized importance weight under the kernel function :math:`K(\cdot, \cdot)` and bandwidth hyperparameter :math:`\tau`. 
By leveraging the similarity among sentences, which more directly affects the reward than prompts, kernel IS expects a smaller bias compared to action-based IS methods, while reducing the variance by using kernels.

Online evalution and learning
----------
Finally, we also provide online methods as a potential skyline.

REINFORCE :cite:`sutton2018reinforcement, deng2022rlprompt`
^^^^^^^^^^
When the policy can interact with the environment, we estimate the policy gradient via Monte-Carlo (MC) estimation as follows.

.. math::

    \nabla_{\theta} V(\pi_{\theta}) \approx \frac{1}{m} \sum_{i=1}^m \mathbb{E}_{x_i \sim p(x), a_i \sim \pi_{\theta}(a | x_i), r_i \sim p(r|x_i,a_i)}\left[ \nabla_{\theta} \log \pi_{\theta}(a_i | x_i) r_i \right],

where we parametrize the policy as :math:`\pi_{\theta}` using some parameters :math:`\theta \in \Theta` (e.g., a neural network). 
:math:`m` is the number of batched samples used in MC. 
While REINFORCE can calculate the gradient with an adequate number of samples, the policy may produce undesirable results before the policy converges to the optimal choice. 

Online evaluation
^^^^^^^^^^
Corresponding to REINFORCE, this method, which is also refered to as online A/B tests, uses Monte-Carlo estimation:

.. math::

    V(\pi_{\theta}) \approx \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{x_i \sim p(x), a_i \sim \pi_{\theta}(a | x_i), r_i \sim p(r|x_i,a_i)}\left[ r_i \right].

The benefit of online testing is to enable accurate estimation of the policy value. 
However, when the tested policy performs poorly, online testing may harm the user experience and even cause ethical problems :cite:`gilotte2018offline`. 
Also, deploying policy online requires huge implementation costs in practice :cite:`matsushima2021deployment`. 

Comparison of the implemented OPE/L methods
----------
Finally, we provide the overview and comparison of the implemented OPE/L methods as follows.

.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/comparison.png
   :text-align: center

   Comparison of the implemented OPE/L methods

.. raw:: html

    <div class="white-space-20px"></div>

Note that, in the table, whether the regression-based and DR approaches use the similarity among prompts depends on the way the regression model is trained. 
DSO (and Kernel IS) implicitly consider the similarity among prompts by using the similarity of :math:`p_{\text{LLM}}(s|x, a)` via the re-sampling technique.

.. raw:: html

    <div class="white-space-5px"></div>

.. grid::
    :margin: 0

    .. grid-item::
        :columns: 3
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: quickstart
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                <<< Prev
                **Quickstart**

    .. grid-item::
        :columns: 6
        :margin: 0
        :padding: 0

    .. grid-item::
        :columns: 3
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: dso
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **DSO, efficient OPL**

            .. grid-item-card::
                :link: dso
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **API reference**
