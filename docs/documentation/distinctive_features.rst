:html_theme.sidebar_secondary.remove:

==========
Why OfflinePrompts?
==========

Motivation
~~~~~~~~~~

Personalizing sentence generation is crucial in interactive platforms including recommender systems, online ads, and educational apps. 
For example, consider a situation where we recommend the movie "Wall-E (2008)" with some short descriptions or slogans of the movie, as shown in the following figure. 
If the user is a sci-fi lover, the description focusing on the "sci-fi" aspect of the movie would gain more attention than that focusing on the "romance" aspect, and vice versa. Such personalization is crucial for maximizing some business metrics and users' welfare.


.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/motivate_example.png
   :text-align: center

   Motivative example of for generating personalized sentence in movie recommendation.

.. raw:: html

    <div class="white-space-20px"></div>

This package, called *OfflinePrompt*, aims to provide tools for the above personalization focusing on the prompt-policy learning from logged bandit data. Specifically, we consider the following workflow -- (i) For each coming user, a policy chooses which prompt to use to generate sentences with a frozen LLM. (ii) Each user observes only the sentence generated by the chosen prompt and provides the reward for the corresponding sentence. -- Figure xxx illustrates the interaction process. Through daily operations in the platform, a (logging) policy naturally collects user responses to the sentence generated by the logging policy. Our goal is to use such informative logged data to learn or evaluate a new policy **(Off-policy evaluation and learning; OPE/OPL)** for better personalization in the future. 

.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/personalized_sentence_generation.png
   :text-align: center

   Prompt-guided sentence personalization as a off-policy learning of contextual bandits. 

.. raw:: html

    <div class="white-space-20px"></div>

To facilitate the research and practical applications of OPL of prompt policies, OfflinePrompts contributes to the followings; 

* To implement representative OPL methods for prompt optimization. 
* To provide two benchmark environments, including synthetic and full-LLM.
* To streamline the workflow to connect OPL and sentence generation modules for smooth experimentation. 

In particular, the provided full-LLM benchmark models realistic users' responses to the presented sentence in the movie recommendation settings, by training a semi-synthetic reward simulator with the sentence-augmented [MovieLens](https://grouplens.org/datasets/movielens/) dataset. The benchmark design is a remarkable contribution to the research community, as we currently lack a realistic simulator on personalized sentence generation tasks that satisfies the following four key qualifications:

* LLMs have knowledge about items (e.g., movies) so that they can generate descriptions.
* Items have more than two aspects (e.g., sci-fi and romance) so that choosing a prompt makes the difference in expected rewards.
* The above \textit{prompt effects} can be different among individual contexts (i.e., users).
* The prompt effects are learnable from datasets, so we can learn a realistic simulator (e.g., MovieLens enables us to learn affinity between user preference and movie features).

We hope this work will be an important cornerstone to the real-world applications of OPL for prompt-guided language personalization!


Key contributions
~~~~~~~~~~

The distinctive features of our OfflinePrompts platform are summarized as follows.

* :ref:`feature_end_to_end`

* :ref:`feature_variety_ope`

* :ref:`feature_movielens`

Below, we describe each advantage one by one.

.. _feature_end_to_end:

End-to-end implementation of data collection, OPE/L, and LLM-based generation
----------

While existing prompt-tuning platforms support flexible implementations for online or model-based learning, we focus on off-policy evaluation and learning (OPE/L) from logged bandit data.
Specifically, OfflinePrompts mainly consists of the following theree modules as shown in the bottom figure and streamline an end-to-end procedure:

.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/workflow.png
   :text-align: center

   Workflow of OPL of prompt-guided language generation policy streamlined by OfflinePrompts

.. raw:: html

    <div class="white-space-20px"></div>

* Dataset module
* Policy module
* Off-Policy Learning (OPL) module

First, the *Dataset* module handles the data collection and simulation of prompt-guided language generation.
Since our Dataset module contains two settings: Semi-synthetic and Synthetic. 
The Semi-synthetic dataset provides codes to setup an environment using collaborative-filtering (CF) datasets (i.e., containing user, item, rating infos where item should be proper nouns so item description can be generated).
This module is initialized with the MovieLens dataset.
In contrast, the Synthetic dataset provides a light-weight, flexible environment based on linear vectorial representations. 
This also allows us to test the performance of OPL with various logging policies or other experimental settings in a computationally efficient way.

Next, the *Policy* module provides basic implementation of policies such as Epsilon-greedy or Softmax. This module also contains the implementation of two-stage policy
where the first-stage policy begins by choosing which cluster to select and then decides which item within the cluster to select.

Finally, the *OPL* module is our main focus. As shown in the next subsections, we implement various OPE/L methods along with online-learning implementation.
These ready-to-use baselines should allow researchers to compare their own method or practical engineers test various OPL methods on their own application scenarios.


.. _feature_variety_ope:

Variety of OPE estimators and OPL methods available
----------

OfflinePrompts implements various OPL methods to handle large and discrete prompt space.
Specifically, we implement the following OPL methods and corresponding OPE estimators.


.. raw:: html

    <div class="white-space-5px"></div>

.. seealso::

    The detailed descriptions of OPE/L methods are in :doc:`Supported Implemetation (OPE/OPS) <implementations>`.


.. raw:: html

    <div class="white-space-5px"></div>


**OPL methods**

* REINFORCE (online) :citep:``
* Actor-critic :cite:``
* Importance Sampling (IS) :cite:`precup2000eligibility`
* Doubly Robust (DR) :cite:``
* POTEC :cite:``
* DSO (our proposal) :cite:``

**Correponding OPE estimators**

* (online)
* Direct Method (DM) :cite:``
* Importance Sampling (IS) :cite:`precup2000eligibility`
* Doubly Robust (DR) :cite:``
* OffCEM :cite:``
* Kernel IS (our proposal) :cite:``

.. raw:: html

    <div class="white-space-5px"></div>


.. _feature_movielens:

Movie description personalization using MovieLens
----------
One of the most important features of OfflinePrompts is to implement semi-synthtic environemnt that can be instantiated using recommendation datasets. 
Specifically, OfflinePrompts provides implementation using MovieLens dataset to simulate the personalized sentence generation for improved user engagement.
Our library is also compatible to HuggingFace and multiple language models can be easily used.

.. card::
    :width: 75%
    :margin: auto
    :img-top: ../_static/images/dataset_module.png
    :text-align: center

    Implementation of Semi-synthetic (data-driven) and synthetic simulation environment.

.. raw:: html

    <div class="white-space-20px"></div>


**Remark**

Our implementations get inspirations from `OpenBanditPipeline (OBP) <https://zr-obp.readthedocs.io/en/latest/>`_ :cite:`saito2021open` and `SCOPE-RL <>`_ :cite:``, which streamlines OPE/L in the general contextual bandit and RL settings.
Compared to these libraries, ours specializes on the prompt-guided language generation, streamlines the implementation workflow that is compatible with `HugginFace <>`_ :cipe:, and enables data-driven simulation based on real-world recommendation datasets.

.. raw:: html

    <div class="white-space-5px"></div>

.. grid::
    :margin: 0

    .. grid-item::
        :columns: 3
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: index
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                <<< Prev
                **Documentation (Back to Top)**

    .. grid-item::
        :columns: 6
        :margin: 0
        :padding: 0

    .. grid-item::
        :columns: 3
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: implementations
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Formulations and methods**

            .. grid-item-card::
                :link: quickstart
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Quickstart**