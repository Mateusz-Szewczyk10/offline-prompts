==========
DSO for Data-Efficient OPL
==========


The issue of vanilla importance sampling (IS)
~~~~~~~~~~

As a prevalent appraoch of Off-Policy Learning (OPL), the Importance Sampling (IS)-based policy gradient estimator corrects the distribution shift between :math:`\pi_0` and :math:`\pi_{\theta}` by reweighing the observations:

.. math::

    \nabla_{\theta} V(\pi_{\theta}) \approx \frac{1}{n} \sum_{i=1}^n \frac{\pi_{\theta}(a_i | x_i)}{\pi_0(a_i | x_i)} \nabla_{\theta} \log \pi_{\theta}(a_i | x_i) r_i.

where the logged data :math:`\mathcal{D}` is collected in the form of 

.. math::

    \mathcal{D} 
    := \{ x_i, a_i, s_i, r_i \}_{i=1}^n \, \sim \prod_{i=1}^n p(x) \pi_0(a | x) p_{\text{LLM}}(s | x, a) p(r | x, s).

:math:`n` is the data size and :math:`i` is its index. :math:`x` is the (user) context, :math:`a` is the prompt chosen by the logging policy :math:`\pi_0`, :math:`s` is the sentence generated by the (frozen) LLM, and :math:`r` is the reward. 
For the detailed notations, please also refer to :ref:`implementation`.

IS is unbiased under the *action support* condition, i.e., :math:`\forall (x, a) \in \mathcal{X} \times \mathcal{A}, \, \pi_{\theta}(a | x) > 0 \implies \pi_0(a | x) > 0`. 
However, IS produces considerable bias due to the violation of the condition (deficient support) :cite:`sachdeva2020off` and extremely high variance due to large importance weight :cite:`saito2023off, saito2024potec,sachdeva2024off`, which are likely when the action space is large. 
The key shortcoming here is that the typical methods treat each prompt independently and discard the rich information about the generated sentence when estimating the policy gradient.


Direct Sentence Off-Policy Gradient (DSO)
~~~~~~~~~~

The key idea is to make the most of the information about the generated sentence by **taking the policy gradient directly in the sentence space** as follows. 

.. math::

    \nabla_{\theta} V(\pi_{\theta}) 
    = \mathbb{E}_{p(x)\pi_{\theta}(s|x)}[\nabla_{\theta} \log \pi_{\theta}(s|x) q(x,s)].

Even when we parameterize the policy in the prompt space, this is conceptually possible because we can write the sentence distribution and the score function as :math:`\pi_{\theta}(s|x) = \sum_{a \in \mathcal{A}} p_{\text{LLM}}(s|x,a) \pi_{\theta}(a|x)` and :math:`\nabla_{\theta} \log \pi_{\theta}(s|x) = \mathbb{E}_{\pi_{\theta}(a|x,s)}[\nabla_{\theta} \log \pi_{\theta}(a|x)]`, respectively. 

However, one potential concern of this approach is that we may suffer from data sparsity when estimating the gradient for each sentence :math:`s`, as sentences are high-dimensional. 
Thus, we further consider taking the gradient in the **marginalized sentence space** to enable data-efficient OPE as

.. math::

    \nabla_{\theta} V(\pi_{\theta}) 
    = \mathbb{E}_{p(x)\pi_{\theta}(\phi(s)|x)}[\nabla_{\theta} \log \pi_{\theta}(\phi(s)|x) q^{\pi_{\theta}}(x,\phi(s))],

where :math:`\phi(s) \in \Phi(\mathcal{S})` is the kernel-based neighbors of sentence :math:`s`. 
Its probability density, policy distribution, and expected reward are defined as follows.

* :math:`\mathbb{P}(\phi(s)|\cdot):= \int_{s' \in \mathcal{S}} K(s', s; \, x, \tau) \mathbb{P}(s'|\cdot) ds', \, \forall \mathbb{P}` (marginal density),
* :math:`\pi(\phi(s)|x):= \sum_{a \in \mathcal{A}} p_{\text{LLM}}(\phi(s)|x,a)\pi(a|x), \, \forall \pi` (policy marginal distribution), 
* :math:`q^{\pi}(x,\phi(s)) := \int_{s' \in \mathcal{S}} \frac{K(s,s'; \, x, \tau) \pi(s'|x)}{\pi(\phi(s)|x)} q(x,s') ds', \, \forall \pi` (expected reward).

:math:`K(\cdot)` is a kernel function, which must satisfy :math:`\int_{s' \in \mathcal{S}} K(s', s; x, \tau) = 1`,
and :math:`\tau` is a bandwidth hyperparameter that controls the magnitude of marginalization. 

The intuition behind DSO is to implicitly augment the data by taking the observations for the neighboring sentences into account, as shown in the following figure. 
Specifically, when using a smooth kernel like a Gaussian kernel, neighboring sentences are weighted proportional to :math:`K(s', s; \, x, \tau) \propto \exp(- d(s, s'))`, where :math:`d(s,s')` is the distance between two sentences (e.g., sentence embedding distance). 
In contrast, when using a piecewise constant kernel, all the sentences within a certain threshold are equally weighted, while all the others are rejected with the weight of 0. 

.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/kernel_weights.png
   :text-align: center

   Examples of the kernel weights and (soft) rejection sampling in the marginalized sentence space.

.. raw:: html

    <div class="white-space-20px"></div>


To estimate the policy gradient in the marginalized sentence space induced by kernels,
**Direct Sentence Off-policy Gradient} (DSO)** applies IS as follows.

.. math::

    \nabla_{\theta} V(\pi_{\theta}) 
    \approx \frac{1}{n} \sum_{i=1}^n \underbrace{\frac{\pi_{\theta}(\phi(s_i)|x_i)}{\pi_0(\phi(s_i)|x_i)}}_{:=w(\phi(s_i), x_i)} \nabla_{\theta} \log \pi_{\theta}(\phi(s_i)|x_i) \, r_i.

By applying IS on the marginalized sentence space (:math:`\Phi(\mathcal{S})`), DSO avoids large importance weights, making large-scale OPL more scalable regarding the number of candidate prompts, while keeping the bias small by leveraging the similarity among sentences.


Moreover, we can use the following expression to estimate the weighted score function.

.. math::

    & w(\phi(s), x) \nabla_{\theta} \log \pi_{\theta}(\phi(s)|x) \\
    &= \mathbb{E}_{(a, s') \sim \pi_{\theta}(a|x)p_{\text{LLM}}(s'|x,a)} \biggl[ \frac{K(s, s'; \, x, \tau) \nabla_{\theta} \log \pi_{\theta}(a | x)}{\pi_{0}(\phi(s)|x)} \biggr].

This expression indicates that DSO can be seen as performing soft rejection sampling on the data :math:`(a, s')` augmented by :math:`\pi_{\theta}`, while correcting the bias in the logged data by applying the inverse propensity of :math:`\pi_0` in the marginalized sentence space. 
This means that even though we observe only a single prompt in the original logged data, DSO can further distribute the reward observation among 
multiple prompts that generate similar sentences.
This *implicit data augmentation* among multiple counterfactual prompts also contributes to reducing variance. 



The above expression of the weighted score function also suggests that our estimation problem of the weighted score function is reduced to only the estimation of :math:`\pi_0(\phi(s)|x)`. 
This is useful, as :math:`\pi_0(\phi(s)|x)` does not depend on the parameterized policy (:math:`\pi_{\theta}`), and it thus suffices to fit a marginal density model only once before running the policy gradient method. 
Because the marginal distribution is defined as :math:`\pi_0(\phi(s)|x) = \mathbb{E}_{\pi_0(s'|x)}[K(s, s'; \, x, \tau)]`, 
we can estimate the marginal density via the monte-carlo sampling:

.. math::

    \pi_0(\phi(s_i)|x_i) \approx \frac{1}{m} \sum_{j=1}^m \mathbb{E}_{s_j \sim \pi_0(s_j|x)}[K(s_i,s_j; x_i, \tau)],

where :math:`m` is the number of the monte-carlo samples.
Similarly, we can also estimate the marginal density with function approximation (:math:`f_{\psi}(x, s) \approx \pi_0(\phi(s)|x)`) using the following loss:

.. math::

    \ell(f_{\psi}) \approx \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{(s,s') \sim \pi_0(s|x_i)\pi_0(s'|x_i)} \left[ (f_{\psi}(x_i,s) - K(s,s' ; \, x_i,\tau))^2 \right].

Since the computation of this loss does not scale with
the size of the action space :math:`|\mathcal{A}|`, we can easily apply DSO even when the action (i.e., prompt) space is large or continuous.


Theoretical Properties
~~~~~~~~~~
While we omit the detailed analysis, here we summarize key takeaway from our theoretical results. For the detailed analysis and proof, please refer to our paper :cite:``.

* DSO's bias is guaranteed under a **relaxed support assumption** called *similar sentence support*, which is defined as :math:`\forall (x, \phi(s)) \in \mathcal{X} \times \Phi(\mathcal{S}), \, \pi_{\theta}(\phi(s) | x) > 0 \implies \pi_0(\phi(s) | x) > 0`. 
* The dominant factor of the bias of DSO comes from the **within-neighbor expected reward shift** between :math:`\pi_0` and :math:`\pi`. This becomes small when using a **small bandwidth hyperparameter** :math:`\tau` **or a smooth kernel like a Gaussian kernel**.
* Variance reduction of DSO comes from two factors: (1) considering **marginalized distribution** instead of the original distribution and (2) applying **implicit data augmentation** via re-sampling technique. The variance reduction becomes large when using a **large kernel bandwidth hyperparameter** :math:`\tau`.

Together with the analysis of bias and variance, we can see that the value of :math:`\tau` plays an important role in trading off the bias and variance of DSO, as illustrated in the following figure. 
Specifically, when :math:`\tau` is large, the overlap between the logging policy (:math:`\pi_0`) and the current policy (:math:`\pi_{\theta}`) within a neighbor :math:`\phi(s)` becomes large, thus the scale of the importance weight becomes small. 
This contributes to reducing the variance compared to naive IS. 
In contrast, a small value of :math:`\tau` helps keep the bias small, as the within-neighbor reward shift (i.e., the difference between :math:`q^{\pi_0}(x,\phi(s))` and :math:`q^{\pi_{\theta}}(x,\phi(s))`) becomes small. 
Later in the next section, we study how the performance changes with varying values of the bandwidth hyperparameter :math:`\tau` and demonstrate that a smooth kernel like a Gaussian kernel is beneficial for reducing both bias and variance simultaneously.

.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/bias_variance_tradeoff.png
   :text-align: center

   Bias-variance tradeoff of DSO and its relations to the bandwidth hyperparameter (:math:`\tau`) of a kernel function.

.. raw:: html

    <div class="white-space-20px"></div>



Experiments
~~~~~~~~~~
We first compare DSO (our proposal) with regression :cite:``, IS :cite:``, DR :cite:``, and POTEC :cite:`` on synthetic benchmark with varying configurations:

- data size: :math:`n \in \{500, 1000, 2000, 4000, \underline{8000}\}`
- number of candidate actions: :math:`|\mathcal{A}| \in \{ 10, 50, 100, 500, \underline{1000} \}`,
- reward noises: :math:`\sigma_{r} \in \{ 0.0, \underline{1.0}, 2.0, 3.0 \}`

The underlined values are the default values. We also use the `optimality` score defined as :math:`(V(\pi) - V(\pi_{\text{unif}})) / V(\pi_{\text{opt}} - V(\pi_{\text{unif}}))` as the evaluation metrics.
Here, :math:`pi` is the policy of interest, :math:`\pi_{\text{opt}}` is the optimal policy, and :math:`\pi_{\text{unif}}` is the uniform random policy. 
For the detailed experiment settings, please also refer to our paper :cite:``.

Performance comparison with baselines
----------
The following figure compares the policy learning results of the OPL methods with varying data sizes (:math:`n`), number of candidate actions (:math:`|\mathcal{A}|`), and reward noises (:math:`\sigma_r`), from the left to right.
Note that we use the Gaussian kernel with a bandwidth hyperparameter :math:`tau = 1.0` and employ function approximation to estimate the logging marginal density in this experiment.

.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/synthetic_experiment.png
   :text-align: center

   Performance comparison of various OPL methods.
   
.. raw:: html

    <div class="white-space-20px"></div>

The results demonstrate that DSO works particularly well in challenging scenarios where the baselines fall short due to variance. 
Specifically, while we observe a sharp drop of performance for the baselines when the action space is large (:math:`|\mathcal{A}| \geq 500`) and reward noise is large (:math:`\sigma_r \geq 1.0`), 
DSO maintains a favorable performance even under these configurations. 
Moreover, comparing the performance with :math:`|\mathcal{A}|=1000` and :math:`\sigma_r=1.0`, 
we observe that the performance of DSO at :math:`n=500` outperforms that of the baselines at :math:`n=8000`. 
This indicates that DSO is far more data-efficient than the baselines when the action space is large, leveraging the similarity among sentences via kernels and performing implicit data augmentation.


Ablation study about kernels
----------
Next, we vary the following configurations of DSO and see how performance changes.

- type of kernel: :math:`\{ \underline{\mathrm{Gaussian}}, \mathrm{Uniform} \}`
- kernel bandwidth hyperparameters: :math:`\tau \in \{ 0.5, \underline{1.0}, 2.0, 4.0 \}`
- estimation of logging marginal density: :math:`\{ \text{monte-carlo}, \underline{\text{function approximation}} \}`

We report the results in the following figure.

.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/ablation_experiment.png
   :text-align: center

   Ablation results of DSO.

.. raw:: html

    <div class="white-space-20px"></div>

The results tell us several interesting findings: using (1) a Gaussian kernel and (2) function approximation improve the robustness of DSO to the choice of bandwidth hyperparameter :math:`\tau`. 
The first observation is evident from the fact that a Gaussian kernel allocates larger weights to closer sentences compared to a uniform kernel. 
However, when using monte-carlo estimation, we observe that even a Gaussian kernel needs careful tuning of :math:`\tau`, where a small value of :math:`\tau` incurs high variance and a large value of :math:`\tau` produces non-negligible bias. 
In contrast, by using function approximation, we can avoid a small value of :math:`\hat{\pi}_{0}(\phi(s)|x)`, contributing to the variance reduction (This is because, for example, when the true marginal density is 1e-5, estimating it as 1e-5 and 1e-4 does not change the MSE loss too much. However, in terms of variance, 1e-4 and 1e-5 make a significant difference.
Using function approximation, we can avoid being too precise about small values of the marginal density). 
Therefore, function approximation indeed helps improve the robustness to a small value of :math:`\tau`, and we do not need extensive hyperparameter tuning of :math:`\tau`. This implies that DSO is applicable to practical situations, where a pre-trained model of :math:`\hat{\pi}_{0}(\phi(s)|x)` can provide substantial efficiency gains.


Full-LLM experiment
----------
While we omit the detailed experiment setting here, we also conduct experiment using a semi-synthetic benchmark built on the MovieLens-10M dataset :cite:``.
We use the performance improvement over the no-prompt baseline (i.e., sentences generated without prompts) as the evaluation metrics and report the results in the following figure.


.. card::
   :width: 75%
   :margin: auto
   :img-top: ../_static/images/full_llm_experiment.png
   :text-align: center

   Performance comparison of OPL methods in the full-LLM experiment.

.. raw:: html

    <div class="white-space-20px"></div>


The results indicate that DSO often improves the effectiveness of the sentences more than other OPL methods,
by effectively leveraging the information about similar sentences.
Specifically, DSO is more resilient to performance corruption than IS by substantially reducing the variance and than regression by reducing the bias. 
As a result, we have **5x increase of the performance than other baselines on average**.
It should also be worth noting that this result is observed for the off-the-shelf embeddings of sentences, which do not require extensive tuning of the embedding model. 
This minimizes the difficulty in applying the proposed OPL method in practice. However, learning (application-specific) embeddings that further improve the performance of DSO is an interesting direction for future work.



Citation
~~~~~~~~~~
If you use the proposed method (DSO) or refer to our findings in your work, please cite our paper below.

.. card::

    | Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims
    | **Off-Policy Learning for Prompt-Guided Sentence Personalization Using Logged Bandit Data**

    .. code-block::

        @article{kiyohara2025off,
            title = {Off-Policy Learning for Prompt-Guided Sentence Personalization Using Logged Bandit Data},
            author = {Kiyohara, Haruka and Cao, Daniel Yiming and Saito, Yuta and Joachims, Thorsten},
            journal = {xxx},
            pages = {xxx--xxx},
            year = {2025},
        }

.. raw:: html

    <div class="white-space-20px"></div>

.. grid::
    :margin: 0

    .. grid-item::
        :columns: 3
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: implementations
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                <<< Prev
                **Implementation**

    .. grid-item::
        :columns: 6
        :margin: 0
        :padding: 0

    .. grid-item::
        :columns: 3
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: api
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **API reference**
