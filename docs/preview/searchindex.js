Search.setIndex({"docnames": ["documentation/_autosummary/dataset/src.dataset.base", "documentation/_autosummary/dataset/src.dataset.base.BaseCandidateActionsLoader", "documentation/_autosummary/dataset/src.dataset.base.BaseContextQueryLoader", "documentation/_autosummary/dataset/src.dataset.base.BaseFrozenLLM", "documentation/_autosummary/dataset/src.dataset.base.BasePromptFormatter", "documentation/_autosummary/dataset/src.dataset.base.BaseRewardSimulator", "documentation/_autosummary/dataset/src.dataset.benchmark", "documentation/_autosummary/dataset/src.dataset.benchmark.SemiSyntheticDataset", "documentation/_autosummary/dataset/src.dataset.frozen_llm", "documentation/_autosummary/dataset/src.dataset.function", "documentation/_autosummary/dataset/src.dataset.function.DefaultCandidateActionsLoader", "documentation/_autosummary/dataset/src.dataset.function.DefaultContextQueryLoader", "documentation/_autosummary/dataset/src.dataset.prompt_formatter", "documentation/_autosummary/dataset/src.dataset.reward_simulator", "documentation/_autosummary/dataset/src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator", "documentation/_autosummary/dataset/src.dataset.reward_simulator.TransformerRewardSimulator", "documentation/_autosummary/dataset/toy.dataset.function", "documentation/_autosummary/dataset/toy.dataset.function.AuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.CandidateActionsGenerator", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedExponentialAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedPowerAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedRationalAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedTrigonometricAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.ContextQueryGenerator", "documentation/_autosummary/dataset/toy.dataset.function.ExponentialAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.MixtureOfGaussianCandidateActionsGenerator", "documentation/_autosummary/dataset/toy.dataset.function.PowerAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.RationalAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.RewardSimulator", "documentation/_autosummary/dataset/toy.dataset.function.SigmoidAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.function.SparseRewardSimulator", "documentation/_autosummary/dataset/toy.dataset.function.TrigonometricAuxiliaryOutputGenerator", "documentation/_autosummary/dataset/toy.dataset.synthetic", "documentation/_autosummary/dataset/toy.dataset.synthetic.SyntheticDataset", "documentation/_autosummary/src.opl.behavior_cloning", "documentation/_autosummary/src.opl.dataset", "documentation/_autosummary/src.opl.marginal_learner", "documentation/_autosummary/src.opl.policy_learner", "documentation/_autosummary/src.opl.reward_learner", "documentation/_autosummary/src.policy.base", "documentation/_autosummary/src.policy.model", "documentation/_autosummary/src.policy.policy", "documentation/_autosummary/src.utils", "documentation/_autosummary/toy.opl.behavior_cloning", "documentation/_autosummary/toy.opl.behavior_cloning.BehaviorCloningLearner", "documentation/_autosummary/toy.opl.marginal_learner", "documentation/_autosummary/toy.opl.marginal_learner.MarginalDensityLearner", "documentation/_autosummary/toy.opl.policy_learner", "documentation/_autosummary/toy.opl.policy_learner.KernelPolicyLearner", "documentation/_autosummary/toy.opl.policy_learner.PolicyLearner", "documentation/_autosummary/toy.opl.reward_learner", "documentation/_autosummary/toy.opl.reward_learner.ActionRewardLearner", "documentation/_autosummary/toy.opl.reward_learner.OutputRewardLearner", "documentation/_autosummary/toy.policy.base", "documentation/_autosummary/toy.policy.base.BaseActionPolicyModel", "documentation/_autosummary/toy.policy.base.BaseActionRewardModel", "documentation/_autosummary/toy.policy.base.BaseClusterPolicyModel", "documentation/_autosummary/toy.policy.base.BaseClusteringModel", "documentation/_autosummary/toy.policy.base.BaseKernelMarginalDensityModel", "documentation/_autosummary/toy.policy.base.BaseOutputRewardModel", "documentation/_autosummary/toy.policy.base.BasePolicy", "documentation/_autosummary/toy.policy.model", "documentation/_autosummary/toy.policy.model.KmeansActionClustering", "documentation/_autosummary/toy.policy.model.NeuralActionPolicy", "documentation/_autosummary/toy.policy.model.NeuralActionRewardPredictor", "documentation/_autosummary/toy.policy.model.NeuralClusterPolicy", "documentation/_autosummary/toy.policy.model.NeuralMarginalDensityEstimator", "documentation/_autosummary/toy.policy.model.NeuralOutputRewardPredictor", "documentation/_autosummary/toy.policy.policy", "documentation/_autosummary/toy.policy.policy.EpsilonGreedyPolicy", "documentation/_autosummary/toy.policy.policy.SoftmaxPolicy", "documentation/_autosummary/toy.policy.policy.TwoStagePolicy", "documentation/_autosummary/toy.policy.policy.UniformRandomPolicy", "documentation/_autosummary/toy.utils", "documentation/_autosummary/toy.utils.check_logged_feedback", "documentation/_autosummary/toy.utils.check_tensor", "documentation/_autosummary/toy.utils.defaultdict_to_dict", "documentation/_autosummary/toy.utils.gaussian_kernel", "documentation/_autosummary/toy.utils.torch_seed", "documentation/_autosummary/toy.utils.uniform_kernel", "documentation/installation", "documentation/offline_prompts_api", "documentation/opl_for_prompts", "documentation/overview", "documentation/quickstart", "index"], "filenames": ["documentation/_autosummary/dataset/src.dataset.base.rst", "documentation/_autosummary/dataset/src.dataset.base.BaseCandidateActionsLoader.rst", "documentation/_autosummary/dataset/src.dataset.base.BaseContextQueryLoader.rst", "documentation/_autosummary/dataset/src.dataset.base.BaseFrozenLLM.rst", "documentation/_autosummary/dataset/src.dataset.base.BasePromptFormatter.rst", "documentation/_autosummary/dataset/src.dataset.base.BaseRewardSimulator.rst", "documentation/_autosummary/dataset/src.dataset.benchmark.rst", "documentation/_autosummary/dataset/src.dataset.benchmark.SemiSyntheticDataset.rst", "documentation/_autosummary/dataset/src.dataset.frozen_llm.rst", "documentation/_autosummary/dataset/src.dataset.function.rst", "documentation/_autosummary/dataset/src.dataset.function.DefaultCandidateActionsLoader.rst", "documentation/_autosummary/dataset/src.dataset.function.DefaultContextQueryLoader.rst", "documentation/_autosummary/dataset/src.dataset.prompt_formatter.rst", "documentation/_autosummary/dataset/src.dataset.reward_simulator.rst", "documentation/_autosummary/dataset/src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.rst", "documentation/_autosummary/dataset/src.dataset.reward_simulator.TransformerRewardSimulator.rst", "documentation/_autosummary/dataset/toy.dataset.function.rst", "documentation/_autosummary/dataset/toy.dataset.function.AuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.CandidateActionsGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedExponentialAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedPowerAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedRationalAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.ConfoundedTrigonometricAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.ContextQueryGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.ExponentialAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.MixtureOfGaussianCandidateActionsGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.PowerAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.RationalAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.RewardSimulator.rst", "documentation/_autosummary/dataset/toy.dataset.function.SigmoidAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.function.SparseRewardSimulator.rst", "documentation/_autosummary/dataset/toy.dataset.function.TrigonometricAuxiliaryOutputGenerator.rst", "documentation/_autosummary/dataset/toy.dataset.synthetic.rst", "documentation/_autosummary/dataset/toy.dataset.synthetic.SyntheticDataset.rst", "documentation/_autosummary/src.opl.behavior_cloning.rst", "documentation/_autosummary/src.opl.dataset.rst", "documentation/_autosummary/src.opl.marginal_learner.rst", "documentation/_autosummary/src.opl.policy_learner.rst", "documentation/_autosummary/src.opl.reward_learner.rst", "documentation/_autosummary/src.policy.base.rst", "documentation/_autosummary/src.policy.model.rst", "documentation/_autosummary/src.policy.policy.rst", "documentation/_autosummary/src.utils.rst", "documentation/_autosummary/toy.opl.behavior_cloning.rst", "documentation/_autosummary/toy.opl.behavior_cloning.BehaviorCloningLearner.rst", "documentation/_autosummary/toy.opl.marginal_learner.rst", "documentation/_autosummary/toy.opl.marginal_learner.MarginalDensityLearner.rst", "documentation/_autosummary/toy.opl.policy_learner.rst", "documentation/_autosummary/toy.opl.policy_learner.KernelPolicyLearner.rst", "documentation/_autosummary/toy.opl.policy_learner.PolicyLearner.rst", "documentation/_autosummary/toy.opl.reward_learner.rst", "documentation/_autosummary/toy.opl.reward_learner.ActionRewardLearner.rst", "documentation/_autosummary/toy.opl.reward_learner.OutputRewardLearner.rst", "documentation/_autosummary/toy.policy.base.rst", "documentation/_autosummary/toy.policy.base.BaseActionPolicyModel.rst", "documentation/_autosummary/toy.policy.base.BaseActionRewardModel.rst", "documentation/_autosummary/toy.policy.base.BaseClusterPolicyModel.rst", "documentation/_autosummary/toy.policy.base.BaseClusteringModel.rst", "documentation/_autosummary/toy.policy.base.BaseKernelMarginalDensityModel.rst", "documentation/_autosummary/toy.policy.base.BaseOutputRewardModel.rst", "documentation/_autosummary/toy.policy.base.BasePolicy.rst", "documentation/_autosummary/toy.policy.model.rst", "documentation/_autosummary/toy.policy.model.KmeansActionClustering.rst", "documentation/_autosummary/toy.policy.model.NeuralActionPolicy.rst", "documentation/_autosummary/toy.policy.model.NeuralActionRewardPredictor.rst", "documentation/_autosummary/toy.policy.model.NeuralClusterPolicy.rst", "documentation/_autosummary/toy.policy.model.NeuralMarginalDensityEstimator.rst", "documentation/_autosummary/toy.policy.model.NeuralOutputRewardPredictor.rst", "documentation/_autosummary/toy.policy.policy.rst", "documentation/_autosummary/toy.policy.policy.EpsilonGreedyPolicy.rst", "documentation/_autosummary/toy.policy.policy.SoftmaxPolicy.rst", "documentation/_autosummary/toy.policy.policy.TwoStagePolicy.rst", "documentation/_autosummary/toy.policy.policy.UniformRandomPolicy.rst", "documentation/_autosummary/toy.utils.rst", "documentation/_autosummary/toy.utils.check_logged_feedback.rst", "documentation/_autosummary/toy.utils.check_tensor.rst", "documentation/_autosummary/toy.utils.defaultdict_to_dict.rst", "documentation/_autosummary/toy.utils.gaussian_kernel.rst", "documentation/_autosummary/toy.utils.torch_seed.rst", "documentation/_autosummary/toy.utils.uniform_kernel.rst", "documentation/installation.rst", "documentation/offline_prompts_api.rst", "documentation/opl_for_prompts.rst", "documentation/overview.rst", "documentation/quickstart.rst", "index.rst"], "titles": ["src.dataset.base", "src.dataset.base.BaseCandidateActionsLoader", "src.dataset.base.BaseContextQueryLoader", "src.dataset.base.BaseFrozenLLM", "src.dataset.base.BasePromptFormatter", "src.dataset.base.BaseRewardSimulator", "src.dataset.benchmark", "src.dataset.benchmark.SemiSyntheticDataset", "src.dataset.frozen_llm", "src.dataset.function", "src.dataset.function.DefaultCandidateActionsLoader", "src.dataset.function.DefaultContextQueryLoader", "src.dataset.prompt_formatter", "src.dataset.reward_simulator", "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator", "src.dataset.reward_simulator.TransformerRewardSimulator", "toy.dataset.function", "toy.dataset.function.AuxiliaryOutputGenerator", "toy.dataset.function.CandidateActionsGenerator", "toy.dataset.function.ConfoundedAuxiliaryOutputGenerator", "toy.dataset.function.ConfoundedExponentialAuxiliaryOutputGenerator", "toy.dataset.function.ConfoundedPowerAuxiliaryOutputGenerator", "toy.dataset.function.ConfoundedRationalAuxiliaryOutputGenerator", "toy.dataset.function.ConfoundedTrigonometricAuxiliaryOutputGenerator", "toy.dataset.function.ContextQueryGenerator", "toy.dataset.function.ExponentialAuxiliaryOutputGenerator", "toy.dataset.function.MixtureOfGaussianCandidateActionsGenerator", "toy.dataset.function.PowerAuxiliaryOutputGenerator", "toy.dataset.function.RationalAuxiliaryOutputGenerator", "toy.dataset.function.RewardSimulator", "toy.dataset.function.SigmoidAuxiliaryOutputGenerator", "toy.dataset.function.SparseRewardSimulator", "toy.dataset.function.TrigonometricAuxiliaryOutputGenerator", "toy.dataset.synthetic", "toy.dataset.synthetic.SyntheticDataset", "src.opl.behavior_cloning", "src.opl.dataset", "src.opl.marginal_learner", "src.opl.policy_learner", "src.opl.reward_learner", "src.policy.base", "src.policy.model", "src.policy.policy", "src.utils", "toy.opl.behavior_cloning", "toy.opl.behavior_cloning.BehaviorCloningLearner", "toy.opl.marginal_learner", "toy.opl.marginal_learner.MarginalDensityLearner", "toy.opl.policy_learner", "toy.opl.policy_learner.KernelPolicyLearner", "toy.opl.policy_learner.PolicyLearner", "toy.opl.reward_learner", "toy.opl.reward_learner.ActionRewardLearner", "toy.opl.reward_learner.OutputRewardLearner", "toy.policy.base", "toy.policy.base.BaseActionPolicyModel", "toy.policy.base.BaseActionRewardModel", "toy.policy.base.BaseClusterPolicyModel", "toy.policy.base.BaseClusteringModel", "toy.policy.base.BaseKernelMarginalDensityModel", "toy.policy.base.BaseOutputRewardModel", "toy.policy.base.BasePolicy", "toy.policy.model", "toy.policy.model.KmeansActionClustering", "toy.policy.model.NeuralActionPolicy", "toy.policy.model.NeuralActionRewardPredictor", "toy.policy.model.NeuralClusterPolicy", "toy.policy.model.NeuralMarginalDensityEstimator", "toy.policy.model.NeuralOutputRewardPredictor", "toy.policy.policy", "toy.policy.policy.EpsilonGreedyPolicy", "toy.policy.policy.SoftmaxPolicy", "toy.policy.policy.TwoStagePolicy", "toy.policy.policy.UniformRandomPolicy", "toy.utils", "toy.utils.check_logged_feedback", "toy.utils.check_tensor", "toy.utils.defaultdict_to_dict", "toy.utils.gaussian_kernel", "toy.utils.torch_seed", "toy.utils.uniform_kernel", "Installation", "OfflinePrompts API reference", "Off-policy learning for prompt-guided language generation", "Overview of the software", "Quickstart", "OfflinePrompts: A Python library for optimizing language generation with naturally collected user feedback and prompt"], "terms": {"problem": 83, "formul": 83, "baselin": 83, "propos": 83, "method": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 83], "xxx": [], "opl": 81, "guid": 86, "overview": 86, "github": [81, 86], "licens": 86, "releas": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 86], "note": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 86], "proceed": 86, "off": 86, "polici": [0, 3, 6, 7, 33, 34, 35, 36, 37, 38, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 74, 75, 86], "learn": [36, 86], "softwar": 86, "instal": 86, "quickstart": 86, "api": [38, 40, 41, 42, 48, 50, 54, 57, 62, 63, 66, 69, 70, 71, 73, 86], "figur": [84, 86], "implemet": 84, "pipelin": [81, 84], "featur": [13, 15, 16, 24, 26, 33, 34, 40, 41, 42, 43, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 84], "from": [5, 6, 7, 14, 15, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 81, 84, 85], "appendix": [84, 85], "test": [6, 7, 9, 11], "welcom": 86, "sentenc": [0, 3, 5, 6, 7, 8, 9, 11, 12, 13, 15, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 57, 60, 61, 64, 66, 68, 69, 70, 71, 72, 73, 74, 75, 86], "scope": [], "rl": [], "i": [5, 6, 7, 8, 12, 13, 14, 15, 33, 34, 36, 40, 41, 42, 43, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 81], "avail": [8, 40, 54, 56, 58, 81], "pypi": 81, "can": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 81], "pip": 81, "sourc": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81], "follow": [0, 5, 6, 7, 14, 15, 33, 34, 35, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 74, 75, 81], "offlin": 81, "prompt": [0, 3, 4, 6, 7, 8, 12, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 81], "git": 81, "clone": [35, 44, 45, 81], "http": 81, "com": 81, "aiueola": 81, "cd": 81, "python": 81, "setup": 81, "py": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 81], "If": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 81], "you": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 81], "us": [0, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 81], "our": 81, "your": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 81], "work": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 81], "pleas": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 81], "cite": 81, "paper": 81, "below": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 81], "haruka": 81, "kiyohara": 81, "yuta": 81, "saito": 81, "daniel": 81, "cao": 81, "kiant": 81, "brantli": 81, "thorsten": 81, "joachim": 81, "offlineprompt": 81, "A": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 81], "librari": 81, "optim": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 81], "languag": [33, 34, 81], "gener": [0, 3, 5, 6, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 46, 47, 48, 49, 51, 52, 53, 54, 59, 60, 62, 67, 68, 74, 75, 81], "natur": 81, "collect": [6, 7, 33, 34, 39, 51, 52, 53, 81], "user": [0, 3, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 24, 33, 34, 36, 40, 42, 43, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 81], "feedback": [43, 74, 75, 81], "preprint": 81, "come": 81, "soon": 81, "articl": 81, "kiyohara2024xxx": 81, "titl": 81, "author": 81, "journal": 81, "arxiv": 81, "24xx": 81, "xxxxx": 81, "year": 81, "2024": 81, "exampl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 85], "class": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 79], "handl": [5, 6, 14, 15, 16, 33, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "synthet": [6, 16, 38, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "contextquerygener": 16, "dim_context": [6, 7, 16, 24, 29, 31, 33, 34, 36, 40, 41, 42, 43, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "5": [9, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34], "dim_queri": [16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 40, 41, 42, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "context_query_covari": [16, 24], "none": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76], "devic": [5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 40, 41, 42, 43, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 79], "cuda": [5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 40, 41, 42, 43, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "0": [5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 39, 40, 41, 42, 43, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 78, 80], "random_st": [6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 79], "context": [0, 2, 3, 5, 6, 7, 8, 9, 11, 14, 15, 16, 24, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "queri": [0, 2, 3, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "paramet": [0, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76], "int": [5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76], "default": [5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76], "dimens": [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 40, 41, 43, 62, 64, 65, 66, 67, 68, 74, 76], "torch": [0, 5, 6, 7, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "tensor": [0, 5, 6, 7, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 41, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76], "shape": [0, 3, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 41, 42, 43, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "covari": [16, 24], "between": [5, 14, 15, 16, 24, 40, 41, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68], "vector": [9, 11, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 40, 41, 42, 43, 54, 55, 57, 61, 62, 64, 66, 69, 70, 71, 72, 73, 74, 75], "str": [5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 79], "random": [6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73], "state": [5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73], "attribut": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73], "sample_context_and_queri": [0, 2, 9, 11, 16, 24], "n_sampl": [0, 3, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "sampl": [6, 7, 8, 9, 11, 16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 44, 45, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73], "number": [5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 18, 24, 26, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "return": [5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 38, 40, 41, 42, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "each": [0, 3, 5, 6, 7, 8, 9, 11, 14, 15, 16, 24, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "given": [0, 3, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 24, 29, 31, 33, 34, 37, 38, 40, 41, 42, 43, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "candidateactionsgener": 16, "n_action": [6, 7, 8, 9, 10, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 30, 32, 33, 34, 36, 37, 38, 40, 41, 42, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 63, 64, 66, 69, 70, 71, 72, 73], "1000": [6, 7, 9, 10, 16, 18, 26, 33, 34, 37, 38, 39, 46, 47, 48, 49, 50, 51, 52, 53], "dim_action_embed": [16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 30, 32, 33, 34, 54, 60, 68], "candid": [0, 1, 6, 7, 9, 10, 16, 18, 26, 33, 34, 38, 40, 42, 48, 50, 54, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73], "action": [0, 1, 6, 7, 9, 10, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "discret": [0, 3, 6, 7, 8, 9, 10, 16, 18, 26, 33, 34, 37, 39, 40, 41, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 64, 66, 69, 70, 73, 74, 75], "embed": [6, 7, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 30, 32, 33, 34, 38, 40, 41, 42, 43, 48, 49, 50, 54, 55, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "auxiliaryoutputgener": [16, 46, 47, 48, 49, 51, 52, 53, 54, 59, 60, 62, 67, 68], "dim_auxiliary_output": [6, 7, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 40, 54, 57, 58, 59, 60, 62, 63, 66, 67, 68, 69, 70, 71], "query_coefficient_matrix": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32], "action_coefficient_matrix": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32], "noise_level": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32], "auxiliari": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 40, 46, 47, 48, 49, 51, 52, 53, 54, 57, 58, 59, 60, 62, 63, 66, 67, 68, 69, 70, 71], "output": [5, 6, 7, 8, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39, 40, 41, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], "dim_action_embd": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32, 33, 34], "coeffici": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32], "float": [5, 6, 7, 9, 11, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 39, 40, 42, 43, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76], "nois": [6, 7, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32, 33, 34, 62, 67], "level": [6, 7, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32, 33, 34], "sample_auxiliary_output": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32], "action_embed": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32], "enumerate_act": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32], "fals": [5, 6, 7, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "bool": [5, 6, 7, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "whether": [5, 6, 7, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32, 33, 34, 35, 37, 38, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "all": [5, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32, 33, 34, 38, 40, 42, 48, 50, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "auxiliary_output": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 54, 58, 59, 60, 63, 67, 68], "type": [5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 40, 41, 42, 43, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76], "m_sampl": [16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32], "rewardsimul": 16, "context_coefficient_matrix": [16, 29, 31], "normal": [16, 29, 31], "simul": [0, 5, 6, 7, 9, 10, 13, 14, 15, 16, 29, 31, 33, 34, 37, 40, 43, 46, 47, 54, 59, 67, 74, 75], "expect": [0, 5, 6, 7, 13, 14, 15, 16, 29, 31, 33, 34, 43, 44, 45, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 75, 76], "reward": [0, 5, 6, 7, 13, 14, 15, 16, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 60, 61, 62, 63, 64, 65, 68, 69, 70, 71, 72, 73, 74, 75], "term": [16, 29, 31], "calc_expected_reward": [0, 5, 13, 14, 15, 16, 29, 31], "expected_reward": [6, 7, 13, 14, 15, 16, 29, 31, 33, 34, 35, 37, 39, 43, 44, 45, 46, 47, 51, 52, 53, 74, 75], "mixtureofgaussiancandidateactionsgener": 16, "n_cluster": [16, 26, 40, 41, 42, 54, 57, 58, 62, 63, 66, 69, 70, 71], "100": [5, 13, 14, 15, 16, 26, 35, 37, 39, 41, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68], "n_actions_per_clust": [16, 26], "1": [5, 6, 7, 14, 15, 16, 19, 20, 21, 22, 23, 25, 26, 27, 30, 32, 33, 34, 39, 40, 41, 42, 43, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 78, 80], "cluster_cent": [16, 26, 40, 42, 54, 57, 58, 63, 66, 69, 70, 71, 73], "gaussian": [16, 26, 43, 74, 78], "distribut": [16, 26], "cluster": [6, 7, 16, 26, 33, 34, 38, 40, 41, 42, 44, 45, 48, 50, 51, 52, 53, 54, 57, 58, 62, 63, 66, 69, 70, 71, 72, 73], "center": [16, 26, 40, 41, 42, 54, 57, 58, 62, 63, 66, 69, 70, 71], "confoundedauxiliaryoutputgener": 16, "dim_confounded_auxiliary_output": [16, 19, 20, 21, 22, 23], "confounder_scal": [16, 19, 20, 21, 22, 23], "confound": [16, 19, 20, 21, 22, 23], "scale": [16, 19], "rationalauxiliaryoutputgener": 16, "bia": [5, 14, 15, 16, 22, 28, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "sigmoidauxiliaryoutputgener": 16, "max_abs_v": [16, 20, 21, 23, 25, 27, 30, 32], "scaler": [16, 20, 21, 23, 25, 27, 30, 32], "trigonometricauxiliaryoutputgener": 16, "powerauxiliaryoutputgener": 16, "exponentialauxiliaryoutputgener": 16, "confoundedrationalauxiliaryoutputgener": 16, "confoundedtrigonometricauxiliaryoutputgener": 16, "confoundedpowerauxiliaryoutputgener": 16, "confoundedexponentialauxiliaryoutputgener": 16, "sparserewardsimul": 16, "dim_sparse_auxiliary_output": [16, 31], "syntheticdataset": [33, 35, 37, 38, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "context_query_gener": [33, 34], "candidate_action_gener": [33, 34], "auxiliary_output_gener": [33, 34, 46, 47, 48, 49, 51, 52, 53, 54, 59, 60, 62, 67, 68], "reward_simul": [6, 7, 33, 34], "reward_typ": [6, 7, 33, 34], "continu": [6, 7, 33, 34, 43, 74, 75], "reward_std": [6, 7, 33, 34], "base": [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 33, 34, 35, 36, 37, 38, 39, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "log": [0, 6, 7, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 74, 75], "import": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73], "thi": [0, 5, 8, 14, 15, 33, 34, 38, 40, 42, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "substitut": [33, 34], "part": [5, 14, 15, 33, 34, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "benchmark": [33, 34, 35, 37, 38, 39, 44, 45, 46, 47, 51, 52, 53], "numer": [33, 34], "x": [33, 34], "d_x": [33, 34], "dimension": [33, 34, 40, 41, 42, 54, 57, 58, 63, 66, 69, 70, 71], "q": [33, 34], "d_q": [33, 34], "d_a": [33, 34], "e": [0, 5, 6, 7, 13, 14, 15, 33, 34, 36, 40, 41, 42, 43, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "d_e": [33, 34], "o": [5, 14, 15, 33, 34, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "d_o": [33, 34], "m": [5, 14, 15, 33, 34, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "n": [5, 14, 15, 33, 34, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "where": [33, 34], "matrix": [33, 34], "r": [33, 34], "maxtrix": [33, 34], "binari": [6, 7, 33, 34, 39, 43, 51, 52, 53, 74, 75], "standard": [6, 7, 33, 34, 40, 54, 58, 63], "deviat": [6, 7, 33, 34, 40, 54, 58, 63], "sample_dataset": [6, 7, 33, 34], "10000": [6, 7, 33, 34], "is_oracle_polici": [6, 7, 33, 34], "is_oracle_clustering_logging_polici": [6, 7, 33, 34, 44, 45, 51, 52, 53], "return_action_choice_prob": [6, 7, 33, 34], "return_meta_data": [6, 7, 33, 34], "data": [6, 7, 9, 11, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 59, 62, 67, 74, 75], "choos": [6, 7, 33, 34, 41, 42, 43, 62, 64, 66, 69, 72, 74, 75], "index": [6, 7, 33, 34, 36, 40, 41, 54, 58, 62, 63], "oracl": [6, 7, 33, 34, 44, 45, 48, 50, 51, 52, 53], "record": [5, 6, 7, 14, 15, 33, 34, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "choic": [6, 7, 33, 34, 36, 38, 40, 42, 48, 50, 54, 55, 57, 58, 61, 63, 64, 66, 69, 70, 71, 72, 73], "probabl": [6, 7, 33, 34, 36, 38, 40, 42, 48, 50, 54, 55, 57, 58, 61, 63, 64, 66, 69, 70, 71, 72, 73], "meta": [6, 7, 33, 34], "includ": [5, 6, 7, 12, 14, 15, 33, 34, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "size": [5, 6, 7, 8, 14, 15, 33, 34, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "etc": [5, 6, 7, 14, 15, 33, 34, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "logged_dataset": [6, 7, 33, 34], "dict": [5, 6, 7, 8, 13, 14, 15, 33, 34, 35, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 74, 77], "dictionari": [5, 6, 7, 13, 14, 15, 33, 34, 41, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "contain": [5, 6, 7, 13, 14, 15, 33, 34, 35, 37, 38, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 75], "kei": [5, 6, 7, 14, 15, 33, 34, 35, 37, 38, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 75], "logging_polici": [6, 7, 33, 34, 35, 37, 38, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 74, 75], "chosen": [0, 3, 6, 7, 33, 34, 40, 41, 42, 43, 54, 58, 62, 63, 69, 70, 74, 75], "auiliari": [33, 34], "either": [5, 6, 7, 14, 15, 33, 34, 38, 41, 43, 48, 50, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 75], "soft": [6, 7, 33, 34, 43, 74, 75], "meta_data": [6, 7, 33, 34], "set": [5, 6, 7, 14, 15, 33, 34, 40, 42, 43, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 79], "action_list": [6, 7, 33, 34, 36, 37, 38, 39, 40, 41, 42, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 62, 63, 65, 67, 68, 69, 70, 71, 73], "applic": [6, 7, 33, 34, 40, 42, 54, 56, 58, 59, 69, 70], "onli": [5, 6, 7, 8, 14, 15, 33, 34, 40, 42, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70], "when": [5, 6, 7, 9, 11, 13, 14, 15, 33, 34, 38, 40, 41, 42, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "map": [6, 7, 33, 34, 37, 38, 39, 40, 41, 42, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 62, 63, 65, 67, 68, 69, 70, 71, 73], "calc_expected_reward_given_act": [33, 34], "n_outputs_to_approxim": [33, 34], "10": [13, 14, 15, 33, 34, 35, 37, 38, 39, 40, 41, 44, 45, 48, 49, 50, 51, 52], "calcul": [0, 5, 13, 14, 15, 33, 34, 40, 41, 42, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "approzim": [33, 34], "calc_expected_reward_for_all_act": [33, 34], "calc_expected_policy_valu": [6, 7, 33, 34], "n_samples_to_approxim": [6, 7, 33, 34, 40, 41, 48, 49, 54, 59, 62, 67], "two": [5, 6, 7, 14, 15, 33, 34, 42, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "stage": [6, 7, 33, 34, 38, 42, 48, 49, 50, 69, 70, 71, 72, 73], "approxim": [6, 7, 33, 34, 40, 48, 49, 54, 59, 67], "valu": [5, 6, 7, 14, 15, 33, 34, 38, 40, 42, 43, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76], "sample_reward_given_act": [6, 7, 33, 34], "dim_act": [33, 34, 40, 43, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73, 74, 75], "sample_reward_given_output": [6, 7, 33, 34], "learner": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "predict": [6, 7, 35, 36, 38, 39, 40, 42, 43, 44, 48, 50, 51, 52, 53, 54, 55, 56, 58, 60, 61, 63, 64, 65, 68, 69, 70, 71, 72, 73, 74, 75], "model": [5, 6, 7, 8, 13, 14, 15, 35, 37, 38, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 69, 70, 71, 72, 73], "behaviorcloninglearn": [35, 44], "baseactionpolicymodel": [35, 38, 44, 45, 48, 49, 50, 54, 62, 64, 69, 70, 71], "adam": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "optimizer_kwarg": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "option": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68], "ani": [5, 12, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "env": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "dataset": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "alia": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "predictor": [35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 61, 62, 64, 65, 68, 69, 70, 71, 72, 73], "src": [44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73], "promptrewardlearn": [37, 39, 46, 47, 51, 52], "basediscreteactionrewardmodel": [44, 45], "an": [5, 14, 15, 35, 37, 38, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "instanc": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "argument": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "onlin": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "environ": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "evalu": [5, 14, 15, 35, 37, 38, 39, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69], "load": [35, 37, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "path": [5, 6, 7, 9, 10, 11, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "is_init": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "save": [5, 14, 15, 35, 37, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "seed": [35, 37, 38, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 74, 79], "fix": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "online_clon": [35, 39, 44, 45, 51, 52], "teacher_model": [35, 39, 44, 45, 51, 52], "is_oracle_teacher_polici": [44, 45], "n_epoch": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "n_steps_per_epoch": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "batch_siz": [8, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "32": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "make_copi": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "save_path": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "train": [5, 9, 11, 14, 15, 35, 37, 38, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "basepolici": [6, 7, 35, 38, 40, 42, 43, 44, 45, 48, 50, 54, 69, 70, 71, 73, 74, 75], "twostagepolici": [35, 42, 44, 45, 69], "pre": [5, 14, 15, 35, 38, 39, 40, 42, 44, 45, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "epoch": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "gradient": [5, 13, 14, 15, 35, 37, 38, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71], "step": [35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "within": [5, 14, 15, 35, 37, 38, 39, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "batch": [8, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53], "creat": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "copi": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "befor": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "offline_clon": [35, 39, 44, 45, 51, 52], "logged_feedback": [35, 37, 38, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 74, 75], "val_ratio": [35, 39, 44, 45, 51, 52, 53], "2": [5, 6, 7, 14, 15, 35, 39, 44, 45, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "loggeddataset": [35, 37, 38, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 74, 75], "which": [5, 12, 14, 15, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75], "see": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "detail": [5, 14, 15, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "proport": [35, 39, 44, 45, 51, 52, 53], "valid": [35, 39, 43, 44, 45, 51, 52, 53, 74, 76], "first": [38, 42, 48, 50, 69, 70, 71, 72, 73], "second": [38, 42, 48, 50, 69, 72], "policylearn": [38, 48, 49], "union": [38, 48, 50], "baseclusterpolicymodel": [38, 40, 41, 42, 48, 50, 54, 62, 66, 69, 70, 71], "second_stage_polici": [38, 42, 48, 50, 69, 72], "nonetyp": [38, 48, 50], "clustering_polici": [38, 42, 48, 50, 69, 72], "baseclusteringmodel": [38, 40, 41, 42, 48, 50, 54, 62, 63, 69, 72], "action_reward_predictor": [48, 50], "baseactionrewardmodel": [40, 48, 50, 51, 52, 54, 55, 61, 62, 64, 65, 69, 70, 71, 72, 73], "output_reward_predictor": [48, 49], "baseoutputrewardmodel": [38, 48, 49, 51, 52, 53, 54], "singl": [5, 14, 15, 38, 48, 49, 50, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "id": [0, 5, 6, 7, 9, 11, 13, 14, 15, 37, 38, 39, 40, 41, 42, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 62, 63, 65, 67, 68, 69, 70, 71, 73], "its": [5, 14, 15, 38, 40, 41, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 71, 73], "must": [5, 14, 15, 38, 40, 42, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "determin": [6, 7, 38, 48, 50], "hybrid": [38, 48, 49, 50], "appraoch": [38, 48, 49, 50], "output_reward_oredictor": [48, 49], "online_policy_gradi": [38, 48, 50], "return_training_log": [38, 48, 49, 50], "manner": [38, 48, 50], "true": [5, 6, 7, 13, 14, 15, 38, 40, 42, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "model_based_policy_gradi": [38, 48, 50], "logging_action_choice_prob": [36, 38, 48, 50], "logging_predicted_reward": [38, 40, 41, 48, 50, 54, 58, 63], "is_oracle_logging_polici": [48, 50, 51, 52, 53], "via": [37, 38, 40, 46, 47, 48, 49, 50, 54, 59, 67], "approach": [38, 42, 48, 49, 50, 69, 72], "For": [5, 14, 15, 38, 41, 42, 48, 50, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 73], "consist": [5, 14, 15, 38, 40, 41, 42, 48, 50, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73], "importance_sampling_based_policy_gradi": [38, 48, 49, 50], "hybrid_policy_gradi": [38, 48, 49, 50], "adaptivepolicylearn": [], "list": [5, 8, 14, 15, 36, 39, 40, 41, 42, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73], "base_clustering_polici": [], "adapt": [], "switch": [], "adapitvepolicylearn": [], "outputrewardlearn": 51, "function": [0, 5, 14, 15, 38, 40, 41, 43, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 74], "sentencerewardlearn": [39, 51, 53], "dim_action_emb": [37, 46, 47, 51, 52, 53], "online_train": [39, 51, 52, 53], "loss_typ": [39, 51, 52, 53], "mse": [39, 51, 52, 53], "bce": [39, 51, 52, 53], "mean": [39, 40, 42, 51, 52, 53, 54, 55, 61, 64, 69, 70, 71, 72, 73], "squar": [39, 51, 52, 53], "error": [5, 14, 15, 39, 51, 52, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "loss": [39, 51, 52, 53], "cross": [39, 51, 52, 53], "entropi": [39, 51, 52, 53], "offline_train": [39, 51, 52, 53], "is_pessimist": [39, 51, 52, 53], "pessimistic_weight": [39, 51, 52, 53], "pessimist": [39, 51, 52, 53], "weight": [5, 14, 15, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "actionrewardlearn": 51, "output_gener": [], "abstract": [0, 3, 4, 5, 40, 54, 58, 59, 61], "defin": [5, 9, 14, 15, 40, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "The": [5, 14, 15, 40, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68], "should": [5, 14, 15, 40, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68], "specifi": [0, 3, 5, 6, 7, 8, 9, 10, 11, 14, 15, 40, 42, 43, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 72, 74, 75], "init": [5, 14, 15, 40, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68], "sample_multiple_act": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "candidate_act": [40, 42, 54, 56, 58, 60, 61, 63, 65, 68, 69, 70, 71, 73], "return_action_typ": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "idx": [5, 14, 15, 40, 41, 42, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "n_actions_for_each": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "replac": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "kwarg": [0, 5, 8, 13, 14, 15, 40, 41, 42, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "multipl": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "ontext": [54, 61], "g": [0, 5, 13, 14, 15, 40, 41, 42, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "demograph": [40, 42, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "origin": [40, 42, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "draw": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "indic": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "input": [0, 5, 12, 14, 15, 40, 41, 42, 43, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76], "sample_act": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "calc_gradi": [13, 14, 15, 40, 42, 54, 55, 56, 57, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71], "onehot": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "sample_action_and_output_prob": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "is_log_prob": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "prob": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "abov": [5, 14, 15, 40, 42, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], "calc_action_choice_prob": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "calc_prob_given_act": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "action_choice_prob": [40, 42, 54, 58, 61, 63, 69, 70, 71, 72, 73], "predict_policy_valu": [40, 42, 54, 55, 61, 64, 69, 70, 71, 72, 73], "predicted_reward": [36, 40, 42, 54, 55, 61, 64, 69, 70, 71, 72, 73], "reward_predictor": [40, 42, 54, 55, 61, 64, 69, 70, 71, 72, 73], "return_per_sampl": [40, 42, 54, 55, 61, 64, 69, 70, 71, 72, 73], "logit": [40, 42, 54, 55, 57, 61, 64, 66, 69, 70, 71, 72, 73], "per": [40, 42, 54, 55, 61, 64, 69, 70, 71, 72, 73], "take": [0, 5, 40, 42, 54, 55, 61, 64, 69, 70, 71, 72, 73], "policy_valu": [40, 42, 54, 55, 61, 64, 69, 70, 71, 72, 73], "multi": [5, 14, 15, 40, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "head": [40, 54, 55, 57], "__call__": [0, 5, 13, 14, 15, 40, 41, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68], "calc_logit": [40, 54, 55, 57, 64, 66], "action_logit": [40, 54, 55, 57, 64, 66], "some": [5, 6, 7, 14, 15, 40, 43, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 75], "gumbl": [40, 54, 55, 57, 64, 66], "softmax": [40, 42, 54, 55, 57, 64, 66, 69, 71], "refer": [5, 14, 15, 40, 41, 42, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73], "due": [40, 41, 42, 54, 57, 62, 66, 69, 70, 71, 73], "low": [40, 41, 42, 54, 57, 58, 63, 66, 69, 70, 71], "predict_valu": [40, 54, 56, 60, 65, 68], "generated_output": [], "output_valu": [54, 60, 68], "n_sentences_to_approxim": [40, 54, 60, 68], "frozen_llm": [6, 7, 36, 37, 38, 39, 40, 41, 54, 60, 68], "basefrozenllm": [0, 6, 7, 8, 36, 37, 38, 39, 40, 41, 54, 60, 68], "frozen": [0, 3, 5, 6, 7, 8, 12, 13, 15, 36, 37, 38, 39, 40, 41, 43, 54, 60, 68, 74, 75], "llm": [0, 3, 5, 6, 7, 8, 12, 13, 15, 36, 37, 38, 39, 40, 41, 43, 54, 60, 68, 74, 75], "action_valu": [40, 54, 56, 60, 65, 68], "n_candidate_act": [40, 42, 54, 56, 60, 65, 68, 69, 73], "embeddind": [40, 41, 54, 56, 58, 59, 62, 67], "sample_clust": [40, 41, 54, 58, 62, 63], "retrieve_clust": [40, 41, 54, 58, 62, 63], "resample_clust": [40, 41, 54, 58, 62, 63], "retriev": [40, 41, 42, 54, 58, 62, 63, 69, 70], "observ": [40, 41, 54, 58, 59, 62, 63, 67], "n_subsampl": [40, 41, 54, 58, 62, 63], "subsampl": [40, 41, 54, 58, 62, 63], "resampl": [40, 54, 58, 63], "belong": [40, 41, 54, 58, 62, 63], "retrieve_cluster_cent": [40, 54, 58, 63], "retrieve_candidate_actions_for_all_clust": [40, 54, 58, 63], "nest": [5, 14, 15, 40, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "retrieve_candidate_act": [40, 54, 58, 63], "calc_cluster_choice_prob": [40, 54, 58, 63], "cluster_choice_prob": [40, 54, 58, 63], "calc_cluster_vari": [40, 54, 58, 63], "std": [40, 54, 58, 63], "invers": [40, 42, 54, 58, 63, 69, 71], "add_modul": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "name": [5, 12, 14, 15, 43, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 76], "modul": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "add": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "child": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "current": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "access": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "ad": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "appli": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "fn": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "recurs": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "everi": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "submodul": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "children": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "well": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "self": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "typic": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "initi": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "also": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "nn": [5, 14, 15, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68], "doc": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "no_grad": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "def": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "init_weight": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "print": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "linear": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "fill_": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "net": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "sequenti": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "in_featur": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "out_featur": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "requires_grad": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "bfloat16": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "cast": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "point": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "buffer": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "datatyp": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "modifi": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "place": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "iter": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "over": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "yield": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "otherwis": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "ar": [0, 5, 14, 15, 40, 41, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68], "direct": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "member": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "xdoctest": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "skip": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "undefin": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "var": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "buf": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "20l": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "1l": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "5l": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "immedi": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "compil": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "arg": [0, 5, 13, 14, 15, 41, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "forward": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "pass": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "cpu": [5, 6, 7, 13, 14, 15, 40, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "move": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "gpu": [5, 8, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "make": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "associ": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "differ": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "object": [5, 14, 15, 43, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 76], "so": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "call": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "construct": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "live": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "while": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "being": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "doubl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "eval": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "mode": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "ha": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "effect": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "certain": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "document": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "particular": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "behavior": [5, 14, 15, 35, 42, 44, 45, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69], "thei": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "affect": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "dropout": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "batchnorm": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "equival": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "local": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "disabl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "grad": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "comparison": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "sever": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "similar": [5, 14, 15, 41, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68], "mechan": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "mai": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "confus": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "extra_repr": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "extra": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "represent": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "To": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "custom": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "inform": [0, 5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "re": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "implement": [5, 8, 13, 14, 15, 41, 42, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69], "own": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "both": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "line": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "string": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "accept": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "get_buff": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "target": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "exist": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "throw": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "docstr": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "get_submodul": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "more": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "explan": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "how": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "correctli": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "fulli": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "qualifi": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "look": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "referenc": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "rais": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "attributeerror": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "invalid": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "resolv": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "someth": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "get_extra_st": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "state_dict": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "correspond": [5, 6, 7, 14, 15, 43, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 77], "set_extra_st": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "need": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "store": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "build": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "picklabl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "ensur": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "serial": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "we": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "provid": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "backward": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "compat": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "guarante": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "other": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "break": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "pickl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "form": [5, 14, 15, 42, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 72], "chang": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "get_paramet": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "let": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "sai": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "have": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "like": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "net_b": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "net_c": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "conv": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "conv2d": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "16": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "33": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "kernel_s": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "3": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "stride": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "200": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "diagram": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "show": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "itself": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "check": [5, 14, 15, 43, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 75, 76], "would": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "runtim": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "bound": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "degre": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "against": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "named_modul": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "achiev": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "same": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "result": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "transit": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "simpl": [5, 13, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "alwai": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "half": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "ipu": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "load_state_dict": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "strict": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "assign": [5, 14, 15, 40, 41, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68], "descend": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "exactli": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "match": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "after": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "persist": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "strictli": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "enforc": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "item": [0, 5, 6, 7, 9, 11, 13, 14, 15, 36, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "instead": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "them": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "inplac": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "properti": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "preserv": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "missing_kei": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "miss": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "unexpected_kei": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "unexpect": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "namedtupl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "field": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "regist": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "runtimeerror": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "network": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71], "duplic": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "onc": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "In": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "l": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "enumer": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "named_buff": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "prefix": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "remove_dupl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "prepend": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "remov": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "tupl": [5, 14, 15, 43, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 76], "running_var": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "named_children": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "conv4": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "conv5": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "memo": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "alreadi": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "named_paramet": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "param": [5, 13, 14, 15, 41, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_backward_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "deprec": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "favor": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_full_backward_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "futur": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "version": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "util": [5, 14, 15, 36, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "removablehandl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_buff": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "consid": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "running_mean": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "alongsid": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "non": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "latter": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "oper": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "run": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "ignor": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "zero": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "num_featur": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_forward_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "with_kwarg": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "always_cal": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "time": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "comput": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "posit": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "keyword": [5, 14, 15, 40, 42, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "won": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "t": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "It": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "sinc": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "signatur": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "possibli": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "callabl": [5, 14, 15, 40, 41, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68], "fire": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "global": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_module_forward_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "regardless": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "except": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_forward_pre_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "invok": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "wrap": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "unless": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "And": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "forward_pr": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_module_forward_pre_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "respect": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "execut": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "grad_input": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "grad_output": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "new": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "subsequ": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "entri": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "technic": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "reason": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "receiv": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "view": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "similarli": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "caller": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "allow": [5, 14, 15, 43, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 76], "register_module_full_backward_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_full_backward_pre_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "backward_pr": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_module_full_backward_pre_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_load_state_dict_post_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "post": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "incompatible_kei": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "perform": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "modif": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "addit": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "thrown": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "clear": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "out": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "avoid": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_modul": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_paramet": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "register_state_dict_pre_hook": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "These": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "keep_var": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "process": [5, 9, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "made": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "requires_grad_": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "autograd": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "help": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "freez": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "finetun": [5, 13, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "individu": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "gan": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "found": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "share_memori": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "share_memory_": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "destin": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "whole": [5, 12, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "averag": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "shallow": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "order": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "howev": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "design": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "end": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "updat": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "ordereddict": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "compos": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "detach": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "dtype": [5, 14, 15, 43, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 74, 76], "non_block": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "memory_format": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "channels_last": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "Its": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "complex": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "integr": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "unchang": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "tri": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "convert": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "asynchron": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "host": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "possibl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "pin": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "memori": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "desir": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "whose": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "format": [0, 4, 5, 12, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "4d": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "ignore_w": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "determinist": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "1913": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "3420": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "5113": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "2325": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "float64": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "requir": [5, 14, 15, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "torch_doctest_cuda1": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "gpu1": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "1914": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "5112": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "2324": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "float16": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "cdoubl": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "3741": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "j": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "2382": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "5593": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "4443": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "complex128": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "ones": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "6122": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "1150": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "to_empti": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "without": [5, 12, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "storag": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "dst_type": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "xpu": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "zero_grad": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "set_to_non": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "reset": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "under": [5, 14, 15, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68], "neuralactionpolici": 62, "linear_hidden_dim": [41, 62, 64, 65, 66, 67, 68], "hidden": [41, 62, 64, 65, 66, 67, 68], "layer": [41, 62, 64, 65, 66, 67, 68], "mlp": [41, 62, 64, 65, 66, 67, 68], "neuralclusterpolici": 62, "dim_feature_emb": [62, 66], "neuraloutputrewardpredictor": 62, "baseoutputrewardpredictor": [62, 68], "neuralactionrewardpredictor": 62, "fixedclust": [], "cosin": [], "fpr": [41, 62, 63], "actionsimilaritybasedclust": [], "outputsimilaritybasedclust": [], "first_stage_polici": [42, 69, 72], "is_reward_based_first_stage_polici": [42, 69, 72], "wai": [42, 69, 72], "space": [42, 69, 72], "adopt": [42, 69, 72], "clustering_logging_predicted_reward": [42, 69, 72], "adaptivetwostagepolici": [], "clustering_logging_polici": [], "softmaxpolici": [42, 69], "is_first_stage_polici": [42, 69, 70, 71, 73], "base_model": [13, 15, 41, 42, 69, 70, 71], "beta": [42, 69, 71], "neural": [69, 70, 71], "temperatur": [42, 69, 71], "hyperparamet": [42, 69, 70, 71], "epsilongreedypolici": [42, 69], "epsilon": [42, 69, 70], "greedi": [42, 69, 70], "explor": [42, 69, 70], "greedy_act": [42, 69, 70], "greedili": [42, 69, 70], "uniformrandompolici": [42, 69], "uniform": [42, 43, 69, 73, 74, 80], "tool": [43, 74], "torch_se": [43, 74], "pytorch": [43, 74, 79], "defaultdict_to_dict": [43, 74], "dict_": [43, 74, 77], "transform": [13, 15, 41, 43, 74, 77], "defaultdict": [43, 74, 77], "check_tensor": [43, 74], "expected_dim": [43, 74, 76], "expected_dtyp": [43, 74, 76], "min_val": [43, 74, 76], "max_val": [43, 74, 76], "minimum": [43, 74, 76], "maximum": [8, 43, 74, 76], "check_logged_feedback": [43, 74], "propmpt": [43, 74, 75], "generate_output_sent": [0, 3, 8], "enumerate_prompt": [], "pair": [], "text": [], "baserewardsimul": [0, 6, 7, 13, 15], "semisyntheticdataset": [6, 35, 37, 38, 39], "context_query_load": [6, 7], "candidate_actions_load": [6, 7], "path_to_dataset": [], "asset": [6, 7, 9, 10, 11], "recipe_preprocessed_recipe_dataset": [], "csv": [6, 7, 9, 10, 11], "path_to_candidate_prompt": [6, 7, 9, 10], "recipe_benchmark_prompt": [], "path_to_finetuned_param": [6, 7], "recipe_finetuned_reward_simul": [], "pt": [6, 7, 9, 11], "contextqueryload": [6, 7], "loader": [0, 1, 2, 6, 7], "candidateactionsload": [6, 7], "preprocessed_recipe_dataset": [], "file": [6, 7, 9, 10, 11], "vocabulari": [6, 7, 9, 10], "fine": [6, 7, 13], "tune": [6, 7, 13], "is_test": [6, 7, 9, 11], "llamafrozenllm": [], "model_id": [], "llama": [], "7b": [6, 7], "chat": [], "hf": [], "prompt_templ": [], "format_prompt": [], "auto": [], "recip": 8, "task": 8, "templat": [], "max_word": [], "256": [], "concaten": [], "context_column": [], "query_column": [], "context_templ": [], "format_context": [], "query_templ": [], "format_queri": [], "test_ratio": [9, 11], "shuffl": [9, 11], "column": [], "descriot": [], "descript": [12, 36], "ingredi": [], "ratio": [9, 11], "split": [9, 11], "transformerrewardsimul": 13, "token": [0, 3, 4, 5, 6, 7, 8, 12, 13, 15, 40, 41, 42, 43], "tokenizer_kwarg": [8, 12, 13, 15, 41, 43], "input_templ": [], "format_inputs_for_reward_simul": [], "pretrainedmodel": [8, 13, 15, 41], "distillbert": [13, 15, 41], "pretrainedtoken": [8, 13, 15, 41], "pretrainedtokenizerfast": [8, 13, 15, 41], "distillberttokenizerfast": [13, 15, 41], "max_length": [13, 15, 41], "functtion": [], "finetune_model": [], "expeect": [13, 14, 15], "basepromptformatt": [0, 6, 7, 8, 12], "format_token": [0, 4, 12], "basecandidateactionsload": [0, 9, 10], "basecandidateactionload": [0, 1], "basecontextqueryload": [0, 9, 11], "qurei": [0, 3, 8], "movielen": [0, 5], "user_id": [0, 5, 6, 7, 9, 11, 13, 14, 15], "item_id": [0, 5, 6, 7, 9, 11, 13, 14, 15], "frozen_llm_prompt_formatt": [6, 7], "path_to_user_embed": [6, 7, 9, 11], "movielens_transformer_user_embed": [6, 7, 9, 11], "path_to_queri": [6, 7, 9, 11], "movielens_queri": [6, 7, 9, 11], "movielens_benchmark_prompt": [6, 7, 9, 10], "movielens_distilbert_reward_simul": [6, 7], "frozen_llm_base_model_id": [6, 7], "mistralai": [6, 7], "mistral": [6, 7], "instruct": [6, 7], "v0": [6, 7], "frozen_llm_base_tokenizer_id": [6, 7], "frozen_llm_use_tokenizer_fast": [6, 7], "frozen_llm_pattern": [6, 7], "reward_simulator_base_model_id": [6, 7], "distilbert": [6, 7], "uncas": [6, 7], "reward_simulator_base_tokenizer_id": [6, 7], "reward_simulator_use_tokenizer_fast": [6, 7], "formatt": [6, 7, 8, 36], "tokenizerfast": [6, 7], "unnecessari": [6, 7], "pattern": [6, 7, 8], "appear": [6, 7], "return_user_id": [6, 7], "return_item_id": [6, 7], "return_context": [6, 7], "return_queri": [6, 7], "return_cpu_tensor": [6, 7, 13, 14, 15, 40], "autofrozenllm": 8, "prompt_formatt": 8, "max_new_token": 8, "30": 8, "128": 8, "length": 8, "defaultcandidateactionsload": 9, "defaultcontextqueryload": 9, "movielenspromptformatt": 12, "prefix_prompt": 12, "broadli": 12, "describ": 12, "genr": 12, "movi": 12, "specif": 12, "ntitl": 12, "mid_prompt": 12, "nkeyword": 12, "postfix_prompt": 12, "nmovi": 12, "query_token": 12, "prompt_token": 12, "concat_token": 12, "combin": 12, "system": 12, "collaborativefilteringrewardsimul": 13, "n_user": [13, 14, 15], "n_item": [13, 14, 15], "dim_emb": [13, 14, 15, 40, 41], "hidden_dim": [13, 14, 15], "collabor": [13, 14], "filter": [13, 14], "is_finetun": [13, 15], "basepromptpolicymodel": [35, 38, 40, 41], "query_encod": [35, 36, 37, 38, 39, 41], "baseencod": [35, 36, 37, 38, 39, 40, 41], "encod": [35, 36, 37, 38, 39, 40, 41], "torchloggeddataset": 36, "logging_marginal_dens": 36, "prompt_encod": [36, 39, 41], "sentence_encod": [36, 37, 38, 39, 41], "fit": [36, 41], "dataload": 36, "margin": [36, 37, 40, 41, 46, 47, 48, 49, 54, 59, 62, 67], "densiti": [37, 40, 41, 46, 47, 48, 49, 54, 59, 62, 67], "marginaldensitylearn": [37, 46], "basekernelmarginaldensitymodel": [37, 38, 40, 41, 46, 47, 48, 49, 54, 62, 67], "estim": [37, 46, 47, 48, 49], "simulation_train": [37, 46, 47], "prompt_reward_predictor": 38, "basepromptrewardmodel": [38, 39, 40, 41, 42], "n_epochs_per_log": [38, 48, 49, 50], "interv": [38, 48, 49, 50], "kernelpolicylearn": [38, 48], "kernel_marginal_estim": [38, 48, 49], "sentence_reward_predictor": 38, "basesentencerewardmodel": [38, 39, 40, 41], "kernel": [38, 40, 41, 43, 48, 49, 54, 59, 62, 67, 74, 78, 80], "sentence_reward_oredictor": 38, "dim_cluster_emb": [40, 42], "dim_sent": 40, "sentence_valu": 40, "query_for_frozen_llm": 40, "kernel_funct": [40, 41, 54, 59, 62, 67], "gaussian_kernel": [40, 41, 43, 54, 59, 62, 67, 74], "kernel_kwarg": [40, 41, 54, 59, 62, 67], "tau": [40, 41, 43, 54, 59, 62, 67, 74, 78, 80], "calc_pairwise_dist": [40, 41, 54, 59, 62, 67], "pivot_sent": [40, 41], "sampled_sent": [40, 41], "pairwis": [40, 41, 54, 59, 62, 67], "distanc": [40, 41, 43, 54, 59, 62, 67, 74, 78, 80], "pivot": [40, 41, 54, 59, 62, 67], "pairwise_dist": [40, 41, 54, 59, 62, 67], "calc_pairwise_weight": [40, 54, 59, 67], "pairwise_weight": [40, 54, 59, 67], "estimate_marginal_dens": [40, 54, 59, 67], "marginal_dens": [40, 54, 59, 67], "transformerencod": 41, "fit_pca": 41, "pca": 41, "high": 41, "promptpolici": 41, "clusterpolici": 41, "cluster_center_encod": 41, "sentencerewardpredictor": 41, "promptrewardpredictor": 41, "kernelmarginaldensityestim": 41, "l2": [41, 62, 67], "norm": [41, 62, 67], "transformermarginaldensityestim": 41, "n_samles_to_approxim": 41, "kmeanspromptclust": 41, "basepolicymodel": 42, "frozenllmdataset": 43, "to_devic": 43, "transfer": 43, "uniform_kernel": [43, 74], "use_monte_carlo": [48, 49, 54, 59, 67], "mont": [48, 49, 54, 59, 67], "carlo": [48, 49, 54, 59, 67], "pivot_output": [54, 59, 62, 67], "sampled_output": [54, 59, 62, 67], "neuralmarginaldensityestim": 62, "emb_nois": [62, 67], "magnitud": [62, 67], "explicit": [62, 67], "ablat": [62, 67], "kmeansactionclust": 62}, "objects": {"src.dataset": [[0, 0, 0, "-", "base"], [6, 0, 0, "-", "benchmark"], [8, 0, 0, "-", "frozen_llm"], [9, 0, 0, "-", "function"], [12, 0, 0, "-", "prompt_formatter"], [13, 0, 0, "-", "reward_simulator"]], "src.dataset.base": [[1, 1, 1, "", "BaseCandidateActionsLoader"], [2, 1, 1, "", "BaseContextQueryLoader"], [3, 1, 1, "", "BaseFrozenLLM"], [4, 1, 1, "", "BasePromptFormatter"], [5, 1, 1, "", "BaseRewardSimulator"]], "src.dataset.base.BaseFrozenLLM": [[3, 2, 1, "", "generate_output_sentence"]], "src.dataset.base.BasePromptFormatter": [[4, 2, 1, "", "format_tokens"]], "src.dataset.base.BaseRewardSimulator": [[5, 2, 1, "", "add_module"], [5, 2, 1, "", "apply"], [5, 2, 1, "", "bfloat16"], [5, 2, 1, "", "buffers"], [5, 2, 1, "", "calc_expected_reward"], [5, 2, 1, "", "children"], [5, 2, 1, "", "compile"], [5, 2, 1, "", "cpu"], [5, 2, 1, "", "cuda"], [5, 2, 1, "", "double"], [5, 2, 1, "", "eval"], [5, 2, 1, "", "extra_repr"], [5, 2, 1, "", "float"], [5, 2, 1, "", "get_buffer"], [5, 2, 1, "", "get_extra_state"], [5, 2, 1, "", "get_parameter"], [5, 2, 1, "", "get_submodule"], [5, 2, 1, "", "half"], [5, 2, 1, "", "ipu"], [5, 2, 1, "", "load_state_dict"], [5, 2, 1, "", "modules"], [5, 2, 1, "", "named_buffers"], [5, 2, 1, "", "named_children"], [5, 2, 1, "", "named_modules"], [5, 2, 1, "", "named_parameters"], [5, 2, 1, "", "parameters"], [5, 2, 1, "", "register_backward_hook"], [5, 2, 1, "", "register_buffer"], [5, 2, 1, "", "register_forward_hook"], [5, 2, 1, "", "register_forward_pre_hook"], [5, 2, 1, "", "register_full_backward_hook"], [5, 2, 1, "", "register_full_backward_pre_hook"], [5, 2, 1, "", "register_load_state_dict_post_hook"], [5, 2, 1, "", "register_module"], [5, 2, 1, "", "register_parameter"], [5, 2, 1, "", "register_state_dict_pre_hook"], [5, 2, 1, "", "requires_grad_"], [5, 2, 1, "", "set_extra_state"], [5, 2, 1, "", "share_memory"], [5, 2, 1, "", "state_dict"], [5, 2, 1, "", "to"], [5, 2, 1, "", "to_empty"], [5, 2, 1, "", "train"], [5, 2, 1, "", "type"], [5, 2, 1, "", "xpu"], [5, 2, 1, "", "zero_grad"]], "src.dataset.benchmark": [[7, 1, 1, "", "SemiSyntheticDataset"]], "src.dataset.benchmark.SemiSyntheticDataset": [[7, 2, 1, "", "calc_expected_policy_value"], [7, 2, 1, "", "sample_dataset"], [7, 2, 1, "", "sample_reward_given_action"], [7, 2, 1, "", "sample_reward_given_output"]], "src.dataset.frozen_llm": [[8, 1, 1, "", "AutoFrozenLLM"]], "src.dataset.frozen_llm.AutoFrozenLLM": [[8, 2, 1, "", "generate_output_sentence"]], "src.dataset.function": [[10, 1, 1, "", "DefaultCandidateActionsLoader"], [11, 1, 1, "", "DefaultContextQueryLoader"]], "src.dataset.function.DefaultContextQueryLoader": [[11, 2, 1, "", "sample_context_and_query"]], "src.dataset.prompt_formatter": [[12, 1, 1, "", "MovielensPromptFormatter"]], "src.dataset.prompt_formatter.MovielensPromptFormatter": [[12, 2, 1, "", "format_tokens"]], "src.dataset.reward_simulator": [[14, 1, 1, "", "CollaborativeFilteringRewardSimulator"], [15, 1, 1, "", "TransformerRewardSimulator"]], "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator": [[14, 2, 1, "", "add_module"], [14, 2, 1, "", "apply"], [14, 2, 1, "", "bfloat16"], [14, 2, 1, "", "buffers"], [14, 2, 1, "", "calc_expected_reward"], [14, 2, 1, "", "children"], [14, 2, 1, "", "compile"], [14, 2, 1, "", "cpu"], [14, 2, 1, "", "cuda"], [14, 2, 1, "", "double"], [14, 2, 1, "", "eval"], [14, 2, 1, "", "extra_repr"], [14, 2, 1, "", "float"], [14, 2, 1, "", "get_buffer"], [14, 2, 1, "", "get_extra_state"], [14, 2, 1, "", "get_parameter"], [14, 2, 1, "", "get_submodule"], [14, 2, 1, "", "half"], [14, 2, 1, "", "ipu"], [14, 2, 1, "", "load_state_dict"], [14, 2, 1, "", "modules"], [14, 2, 1, "", "named_buffers"], [14, 2, 1, "", "named_children"], [14, 2, 1, "", "named_modules"], [14, 2, 1, "", "named_parameters"], [14, 2, 1, "", "parameters"], [14, 2, 1, "", "register_backward_hook"], [14, 2, 1, "", "register_buffer"], [14, 2, 1, "", "register_forward_hook"], [14, 2, 1, "", "register_forward_pre_hook"], [14, 2, 1, "", "register_full_backward_hook"], [14, 2, 1, "", "register_full_backward_pre_hook"], [14, 2, 1, "", "register_load_state_dict_post_hook"], [14, 2, 1, "", "register_module"], [14, 2, 1, "", "register_parameter"], [14, 2, 1, "", "register_state_dict_pre_hook"], [14, 2, 1, "", "requires_grad_"], [14, 2, 1, "", "set_extra_state"], [14, 2, 1, "", "share_memory"], [14, 2, 1, "", "state_dict"], [14, 2, 1, "", "to"], [14, 2, 1, "", "to_empty"], [14, 2, 1, "", "train"], [14, 2, 1, "", "type"], [14, 2, 1, "", "xpu"], [14, 2, 1, "", "zero_grad"]], "src.dataset.reward_simulator.TransformerRewardSimulator": [[15, 2, 1, "", "add_module"], [15, 2, 1, "", "apply"], [15, 2, 1, "", "bfloat16"], [15, 2, 1, "", "buffers"], [15, 2, 1, "", "calc_expected_reward"], [15, 2, 1, "", "children"], [15, 2, 1, "", "compile"], [15, 2, 1, "", "cpu"], [15, 2, 1, "", "cuda"], [15, 2, 1, "", "double"], [15, 2, 1, "", "eval"], [15, 2, 1, "", "extra_repr"], [15, 2, 1, "", "float"], [15, 2, 1, "", "get_buffer"], [15, 2, 1, "", "get_extra_state"], [15, 2, 1, "", "get_parameter"], [15, 2, 1, "", "get_submodule"], [15, 2, 1, "", "half"], [15, 2, 1, "", "ipu"], [15, 2, 1, "", "load_state_dict"], [15, 2, 1, "", "modules"], [15, 2, 1, "", "named_buffers"], [15, 2, 1, "", "named_children"], [15, 2, 1, "", "named_modules"], [15, 2, 1, "", "named_parameters"], [15, 2, 1, "", "parameters"], [15, 2, 1, "", "register_backward_hook"], [15, 2, 1, "", "register_buffer"], [15, 2, 1, "", "register_forward_hook"], [15, 2, 1, "", "register_forward_pre_hook"], [15, 2, 1, "", "register_full_backward_hook"], [15, 2, 1, "", "register_full_backward_pre_hook"], [15, 2, 1, "", "register_load_state_dict_post_hook"], [15, 2, 1, "", "register_module"], [15, 2, 1, "", "register_parameter"], [15, 2, 1, "", "register_state_dict_pre_hook"], [15, 2, 1, "", "requires_grad_"], [15, 2, 1, "", "set_extra_state"], [15, 2, 1, "", "share_memory"], [15, 2, 1, "", "state_dict"], [15, 2, 1, "", "to"], [15, 2, 1, "", "to_empty"], [15, 2, 1, "", "train"], [15, 2, 1, "", "type"], [15, 2, 1, "", "xpu"], [15, 2, 1, "", "zero_grad"]], "src.opl": [[35, 0, 0, "-", "behavior_cloning"], [36, 0, 0, "-", "dataset"], [37, 0, 0, "-", "marginal_learner"], [38, 0, 0, "-", "policy_learner"], [39, 0, 0, "-", "reward_learner"]], "src.opl.behavior_cloning": [[35, 1, 1, "", "BehaviorCloningLearner"]], "src.opl.behavior_cloning.BehaviorCloningLearner": [[35, 2, 1, "", "load"], [35, 2, 1, "", "offline_cloning"], [35, 2, 1, "", "online_cloning"], [35, 3, 1, "", "optimizer"], [35, 3, 1, "", "random_state"], [35, 2, 1, "", "save"], [35, 2, 1, "", "seed"]], "src.opl.dataset": [[36, 1, 1, "", "TorchLoggedDataset"]], "src.opl.marginal_learner": [[37, 1, 1, "", "MarginalDensityLearner"]], "src.opl.marginal_learner.MarginalDensityLearner": [[37, 2, 1, "", "load"], [37, 3, 1, "", "optimizer"], [37, 3, 1, "", "random_state"], [37, 2, 1, "", "save"], [37, 2, 1, "", "seed"], [37, 2, 1, "", "simulation_training"]], "src.opl.policy_learner": [[38, 1, 1, "", "KernelPolicyLearner"], [38, 1, 1, "", "PolicyLearner"]], "src.opl.policy_learner.KernelPolicyLearner": [[38, 2, 1, "", "hybrid_policy_gradient"], [38, 2, 1, "", "importance_sampling_based_policy_gradient"], [38, 2, 1, "", "load"], [38, 3, 1, "", "optimizer"], [38, 3, 1, "", "random_state"], [38, 2, 1, "", "save"], [38, 2, 1, "", "seed"]], "src.opl.policy_learner.PolicyLearner": [[38, 2, 1, "", "hybrid_policy_gradient"], [38, 2, 1, "", "importance_sampling_based_policy_gradient"], [38, 2, 1, "", "load"], [38, 2, 1, "", "model_based_policy_gradient"], [38, 2, 1, "", "online_policy_gradient"], [38, 3, 1, "", "optimizer"], [38, 3, 1, "", "random_state"], [38, 2, 1, "", "save"], [38, 2, 1, "", "seed"]], "src.opl.reward_learner": [[39, 1, 1, "", "PromptRewardLearner"], [39, 1, 1, "", "SentenceRewardLearner"]], "src.opl.reward_learner.PromptRewardLearner": [[39, 2, 1, "", "load"], [39, 2, 1, "", "offline_cloning"], [39, 2, 1, "", "offline_training"], [39, 2, 1, "", "online_cloning"], [39, 2, 1, "", "online_training"], [39, 3, 1, "", "optimizer"], [39, 3, 1, "", "random_state"], [39, 2, 1, "", "save"], [39, 2, 1, "", "seed"]], "src.opl.reward_learner.SentenceRewardLearner": [[39, 2, 1, "", "load"], [39, 2, 1, "", "offline_training"], [39, 2, 1, "", "online_training"], [39, 3, 1, "", "optimizer"], [39, 3, 1, "", "random_state"], [39, 2, 1, "", "save"], [39, 2, 1, "", "seed"]], "src.policy": [[40, 0, 0, "-", "base"], [41, 0, 0, "-", "model"], [42, 0, 0, "-", "policy"]], "src.policy.base": [[40, 1, 1, "", "BaseClusterPolicyModel"], [40, 1, 1, "", "BaseClusteringModel"], [40, 1, 1, "", "BaseEncoder"], [40, 1, 1, "", "BaseKernelMarginalDensityModel"], [40, 1, 1, "", "BasePolicy"], [40, 1, 1, "", "BasePromptPolicyModel"], [40, 1, 1, "", "BasePromptRewardModel"], [40, 1, 1, "", "BaseSentenceRewardModel"]], "src.policy.base.BaseClusterPolicyModel": [[40, 2, 1, "", "calc_action_choice_probability"], [40, 2, 1, "", "calc_logits"], [40, 2, 1, "", "calc_prob_given_action"], [40, 2, 1, "", "sample_action"], [40, 2, 1, "", "sample_action_and_output_prob"], [40, 2, 1, "", "sample_multiple_actions"]], "src.policy.base.BaseClusteringModel": [[40, 2, 1, "", "calc_cluster_choice_prob"], [40, 2, 1, "", "calc_cluster_variance"], [40, 2, 1, "", "retrieve_candidate_actions"], [40, 2, 1, "", "retrieve_candidate_actions_for_all_clusters"], [40, 2, 1, "", "retrieve_cluster"], [40, 2, 1, "", "retrieve_cluster_center"], [40, 2, 1, "", "retrieve_cluster_centers"], [40, 2, 1, "", "sample_clustering"]], "src.policy.base.BaseEncoder": [[40, 2, 1, "", "encode"]], "src.policy.base.BaseKernelMarginalDensityModel": [[40, 2, 1, "", "calc_pairwise_distance"], [40, 2, 1, "", "calc_pairwise_weight"], [40, 2, 1, "", "estimate_marginal_density"]], "src.policy.base.BasePolicy": [[40, 2, 1, "", "calc_action_choice_probability"], [40, 2, 1, "", "calc_prob_given_action"], [40, 2, 1, "", "predict_policy_value"], [40, 2, 1, "", "sample_action"], [40, 2, 1, "", "sample_action_and_output_prob"], [40, 2, 1, "", "sample_multiple_actions"]], "src.policy.base.BasePromptPolicyModel": [[40, 2, 1, "", "calc_action_choice_probability"], [40, 2, 1, "", "calc_logits"], [40, 2, 1, "", "calc_prob_given_action"], [40, 2, 1, "", "predict_policy_value"], [40, 2, 1, "", "sample_action"], [40, 2, 1, "", "sample_action_and_output_prob"], [40, 2, 1, "", "sample_multiple_actions"]], "src.policy.base.BasePromptRewardModel": [[40, 2, 1, "", "predict_value"], [40, 2, 1, "", "predict_values"]], "src.policy.base.BaseSentenceRewardModel": [[40, 2, 1, "", "predict_value"], [40, 2, 1, "", "predict_values"]], "src.policy.model": [[41, 1, 1, "", "ClusterPolicy"], [41, 1, 1, "", "KernelMarginalDensityEstimator"], [41, 1, 1, "", "KmeansPromptClustering"], [41, 1, 1, "", "PromptPolicy"], [41, 1, 1, "", "PromptRewardPredictor"], [41, 1, 1, "", "SentenceRewardPredictor"], [41, 1, 1, "", "TransformerEncoder"]], "src.policy.model.KernelMarginalDensityEstimator": [[41, 2, 1, "", "calc_pairwise_distance"]], "src.policy.model.KmeansPromptClustering": [[41, 2, 1, "", "retrieve_cluster"], [41, 2, 1, "", "sample_clustering"]], "src.policy.model.TransformerEncoder": [[41, 2, 1, "", "encode"], [41, 2, 1, "", "fit_pca"]], "src.policy.policy": [[42, 1, 1, "", "EpsilonGreedyPolicy"], [42, 1, 1, "", "SoftmaxPolicy"], [42, 1, 1, "", "TwoStagePolicy"], [42, 1, 1, "", "UniformRandomPolicy"]], "src.policy.policy.EpsilonGreedyPolicy": [[42, 2, 1, "", "calc_action_choice_probability"], [42, 2, 1, "", "calc_prob_given_action"], [42, 2, 1, "", "greedy_action"], [42, 2, 1, "", "predict_policy_value"], [42, 2, 1, "", "sample_action"], [42, 2, 1, "", "sample_action_and_output_prob"], [42, 2, 1, "", "sample_multiple_actions"]], "src.policy.policy.SoftmaxPolicy": [[42, 2, 1, "", "calc_action_choice_probability"], [42, 2, 1, "", "calc_prob_given_action"], [42, 2, 1, "", "predict_policy_value"], [42, 2, 1, "", "sample_action"], [42, 2, 1, "", "sample_action_and_output_prob"], [42, 2, 1, "", "sample_multiple_actions"]], "src.policy.policy.TwoStagePolicy": [[42, 2, 1, "", "calc_action_choice_probability"], [42, 2, 1, "", "calc_prob_given_action"], [42, 2, 1, "", "predict_policy_value"], [42, 2, 1, "", "sample_action"], [42, 2, 1, "", "sample_action_and_output_prob"], [42, 2, 1, "", "sample_multiple_actions"]], "src.policy.policy.UniformRandomPolicy": [[42, 2, 1, "", "calc_action_choice_probability"], [42, 2, 1, "", "calc_prob_given_action"], [42, 2, 1, "", "predict_policy_value"], [42, 2, 1, "", "sample_action"], [42, 2, 1, "", "sample_action_and_output_prob"], [42, 2, 1, "", "sample_multiple_actions"]], "src": [[43, 0, 0, "-", "utils"]], "src.utils": [[43, 1, 1, "", "FrozenLLMDataset"], [43, 4, 1, "", "check_logged_feedback"], [43, 4, 1, "", "check_tensor"], [43, 4, 1, "", "defaultdict_to_dict"], [43, 4, 1, "", "gaussian_kernel"], [43, 4, 1, "", "to_device"], [43, 4, 1, "", "tokenize"], [43, 4, 1, "", "torch_seed"], [43, 4, 1, "", "uniform_kernel"]], "toy.dataset": [[16, 0, 0, "-", "function"], [33, 0, 0, "-", "synthetic"]], "toy.dataset.function": [[17, 1, 1, "", "AuxiliaryOutputGenerator"], [18, 1, 1, "", "CandidateActionsGenerator"], [19, 1, 1, "", "ConfoundedAuxiliaryOutputGenerator"], [20, 1, 1, "", "ConfoundedExponentialAuxiliaryOutputGenerator"], [21, 1, 1, "", "ConfoundedPowerAuxiliaryOutputGenerator"], [22, 1, 1, "", "ConfoundedRationalAuxiliaryOutputGenerator"], [23, 1, 1, "", "ConfoundedTrigonometricAuxiliaryOutputGenerator"], [24, 1, 1, "", "ContextQueryGenerator"], [25, 1, 1, "", "ExponentialAuxiliaryOutputGenerator"], [26, 1, 1, "", "MixtureOfGaussianCandidateActionsGenerator"], [27, 1, 1, "", "PowerAuxiliaryOutputGenerator"], [28, 1, 1, "", "RationalAuxiliaryOutputGenerator"], [29, 1, 1, "", "RewardSimulator"], [30, 1, 1, "", "SigmoidAuxiliaryOutputGenerator"], [31, 1, 1, "", "SparseRewardSimulator"], [32, 1, 1, "", "TrigonometricAuxiliaryOutputGenerator"]], "toy.dataset.function.AuxiliaryOutputGenerator": [[17, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.ConfoundedAuxiliaryOutputGenerator": [[19, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.ConfoundedExponentialAuxiliaryOutputGenerator": [[20, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.ConfoundedPowerAuxiliaryOutputGenerator": [[21, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.ConfoundedRationalAuxiliaryOutputGenerator": [[22, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.ConfoundedTrigonometricAuxiliaryOutputGenerator": [[23, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.ContextQueryGenerator": [[24, 2, 1, "", "sample_context_and_query"]], "toy.dataset.function.ExponentialAuxiliaryOutputGenerator": [[25, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.PowerAuxiliaryOutputGenerator": [[27, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.RationalAuxiliaryOutputGenerator": [[28, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.RewardSimulator": [[29, 2, 1, "", "calc_expected_reward"]], "toy.dataset.function.SigmoidAuxiliaryOutputGenerator": [[30, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.function.SparseRewardSimulator": [[31, 2, 1, "", "calc_expected_reward"]], "toy.dataset.function.TrigonometricAuxiliaryOutputGenerator": [[32, 2, 1, "", "sample_auxiliary_output"]], "toy.dataset.synthetic": [[34, 1, 1, "", "SyntheticDataset"]], "toy.dataset.synthetic.SyntheticDataset": [[34, 2, 1, "", "calc_expected_policy_value"], [34, 2, 1, "", "calc_expected_reward_for_all_actions"], [34, 2, 1, "", "calc_expected_reward_given_action"], [34, 2, 1, "", "sample_dataset"], [34, 2, 1, "", "sample_reward_given_action"], [34, 2, 1, "", "sample_reward_given_output"]], "toy.opl": [[44, 0, 0, "-", "behavior_cloning"], [46, 0, 0, "-", "marginal_learner"], [48, 0, 0, "-", "policy_learner"], [51, 0, 0, "-", "reward_learner"]], "toy.opl.behavior_cloning": [[45, 1, 1, "", "BehaviorCloningLearner"]], "toy.opl.behavior_cloning.BehaviorCloningLearner": [[45, 2, 1, "", "load"], [45, 2, 1, "", "offline_cloning"], [45, 2, 1, "", "online_cloning"], [45, 3, 1, "", "optimizer"], [45, 3, 1, "", "random_state"], [45, 2, 1, "", "save"], [45, 2, 1, "", "seed"]], "toy.opl.marginal_learner": [[47, 1, 1, "", "MarginalDensityLearner"]], "toy.opl.marginal_learner.MarginalDensityLearner": [[47, 2, 1, "", "load"], [47, 3, 1, "", "optimizer"], [47, 3, 1, "", "random_state"], [47, 2, 1, "", "save"], [47, 2, 1, "", "seed"], [47, 2, 1, "", "simulation_training"]], "toy.opl.policy_learner": [[49, 1, 1, "", "KernelPolicyLearner"], [50, 1, 1, "", "PolicyLearner"]], "toy.opl.policy_learner.KernelPolicyLearner": [[49, 2, 1, "", "hybrid_policy_gradient"], [49, 2, 1, "", "importance_sampling_based_policy_gradient"], [49, 2, 1, "", "load"], [49, 3, 1, "", "optimizer"], [49, 3, 1, "", "random_state"], [49, 2, 1, "", "save"], [49, 2, 1, "", "seed"]], "toy.opl.policy_learner.PolicyLearner": [[50, 2, 1, "", "hybrid_policy_gradient"], [50, 2, 1, "", "importance_sampling_based_policy_gradient"], [50, 2, 1, "", "load"], [50, 2, 1, "", "model_based_policy_gradient"], [50, 2, 1, "", "online_policy_gradient"], [50, 3, 1, "", "optimizer"], [50, 3, 1, "", "random_state"], [50, 2, 1, "", "save"], [50, 2, 1, "", "seed"]], "toy.opl.reward_learner": [[52, 1, 1, "", "ActionRewardLearner"], [53, 1, 1, "", "OutputRewardLearner"]], "toy.opl.reward_learner.ActionRewardLearner": [[52, 2, 1, "", "load"], [52, 2, 1, "", "offline_cloning"], [52, 2, 1, "", "offline_training"], [52, 2, 1, "", "online_cloning"], [52, 2, 1, "", "online_training"], [52, 3, 1, "", "optimizer"], [52, 3, 1, "", "random_state"], [52, 2, 1, "", "save"], [52, 2, 1, "", "seed"]], "toy.opl.reward_learner.OutputRewardLearner": [[53, 2, 1, "", "load"], [53, 2, 1, "", "offline_training"], [53, 2, 1, "", "online_training"], [53, 3, 1, "", "optimizer"], [53, 3, 1, "", "random_state"], [53, 2, 1, "", "save"], [53, 2, 1, "", "seed"]], "toy.policy": [[54, 0, 0, "-", "base"], [62, 0, 0, "-", "model"], [69, 0, 0, "-", "policy"]], "toy.policy.base": [[55, 1, 1, "", "BaseActionPolicyModel"], [56, 1, 1, "", "BaseActionRewardModel"], [57, 1, 1, "", "BaseClusterPolicyModel"], [58, 1, 1, "", "BaseClusteringModel"], [59, 1, 1, "", "BaseKernelMarginalDensityModel"], [60, 1, 1, "", "BaseOutputRewardModel"], [61, 1, 1, "", "BasePolicy"]], "toy.policy.base.BaseActionPolicyModel": [[55, 2, 1, "", "add_module"], [55, 2, 1, "", "apply"], [55, 2, 1, "", "bfloat16"], [55, 2, 1, "", "buffers"], [55, 2, 1, "", "calc_action_choice_probability"], [55, 2, 1, "", "calc_logits"], [55, 2, 1, "", "calc_prob_given_action"], [55, 2, 1, "", "children"], [55, 2, 1, "", "compile"], [55, 2, 1, "", "cpu"], [55, 2, 1, "", "cuda"], [55, 2, 1, "", "double"], [55, 2, 1, "", "eval"], [55, 2, 1, "", "extra_repr"], [55, 2, 1, "", "float"], [55, 2, 1, "", "get_buffer"], [55, 2, 1, "", "get_extra_state"], [55, 2, 1, "", "get_parameter"], [55, 2, 1, "", "get_submodule"], [55, 2, 1, "", "half"], [55, 2, 1, "", "ipu"], [55, 2, 1, "", "load_state_dict"], [55, 2, 1, "", "modules"], [55, 2, 1, "", "named_buffers"], [55, 2, 1, "", "named_children"], [55, 2, 1, "", "named_modules"], [55, 2, 1, "", "named_parameters"], [55, 2, 1, "", "parameters"], [55, 2, 1, "", "predict_policy_value"], [55, 2, 1, "", "register_backward_hook"], [55, 2, 1, "", "register_buffer"], [55, 2, 1, "", "register_forward_hook"], [55, 2, 1, "", "register_forward_pre_hook"], [55, 2, 1, "", "register_full_backward_hook"], [55, 2, 1, "", "register_full_backward_pre_hook"], [55, 2, 1, "", "register_load_state_dict_post_hook"], [55, 2, 1, "", "register_module"], [55, 2, 1, "", "register_parameter"], [55, 2, 1, "", "register_state_dict_pre_hook"], [55, 2, 1, "", "requires_grad_"], [55, 2, 1, "", "sample_action"], [55, 2, 1, "", "sample_action_and_output_prob"], [55, 2, 1, "", "sample_multiple_actions"], [55, 2, 1, "", "set_extra_state"], [55, 2, 1, "", "share_memory"], [55, 2, 1, "", "state_dict"], [55, 2, 1, "", "to"], [55, 2, 1, "", "to_empty"], [55, 2, 1, "", "train"], [55, 2, 1, "", "type"], [55, 2, 1, "", "xpu"], [55, 2, 1, "", "zero_grad"]], "toy.policy.base.BaseActionRewardModel": [[56, 2, 1, "", "add_module"], [56, 2, 1, "", "apply"], [56, 2, 1, "", "bfloat16"], [56, 2, 1, "", "buffers"], [56, 2, 1, "", "children"], [56, 2, 1, "", "compile"], [56, 2, 1, "", "cpu"], [56, 2, 1, "", "cuda"], [56, 2, 1, "", "double"], [56, 2, 1, "", "eval"], [56, 2, 1, "", "extra_repr"], [56, 2, 1, "", "float"], [56, 2, 1, "", "get_buffer"], [56, 2, 1, "", "get_extra_state"], [56, 2, 1, "", "get_parameter"], [56, 2, 1, "", "get_submodule"], [56, 2, 1, "", "half"], [56, 2, 1, "", "ipu"], [56, 2, 1, "", "load_state_dict"], [56, 2, 1, "", "modules"], [56, 2, 1, "", "named_buffers"], [56, 2, 1, "", "named_children"], [56, 2, 1, "", "named_modules"], [56, 2, 1, "", "named_parameters"], [56, 2, 1, "", "parameters"], [56, 2, 1, "", "predict_value"], [56, 2, 1, "", "predict_values"], [56, 2, 1, "", "register_backward_hook"], [56, 2, 1, "", "register_buffer"], [56, 2, 1, "", "register_forward_hook"], [56, 2, 1, "", "register_forward_pre_hook"], [56, 2, 1, "", "register_full_backward_hook"], [56, 2, 1, "", "register_full_backward_pre_hook"], [56, 2, 1, "", "register_load_state_dict_post_hook"], [56, 2, 1, "", "register_module"], [56, 2, 1, "", "register_parameter"], [56, 2, 1, "", "register_state_dict_pre_hook"], [56, 2, 1, "", "requires_grad_"], [56, 2, 1, "", "set_extra_state"], [56, 2, 1, "", "share_memory"], [56, 2, 1, "", "state_dict"], [56, 2, 1, "", "to"], [56, 2, 1, "", "to_empty"], [56, 2, 1, "", "train"], [56, 2, 1, "", "type"], [56, 2, 1, "", "xpu"], [56, 2, 1, "", "zero_grad"]], "toy.policy.base.BaseClusterPolicyModel": [[57, 2, 1, "", "add_module"], [57, 2, 1, "", "apply"], [57, 2, 1, "", "bfloat16"], [57, 2, 1, "", "buffers"], [57, 2, 1, "", "calc_action_choice_probability"], [57, 2, 1, "", "calc_logits"], [57, 2, 1, "", "calc_prob_given_action"], [57, 2, 1, "", "children"], [57, 2, 1, "", "compile"], [57, 2, 1, "", "cpu"], [57, 2, 1, "", "cuda"], [57, 2, 1, "", "double"], [57, 2, 1, "", "eval"], [57, 2, 1, "", "extra_repr"], [57, 2, 1, "", "float"], [57, 2, 1, "", "get_buffer"], [57, 2, 1, "", "get_extra_state"], [57, 2, 1, "", "get_parameter"], [57, 2, 1, "", "get_submodule"], [57, 2, 1, "", "half"], [57, 2, 1, "", "ipu"], [57, 2, 1, "", "load_state_dict"], [57, 2, 1, "", "modules"], [57, 2, 1, "", "named_buffers"], [57, 2, 1, "", "named_children"], [57, 2, 1, "", "named_modules"], [57, 2, 1, "", "named_parameters"], [57, 2, 1, "", "parameters"], [57, 2, 1, "", "register_backward_hook"], [57, 2, 1, "", "register_buffer"], [57, 2, 1, "", "register_forward_hook"], [57, 2, 1, "", "register_forward_pre_hook"], [57, 2, 1, "", "register_full_backward_hook"], [57, 2, 1, "", "register_full_backward_pre_hook"], [57, 2, 1, "", "register_load_state_dict_post_hook"], [57, 2, 1, "", "register_module"], [57, 2, 1, "", "register_parameter"], [57, 2, 1, "", "register_state_dict_pre_hook"], [57, 2, 1, "", "requires_grad_"], [57, 2, 1, "", "sample_action"], [57, 2, 1, "", "sample_action_and_output_prob"], [57, 2, 1, "", "sample_multiple_actions"], [57, 2, 1, "", "set_extra_state"], [57, 2, 1, "", "share_memory"], [57, 2, 1, "", "state_dict"], [57, 2, 1, "", "to"], [57, 2, 1, "", "to_empty"], [57, 2, 1, "", "train"], [57, 2, 1, "", "type"], [57, 2, 1, "", "xpu"], [57, 2, 1, "", "zero_grad"]], "toy.policy.base.BaseClusteringModel": [[58, 2, 1, "", "add_module"], [58, 2, 1, "", "apply"], [58, 2, 1, "", "bfloat16"], [58, 2, 1, "", "buffers"], [58, 2, 1, "", "calc_cluster_choice_prob"], [58, 2, 1, "", "calc_cluster_variance"], [58, 2, 1, "", "children"], [58, 2, 1, "", "compile"], [58, 2, 1, "", "cpu"], [58, 2, 1, "", "cuda"], [58, 2, 1, "", "double"], [58, 2, 1, "", "eval"], [58, 2, 1, "", "extra_repr"], [58, 2, 1, "", "float"], [58, 2, 1, "", "get_buffer"], [58, 2, 1, "", "get_extra_state"], [58, 2, 1, "", "get_parameter"], [58, 2, 1, "", "get_submodule"], [58, 2, 1, "", "half"], [58, 2, 1, "", "ipu"], [58, 2, 1, "", "load_state_dict"], [58, 2, 1, "", "modules"], [58, 2, 1, "", "named_buffers"], [58, 2, 1, "", "named_children"], [58, 2, 1, "", "named_modules"], [58, 2, 1, "", "named_parameters"], [58, 2, 1, "", "parameters"], [58, 2, 1, "", "register_backward_hook"], [58, 2, 1, "", "register_buffer"], [58, 2, 1, "", "register_forward_hook"], [58, 2, 1, "", "register_forward_pre_hook"], [58, 2, 1, "", "register_full_backward_hook"], [58, 2, 1, "", "register_full_backward_pre_hook"], [58, 2, 1, "", "register_load_state_dict_post_hook"], [58, 2, 1, "", "register_module"], [58, 2, 1, "", "register_parameter"], [58, 2, 1, "", "register_state_dict_pre_hook"], [58, 2, 1, "", "requires_grad_"], [58, 2, 1, "", "retrieve_candidate_actions"], [58, 2, 1, "", "retrieve_candidate_actions_for_all_clusters"], [58, 2, 1, "", "retrieve_cluster"], [58, 2, 1, "", "retrieve_cluster_center"], [58, 2, 1, "", "retrieve_cluster_centers"], [58, 2, 1, "", "sample_clustering"], [58, 2, 1, "", "set_extra_state"], [58, 2, 1, "", "share_memory"], [58, 2, 1, "", "state_dict"], [58, 2, 1, "", "to"], [58, 2, 1, "", "to_empty"], [58, 2, 1, "", "train"], [58, 2, 1, "", "type"], [58, 2, 1, "", "xpu"], [58, 2, 1, "", "zero_grad"]], "toy.policy.base.BaseKernelMarginalDensityModel": [[59, 2, 1, "", "add_module"], [59, 2, 1, "", "apply"], [59, 2, 1, "", "bfloat16"], [59, 2, 1, "", "buffers"], [59, 2, 1, "", "calc_pairwise_distance"], [59, 2, 1, "", "calc_pairwise_weight"], [59, 2, 1, "", "children"], [59, 2, 1, "", "compile"], [59, 2, 1, "", "cpu"], [59, 2, 1, "", "cuda"], [59, 2, 1, "", "double"], [59, 2, 1, "", "estimate_marginal_density"], [59, 2, 1, "", "eval"], [59, 2, 1, "", "extra_repr"], [59, 2, 1, "", "float"], [59, 2, 1, "", "get_buffer"], [59, 2, 1, "", "get_extra_state"], [59, 2, 1, "", "get_parameter"], [59, 2, 1, "", "get_submodule"], [59, 2, 1, "", "half"], [59, 2, 1, "", "ipu"], [59, 2, 1, "", "load_state_dict"], [59, 2, 1, "", "modules"], [59, 2, 1, "", "named_buffers"], [59, 2, 1, "", "named_children"], [59, 2, 1, "", "named_modules"], [59, 2, 1, "", "named_parameters"], [59, 2, 1, "", "parameters"], [59, 2, 1, "", "register_backward_hook"], [59, 2, 1, "", "register_buffer"], [59, 2, 1, "", "register_forward_hook"], [59, 2, 1, "", "register_forward_pre_hook"], [59, 2, 1, "", "register_full_backward_hook"], [59, 2, 1, "", "register_full_backward_pre_hook"], [59, 2, 1, "", "register_load_state_dict_post_hook"], [59, 2, 1, "", "register_module"], [59, 2, 1, "", "register_parameter"], [59, 2, 1, "", "register_state_dict_pre_hook"], [59, 2, 1, "", "requires_grad_"], [59, 2, 1, "", "set_extra_state"], [59, 2, 1, "", "share_memory"], [59, 2, 1, "", "state_dict"], [59, 2, 1, "", "to"], [59, 2, 1, "", "to_empty"], [59, 2, 1, "", "train"], [59, 2, 1, "", "type"], [59, 2, 1, "", "xpu"], [59, 2, 1, "", "zero_grad"]], "toy.policy.base.BaseOutputRewardModel": [[60, 2, 1, "", "add_module"], [60, 2, 1, "", "apply"], [60, 2, 1, "", "bfloat16"], [60, 2, 1, "", "buffers"], [60, 2, 1, "", "children"], [60, 2, 1, "", "compile"], [60, 2, 1, "", "cpu"], [60, 2, 1, "", "cuda"], [60, 2, 1, "", "double"], [60, 2, 1, "", "eval"], [60, 2, 1, "", "extra_repr"], [60, 2, 1, "", "float"], [60, 2, 1, "", "get_buffer"], [60, 2, 1, "", "get_extra_state"], [60, 2, 1, "", "get_parameter"], [60, 2, 1, "", "get_submodule"], [60, 2, 1, "", "half"], [60, 2, 1, "", "ipu"], [60, 2, 1, "", "load_state_dict"], [60, 2, 1, "", "modules"], [60, 2, 1, "", "named_buffers"], [60, 2, 1, "", "named_children"], [60, 2, 1, "", "named_modules"], [60, 2, 1, "", "named_parameters"], [60, 2, 1, "", "parameters"], [60, 2, 1, "", "predict_value"], [60, 2, 1, "", "predict_values"], [60, 2, 1, "", "register_backward_hook"], [60, 2, 1, "", "register_buffer"], [60, 2, 1, "", "register_forward_hook"], [60, 2, 1, "", "register_forward_pre_hook"], [60, 2, 1, "", "register_full_backward_hook"], [60, 2, 1, "", "register_full_backward_pre_hook"], [60, 2, 1, "", "register_load_state_dict_post_hook"], [60, 2, 1, "", "register_module"], [60, 2, 1, "", "register_parameter"], [60, 2, 1, "", "register_state_dict_pre_hook"], [60, 2, 1, "", "requires_grad_"], [60, 2, 1, "", "set_extra_state"], [60, 2, 1, "", "share_memory"], [60, 2, 1, "", "state_dict"], [60, 2, 1, "", "to"], [60, 2, 1, "", "to_empty"], [60, 2, 1, "", "train"], [60, 2, 1, "", "type"], [60, 2, 1, "", "xpu"], [60, 2, 1, "", "zero_grad"]], "toy.policy.base.BasePolicy": [[61, 2, 1, "", "calc_action_choice_probability"], [61, 2, 1, "", "calc_prob_given_action"], [61, 2, 1, "", "predict_policy_value"], [61, 2, 1, "", "sample_action"], [61, 2, 1, "", "sample_action_and_output_prob"], [61, 2, 1, "", "sample_multiple_actions"]], "toy.policy.model": [[63, 1, 1, "", "KmeansActionClustering"], [64, 1, 1, "", "NeuralActionPolicy"], [65, 1, 1, "", "NeuralActionRewardPredictor"], [66, 1, 1, "", "NeuralClusterPolicy"], [67, 1, 1, "", "NeuralMarginalDensityEstimator"], [68, 1, 1, "", "NeuralOutputRewardPredictor"]], "toy.policy.model.KmeansActionClustering": [[63, 2, 1, "", "add_module"], [63, 2, 1, "", "apply"], [63, 2, 1, "", "bfloat16"], [63, 2, 1, "", "buffers"], [63, 2, 1, "", "calc_cluster_choice_prob"], [63, 2, 1, "", "calc_cluster_variance"], [63, 2, 1, "", "children"], [63, 2, 1, "", "compile"], [63, 2, 1, "", "cpu"], [63, 2, 1, "", "cuda"], [63, 2, 1, "", "double"], [63, 2, 1, "", "eval"], [63, 2, 1, "", "extra_repr"], [63, 2, 1, "", "float"], [63, 2, 1, "", "get_buffer"], [63, 2, 1, "", "get_extra_state"], [63, 2, 1, "", "get_parameter"], [63, 2, 1, "", "get_submodule"], [63, 2, 1, "", "half"], [63, 2, 1, "", "ipu"], [63, 2, 1, "", "load_state_dict"], [63, 2, 1, "", "modules"], [63, 2, 1, "", "named_buffers"], [63, 2, 1, "", "named_children"], [63, 2, 1, "", "named_modules"], [63, 2, 1, "", "named_parameters"], [63, 2, 1, "", "parameters"], [63, 2, 1, "", "register_backward_hook"], [63, 2, 1, "", "register_buffer"], [63, 2, 1, "", "register_forward_hook"], [63, 2, 1, "", "register_forward_pre_hook"], [63, 2, 1, "", "register_full_backward_hook"], [63, 2, 1, "", "register_full_backward_pre_hook"], [63, 2, 1, "", "register_load_state_dict_post_hook"], [63, 2, 1, "", "register_module"], [63, 2, 1, "", "register_parameter"], [63, 2, 1, "", "register_state_dict_pre_hook"], [63, 2, 1, "", "requires_grad_"], [63, 2, 1, "", "retrieve_candidate_actions"], [63, 2, 1, "", "retrieve_candidate_actions_for_all_clusters"], [63, 2, 1, "", "retrieve_cluster"], [63, 2, 1, "", "retrieve_cluster_center"], [63, 2, 1, "", "retrieve_cluster_centers"], [63, 2, 1, "", "sample_clustering"], [63, 2, 1, "", "set_extra_state"], [63, 2, 1, "", "share_memory"], [63, 2, 1, "", "state_dict"], [63, 2, 1, "", "to"], [63, 2, 1, "", "to_empty"], [63, 2, 1, "", "train"], [63, 2, 1, "", "type"], [63, 2, 1, "", "xpu"], [63, 2, 1, "", "zero_grad"]], "toy.policy.model.NeuralActionPolicy": [[64, 2, 1, "", "add_module"], [64, 2, 1, "", "apply"], [64, 2, 1, "", "bfloat16"], [64, 2, 1, "", "buffers"], [64, 2, 1, "", "calc_action_choice_probability"], [64, 2, 1, "", "calc_logits"], [64, 2, 1, "", "calc_prob_given_action"], [64, 2, 1, "", "children"], [64, 2, 1, "", "compile"], [64, 2, 1, "", "cpu"], [64, 2, 1, "", "cuda"], [64, 2, 1, "", "double"], [64, 2, 1, "", "eval"], [64, 2, 1, "", "extra_repr"], [64, 2, 1, "", "float"], [64, 2, 1, "", "get_buffer"], [64, 2, 1, "", "get_extra_state"], [64, 2, 1, "", "get_parameter"], [64, 2, 1, "", "get_submodule"], [64, 2, 1, "", "half"], [64, 2, 1, "", "ipu"], [64, 2, 1, "", "load_state_dict"], [64, 2, 1, "", "modules"], [64, 2, 1, "", "named_buffers"], [64, 2, 1, "", "named_children"], [64, 2, 1, "", "named_modules"], [64, 2, 1, "", "named_parameters"], [64, 2, 1, "", "parameters"], [64, 2, 1, "", "predict_policy_value"], [64, 2, 1, "", "register_backward_hook"], [64, 2, 1, "", "register_buffer"], [64, 2, 1, "", "register_forward_hook"], [64, 2, 1, "", "register_forward_pre_hook"], [64, 2, 1, "", "register_full_backward_hook"], [64, 2, 1, "", "register_full_backward_pre_hook"], [64, 2, 1, "", "register_load_state_dict_post_hook"], [64, 2, 1, "", "register_module"], [64, 2, 1, "", "register_parameter"], [64, 2, 1, "", "register_state_dict_pre_hook"], [64, 2, 1, "", "requires_grad_"], [64, 2, 1, "", "sample_action"], [64, 2, 1, "", "sample_action_and_output_prob"], [64, 2, 1, "", "sample_multiple_actions"], [64, 2, 1, "", "set_extra_state"], [64, 2, 1, "", "share_memory"], [64, 2, 1, "", "state_dict"], [64, 2, 1, "", "to"], [64, 2, 1, "", "to_empty"], [64, 2, 1, "", "train"], [64, 2, 1, "", "type"], [64, 2, 1, "", "xpu"], [64, 2, 1, "", "zero_grad"]], "toy.policy.model.NeuralActionRewardPredictor": [[65, 2, 1, "", "add_module"], [65, 2, 1, "", "apply"], [65, 2, 1, "", "bfloat16"], [65, 2, 1, "", "buffers"], [65, 2, 1, "", "children"], [65, 2, 1, "", "compile"], [65, 2, 1, "", "cpu"], [65, 2, 1, "", "cuda"], [65, 2, 1, "", "double"], [65, 2, 1, "", "eval"], [65, 2, 1, "", "extra_repr"], [65, 2, 1, "", "float"], [65, 2, 1, "", "get_buffer"], [65, 2, 1, "", "get_extra_state"], [65, 2, 1, "", "get_parameter"], [65, 2, 1, "", "get_submodule"], [65, 2, 1, "", "half"], [65, 2, 1, "", "ipu"], [65, 2, 1, "", "load_state_dict"], [65, 2, 1, "", "modules"], [65, 2, 1, "", "named_buffers"], [65, 2, 1, "", "named_children"], [65, 2, 1, "", "named_modules"], [65, 2, 1, "", "named_parameters"], [65, 2, 1, "", "parameters"], [65, 2, 1, "", "predict_value"], [65, 2, 1, "", "predict_values"], [65, 2, 1, "", "register_backward_hook"], [65, 2, 1, "", "register_buffer"], [65, 2, 1, "", "register_forward_hook"], [65, 2, 1, "", "register_forward_pre_hook"], [65, 2, 1, "", "register_full_backward_hook"], [65, 2, 1, "", "register_full_backward_pre_hook"], [65, 2, 1, "", "register_load_state_dict_post_hook"], [65, 2, 1, "", "register_module"], [65, 2, 1, "", "register_parameter"], [65, 2, 1, "", "register_state_dict_pre_hook"], [65, 2, 1, "", "requires_grad_"], [65, 2, 1, "", "set_extra_state"], [65, 2, 1, "", "share_memory"], [65, 2, 1, "", "state_dict"], [65, 2, 1, "", "to"], [65, 2, 1, "", "to_empty"], [65, 2, 1, "", "train"], [65, 2, 1, "", "type"], [65, 2, 1, "", "xpu"], [65, 2, 1, "", "zero_grad"]], "toy.policy.model.NeuralClusterPolicy": [[66, 2, 1, "", "add_module"], [66, 2, 1, "", "apply"], [66, 2, 1, "", "bfloat16"], [66, 2, 1, "", "buffers"], [66, 2, 1, "", "calc_action_choice_probability"], [66, 2, 1, "", "calc_logits"], [66, 2, 1, "", "calc_prob_given_action"], [66, 2, 1, "", "children"], [66, 2, 1, "", "compile"], [66, 2, 1, "", "cpu"], [66, 2, 1, "", "cuda"], [66, 2, 1, "", "double"], [66, 2, 1, "", "eval"], [66, 2, 1, "", "extra_repr"], [66, 2, 1, "", "float"], [66, 2, 1, "", "get_buffer"], [66, 2, 1, "", "get_extra_state"], [66, 2, 1, "", "get_parameter"], [66, 2, 1, "", "get_submodule"], [66, 2, 1, "", "half"], [66, 2, 1, "", "ipu"], [66, 2, 1, "", "load_state_dict"], [66, 2, 1, "", "modules"], [66, 2, 1, "", "named_buffers"], [66, 2, 1, "", "named_children"], [66, 2, 1, "", "named_modules"], [66, 2, 1, "", "named_parameters"], [66, 2, 1, "", "parameters"], [66, 2, 1, "", "register_backward_hook"], [66, 2, 1, "", "register_buffer"], [66, 2, 1, "", "register_forward_hook"], [66, 2, 1, "", "register_forward_pre_hook"], [66, 2, 1, "", "register_full_backward_hook"], [66, 2, 1, "", "register_full_backward_pre_hook"], [66, 2, 1, "", "register_load_state_dict_post_hook"], [66, 2, 1, "", "register_module"], [66, 2, 1, "", "register_parameter"], [66, 2, 1, "", "register_state_dict_pre_hook"], [66, 2, 1, "", "requires_grad_"], [66, 2, 1, "", "sample_action"], [66, 2, 1, "", "sample_action_and_output_prob"], [66, 2, 1, "", "sample_multiple_actions"], [66, 2, 1, "", "set_extra_state"], [66, 2, 1, "", "share_memory"], [66, 2, 1, "", "state_dict"], [66, 2, 1, "", "to"], [66, 2, 1, "", "to_empty"], [66, 2, 1, "", "train"], [66, 2, 1, "", "type"], [66, 2, 1, "", "xpu"], [66, 2, 1, "", "zero_grad"]], "toy.policy.model.NeuralMarginalDensityEstimator": [[67, 2, 1, "", "add_module"], [67, 2, 1, "", "apply"], [67, 2, 1, "", "bfloat16"], [67, 2, 1, "", "buffers"], [67, 2, 1, "", "calc_pairwise_distance"], [67, 2, 1, "", "calc_pairwise_weight"], [67, 2, 1, "", "children"], [67, 2, 1, "", "compile"], [67, 2, 1, "", "cpu"], [67, 2, 1, "", "cuda"], [67, 2, 1, "", "double"], [67, 2, 1, "", "estimate_marginal_density"], [67, 2, 1, "", "eval"], [67, 2, 1, "", "extra_repr"], [67, 2, 1, "", "float"], [67, 2, 1, "", "get_buffer"], [67, 2, 1, "", "get_extra_state"], [67, 2, 1, "", "get_parameter"], [67, 2, 1, "", "get_submodule"], [67, 2, 1, "", "half"], [67, 2, 1, "", "ipu"], [67, 2, 1, "", "load_state_dict"], [67, 2, 1, "", "modules"], [67, 2, 1, "", "named_buffers"], [67, 2, 1, "", "named_children"], [67, 2, 1, "", "named_modules"], [67, 2, 1, "", "named_parameters"], [67, 2, 1, "", "parameters"], [67, 2, 1, "", "register_backward_hook"], [67, 2, 1, "", "register_buffer"], [67, 2, 1, "", "register_forward_hook"], [67, 2, 1, "", "register_forward_pre_hook"], [67, 2, 1, "", "register_full_backward_hook"], [67, 2, 1, "", "register_full_backward_pre_hook"], [67, 2, 1, "", "register_load_state_dict_post_hook"], [67, 2, 1, "", "register_module"], [67, 2, 1, "", "register_parameter"], [67, 2, 1, "", "register_state_dict_pre_hook"], [67, 2, 1, "", "requires_grad_"], [67, 2, 1, "", "set_extra_state"], [67, 2, 1, "", "share_memory"], [67, 2, 1, "", "state_dict"], [67, 2, 1, "", "to"], [67, 2, 1, "", "to_empty"], [67, 2, 1, "", "train"], [67, 2, 1, "", "type"], [67, 2, 1, "", "xpu"], [67, 2, 1, "", "zero_grad"]], "toy.policy.model.NeuralOutputRewardPredictor": [[68, 2, 1, "", "add_module"], [68, 2, 1, "", "apply"], [68, 2, 1, "", "bfloat16"], [68, 2, 1, "", "buffers"], [68, 2, 1, "", "children"], [68, 2, 1, "", "compile"], [68, 2, 1, "", "cpu"], [68, 2, 1, "", "cuda"], [68, 2, 1, "", "double"], [68, 2, 1, "", "eval"], [68, 2, 1, "", "extra_repr"], [68, 2, 1, "", "float"], [68, 2, 1, "", "get_buffer"], [68, 2, 1, "", "get_extra_state"], [68, 2, 1, "", "get_parameter"], [68, 2, 1, "", "get_submodule"], [68, 2, 1, "", "half"], [68, 2, 1, "", "ipu"], [68, 2, 1, "", "load_state_dict"], [68, 2, 1, "", "modules"], [68, 2, 1, "", "named_buffers"], [68, 2, 1, "", "named_children"], [68, 2, 1, "", "named_modules"], [68, 2, 1, "", "named_parameters"], [68, 2, 1, "", "parameters"], [68, 2, 1, "", "predict_value"], [68, 2, 1, "", "predict_values"], [68, 2, 1, "", "register_backward_hook"], [68, 2, 1, "", "register_buffer"], [68, 2, 1, "", "register_forward_hook"], [68, 2, 1, "", "register_forward_pre_hook"], [68, 2, 1, "", "register_full_backward_hook"], [68, 2, 1, "", "register_full_backward_pre_hook"], [68, 2, 1, "", "register_load_state_dict_post_hook"], [68, 2, 1, "", "register_module"], [68, 2, 1, "", "register_parameter"], [68, 2, 1, "", "register_state_dict_pre_hook"], [68, 2, 1, "", "requires_grad_"], [68, 2, 1, "", "set_extra_state"], [68, 2, 1, "", "share_memory"], [68, 2, 1, "", "state_dict"], [68, 2, 1, "", "to"], [68, 2, 1, "", "to_empty"], [68, 2, 1, "", "train"], [68, 2, 1, "", "type"], [68, 2, 1, "", "xpu"], [68, 2, 1, "", "zero_grad"]], "toy.policy.policy": [[70, 1, 1, "", "EpsilonGreedyPolicy"], [71, 1, 1, "", "SoftmaxPolicy"], [72, 1, 1, "", "TwoStagePolicy"], [73, 1, 1, "", "UniformRandomPolicy"]], "toy.policy.policy.EpsilonGreedyPolicy": [[70, 2, 1, "", "calc_action_choice_probability"], [70, 2, 1, "", "calc_prob_given_action"], [70, 2, 1, "", "greedy_action"], [70, 2, 1, "", "predict_policy_value"], [70, 2, 1, "", "sample_action"], [70, 2, 1, "", "sample_action_and_output_prob"], [70, 2, 1, "", "sample_multiple_actions"]], "toy.policy.policy.SoftmaxPolicy": [[71, 2, 1, "", "calc_action_choice_probability"], [71, 2, 1, "", "calc_prob_given_action"], [71, 2, 1, "", "predict_policy_value"], [71, 2, 1, "", "sample_action"], [71, 2, 1, "", "sample_action_and_output_prob"], [71, 2, 1, "", "sample_multiple_actions"]], "toy.policy.policy.TwoStagePolicy": [[72, 2, 1, "", "calc_action_choice_probability"], [72, 2, 1, "", "calc_prob_given_action"], [72, 2, 1, "", "predict_policy_value"], [72, 2, 1, "", "sample_action"], [72, 2, 1, "", "sample_action_and_output_prob"], [72, 2, 1, "", "sample_multiple_actions"]], "toy.policy.policy.UniformRandomPolicy": [[73, 2, 1, "", "calc_action_choice_probability"], [73, 2, 1, "", "calc_prob_given_action"], [73, 2, 1, "", "predict_policy_value"], [73, 2, 1, "", "sample_action"], [73, 2, 1, "", "sample_action_and_output_prob"], [73, 2, 1, "", "sample_multiple_actions"]], "toy": [[74, 0, 0, "-", "utils"]], "toy.utils": [[75, 4, 1, "", "check_logged_feedback"], [76, 4, 1, "", "check_tensor"], [77, 4, 1, "", "defaultdict_to_dict"], [78, 4, 1, "", "gaussian_kernel"], [79, 4, 1, "", "torch_seed"], [80, 4, 1, "", "uniform_kernel"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "function", "Python function"]}, "titleterms": {"opl": [35, 36, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 82], "prompt": [83, 86], "guid": 83, "languag": [83, 86], "gener": [82, 83, 86], "overview": 84, "offlineprompt": [82, 86], "A": 86, "python": 86, "librari": 86, "optim": 86, "natur": 86, "collect": 86, "user": 86, "feedback": 86, "tabl": 86, "content": 86, "introduct": 86, "see": 86, "also": 86, "off": 83, "polici": [40, 41, 42, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 82, 83], "learn": 83, "softwar": 84, "instal": 81, "api": 82, "quickstart": 85, "get": 86, "start": 86, "packag": 86, "refer": [82, 86], "toi": [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80], "dataset": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 82], "function": [9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32], "auxiliaryoutputgener": 17, "candidateactionsgener": 18, "confoundedauxiliaryoutputgener": 19, "confoundedexponentialauxiliaryoutputgener": 20, "confoundedpowerauxiliaryoutputgener": 21, "confoundedrationalauxiliaryoutputgener": 22, "confoundedtrigonometricauxiliaryoutputgener": 23, "contextquerygener": 24, "exponentialauxiliaryoutputgener": 25, "mixtureofgaussiancandidateactionsgener": 26, "powerauxiliaryoutputgener": 27, "rationalauxiliaryoutputgener": 28, "rewardsimul": 29, "sigmoidauxiliaryoutputgener": 30, "sparserewardsimul": 31, "trigonometricauxiliaryoutputgener": 32, "synthet": [33, 34, 82], "syntheticdataset": 34, "behavior_clon": [35, 44, 45], "behaviorcloninglearn": 45, "policy_learn": [38, 48, 49, 50], "adaptivepolicylearn": [], "policylearn": 50, "reward_learn": [39, 51, 52, 53], "actionrewardlearn": 52, "outputrewardlearn": 53, "base": [0, 1, 2, 3, 4, 5, 40, 54, 55, 56, 57, 58, 59, 60, 61], "baseactionpolicymodel": 55, "baseactionrewardmodel": 56, "baseclusterpolicymodel": 57, "baseclusteringmodel": 58, "baseoutputrewardmodel": 60, "basepolici": 61, "model": [41, 62, 63, 64, 65, 66, 67, 68], "actionsimilaritybasedclust": [], "fixedclust": [], "neuralactionpolici": 64, "neuralactionrewardpredictor": 65, "neuralclusterpolici": 66, "neuraloutputrewardpredictor": 68, "outputsimilaritybasedclust": [], "adaptivetwostagepolici": [], "epsilongreedypolici": 70, "softmaxpolici": 71, "twostagepolici": 72, "uniformrandompolici": 73, "util": [43, 74, 75, 76, 77, 78, 79, 80], "check_logged_feedback": 75, "check_tensor": 76, "defaultdict_to_dict": 77, "torch_se": 79, "set": 82, "modul": 82, "other": 82, "semi": 82, "e": 82, "g": 82, "recip": 82, "src": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 35, 36, 37, 38, 39, 40, 41, 42, 43], "basefrozenllm": 3, "baserewardsimul": 5, "benchmark": [6, 7], "semisyntheticdataset": 7, "frozen_llm": 8, "llamafrozenllm": [], "candidateactionsload": [], "contextqueryload": [], "reward_simul": [13, 14, 15], "transformerrewardsimul": 15, "basecandidateactionsload": 1, "basecontextqueryload": 2, "basepromptformatt": 4, "defaultcandidateactionsload": 10, "defaultcontextqueryload": 11, "prompt_formatt": 12, "collaborativefilteringrewardsimul": 14, "marginal_learn": [37, 46, 47], "marginaldensitylearn": 47, "kernelpolicylearn": 49, "basekernelmarginaldensitymodel": 59, "kmeansactionclust": 63, "neuralmarginaldensityestim": 67, "gaussian_kernel": 78, "uniform_kernel": 80}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinxcontrib.bibtex": 9, "sphinx": 57}, "alltitles": {"src.dataset.base": [[0, "module-src.dataset.base"]], "src.dataset.base.BaseCandidateActionsLoader": [[1, "src-dataset-base-basecandidateactionsloader"]], "src.dataset.base.BaseContextQueryLoader": [[2, "src-dataset-base-basecontextqueryloader"]], "src.dataset.base.BaseFrozenLLM": [[3, "src-dataset-base-basefrozenllm"]], "src.dataset.base.BasePromptFormatter": [[4, "src-dataset-base-basepromptformatter"]], "src.dataset.base.BaseRewardSimulator": [[5, "src-dataset-base-baserewardsimulator"]], "src.dataset.benchmark": [[6, "module-src.dataset.benchmark"]], "src.dataset.benchmark.SemiSyntheticDataset": [[7, "src-dataset-benchmark-semisyntheticdataset"]], "src.dataset.frozen_llm": [[8, "module-src.dataset.frozen_llm"]], "src.dataset.function": [[9, "module-src.dataset.function"]], "src.dataset.function.DefaultCandidateActionsLoader": [[10, "src-dataset-function-defaultcandidateactionsloader"]], "src.dataset.function.DefaultContextQueryLoader": [[11, "src-dataset-function-defaultcontextqueryloader"]], "src.dataset.prompt_formatter": [[12, "module-src.dataset.prompt_formatter"]], "src.dataset.reward_simulator": [[13, "module-src.dataset.reward_simulator"]], "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator": [[14, "src-dataset-reward-simulator-collaborativefilteringrewardsimulator"]], "src.dataset.reward_simulator.TransformerRewardSimulator": [[15, "src-dataset-reward-simulator-transformerrewardsimulator"]], "toy.dataset.function": [[16, "module-toy.dataset.function"]], "toy.dataset.function.AuxiliaryOutputGenerator": [[17, "toy-dataset-function-auxiliaryoutputgenerator"]], "toy.dataset.function.CandidateActionsGenerator": [[18, "toy-dataset-function-candidateactionsgenerator"]], "toy.dataset.function.ConfoundedAuxiliaryOutputGenerator": [[19, "toy-dataset-function-confoundedauxiliaryoutputgenerator"]], "toy.dataset.function.ConfoundedExponentialAuxiliaryOutputGenerator": [[20, "toy-dataset-function-confoundedexponentialauxiliaryoutputgenerator"]], "toy.dataset.function.ConfoundedPowerAuxiliaryOutputGenerator": [[21, "toy-dataset-function-confoundedpowerauxiliaryoutputgenerator"]], "toy.dataset.function.ConfoundedRationalAuxiliaryOutputGenerator": [[22, "toy-dataset-function-confoundedrationalauxiliaryoutputgenerator"]], "toy.dataset.function.ConfoundedTrigonometricAuxiliaryOutputGenerator": [[23, "toy-dataset-function-confoundedtrigonometricauxiliaryoutputgenerator"]], "toy.dataset.function.ContextQueryGenerator": [[24, "toy-dataset-function-contextquerygenerator"]], "toy.dataset.function.ExponentialAuxiliaryOutputGenerator": [[25, "toy-dataset-function-exponentialauxiliaryoutputgenerator"]], "toy.dataset.function.MixtureOfGaussianCandidateActionsGenerator": [[26, "toy-dataset-function-mixtureofgaussiancandidateactionsgenerator"]], "toy.dataset.function.PowerAuxiliaryOutputGenerator": [[27, "toy-dataset-function-powerauxiliaryoutputgenerator"]], "toy.dataset.function.RationalAuxiliaryOutputGenerator": [[28, "toy-dataset-function-rationalauxiliaryoutputgenerator"]], "toy.dataset.function.RewardSimulator": [[29, "toy-dataset-function-rewardsimulator"]], "toy.dataset.function.SigmoidAuxiliaryOutputGenerator": [[30, "toy-dataset-function-sigmoidauxiliaryoutputgenerator"]], "toy.dataset.function.SparseRewardSimulator": [[31, "toy-dataset-function-sparserewardsimulator"]], "toy.dataset.function.TrigonometricAuxiliaryOutputGenerator": [[32, "toy-dataset-function-trigonometricauxiliaryoutputgenerator"]], "toy.dataset.synthetic": [[33, "module-toy.dataset.synthetic"]], "toy.dataset.synthetic.SyntheticDataset": [[34, "toy-dataset-synthetic-syntheticdataset"]], "src.opl.behavior_cloning": [[35, "module-src.opl.behavior_cloning"]], "src.opl.dataset": [[36, "module-src.opl.dataset"]], "src.opl.marginal_learner": [[37, "module-src.opl.marginal_learner"]], "src.opl.policy_learner": [[38, "module-src.opl.policy_learner"]], "src.opl.reward_learner": [[39, "module-src.opl.reward_learner"]], "src.policy.base": [[40, "module-src.policy.base"]], "src.policy.model": [[41, "module-src.policy.model"]], "src.policy.policy": [[42, "module-src.policy.policy"]], "src.utils": [[43, "module-src.utils"]], "toy.opl.behavior_cloning": [[44, "module-toy.opl.behavior_cloning"]], "toy.opl.behavior_cloning.BehaviorCloningLearner": [[45, "toy-opl-behavior-cloning-behaviorcloninglearner"]], "toy.opl.marginal_learner": [[46, "module-toy.opl.marginal_learner"]], "toy.opl.marginal_learner.MarginalDensityLearner": [[47, "toy-opl-marginal-learner-marginaldensitylearner"]], "toy.opl.policy_learner": [[48, "module-toy.opl.policy_learner"]], "toy.opl.policy_learner.KernelPolicyLearner": [[49, "toy-opl-policy-learner-kernelpolicylearner"]], "toy.opl.policy_learner.PolicyLearner": [[50, "toy-opl-policy-learner-policylearner"]], "toy.opl.reward_learner": [[51, "module-toy.opl.reward_learner"]], "toy.opl.reward_learner.ActionRewardLearner": [[52, "toy-opl-reward-learner-actionrewardlearner"]], "toy.opl.reward_learner.OutputRewardLearner": [[53, "toy-opl-reward-learner-outputrewardlearner"]], "toy.policy.base": [[54, "module-toy.policy.base"]], "toy.policy.base.BaseActionPolicyModel": [[55, "toy-policy-base-baseactionpolicymodel"]], "toy.policy.base.BaseActionRewardModel": [[56, "toy-policy-base-baseactionrewardmodel"]], "toy.policy.base.BaseClusterPolicyModel": [[57, "toy-policy-base-baseclusterpolicymodel"]], "toy.policy.base.BaseClusteringModel": [[58, "toy-policy-base-baseclusteringmodel"]], "toy.policy.base.BaseKernelMarginalDensityModel": [[59, "toy-policy-base-basekernelmarginaldensitymodel"]], "toy.policy.base.BaseOutputRewardModel": [[60, "toy-policy-base-baseoutputrewardmodel"]], "toy.policy.base.BasePolicy": [[61, "toy-policy-base-basepolicy"]], "toy.policy.model": [[62, "module-toy.policy.model"]], "toy.policy.model.KmeansActionClustering": [[63, "toy-policy-model-kmeansactionclustering"]], "toy.policy.model.NeuralActionPolicy": [[64, "toy-policy-model-neuralactionpolicy"]], "toy.policy.model.NeuralActionRewardPredictor": [[65, "toy-policy-model-neuralactionrewardpredictor"]], "toy.policy.model.NeuralClusterPolicy": [[66, "toy-policy-model-neuralclusterpolicy"]], "toy.policy.model.NeuralMarginalDensityEstimator": [[67, "toy-policy-model-neuralmarginaldensityestimator"]], "toy.policy.model.NeuralOutputRewardPredictor": [[68, "toy-policy-model-neuraloutputrewardpredictor"]], "toy.policy.policy": [[69, "module-toy.policy.policy"]], "toy.policy.policy.EpsilonGreedyPolicy": [[70, "toy-policy-policy-epsilongreedypolicy"]], "toy.policy.policy.SoftmaxPolicy": [[71, "toy-policy-policy-softmaxpolicy"]], "toy.policy.policy.TwoStagePolicy": [[72, "toy-policy-policy-twostagepolicy"]], "toy.policy.policy.UniformRandomPolicy": [[73, "toy-policy-policy-uniformrandompolicy"]], "toy.utils": [[74, "module-toy.utils"]], "toy.utils.check_logged_feedback": [[75, "toy-utils-check-logged-feedback"]], "toy.utils.check_tensor": [[76, "toy-utils-check-tensor"]], "toy.utils.defaultdict_to_dict": [[77, "toy-utils-defaultdict-to-dict"]], "toy.utils.gaussian_kernel": [[78, "toy-utils-gaussian-kernel"]], "toy.utils.torch_seed": [[79, "toy-utils-torch-seed"]], "toy.utils.uniform_kernel": [[80, "toy-utils-uniform-kernel"]], "Installation": [[81, "installation"]], "OfflinePrompts API reference": [[82, "offlineprompts-api-reference"]], "Synthetic setting": [[82, "synthetic-setting"]], "dataset module": [[82, "dataset-module"], [82, "semi-synthetic-api-dataset"]], "policy module": [[82, "policy-module"], [82, "id2"]], "opl module": [[82, "opl-module"], [82, "id4"]], "others": [[82, "others"], [82, "id6"]], "Semi-synthetic setting (e.g., recipe generation)": [[82, "semi-synthetic-setting-e-g-recipe-generation"]], "Off-policy learning for prompt-guided language generation": [[83, "off-policy-learning-for-prompt-guided-language-generation"]], "Overview of the software": [[84, "overview-of-the-software"]], "Quickstart": [[85, "quickstart"]], "OfflinePrompts: A Python library for optimizing language generation with naturally collected user feedback and prompt": [[86, "offlineprompts-a-python-library-for-optimizing-language-generation-with-naturally-collected-user-feedback-and-prompt"]], "Table of Contents": [[86, "table-of-contents"]], "Introduction:": [[86, null]], "Getting Started:": [[86, null]], "Package References:": [[86, null]], "See also:": [[86, null]]}, "indexentries": {"basecandidateactionsloader (class in src.dataset.base)": [[0, "src.dataset.base.BaseCandidateActionsLoader"], [1, "src.dataset.base.BaseCandidateActionsLoader"]], "basecontextqueryloader (class in src.dataset.base)": [[0, "src.dataset.base.BaseContextQueryLoader"], [2, "src.dataset.base.BaseContextQueryLoader"]], "basefrozenllm (class in src.dataset.base)": [[0, "src.dataset.base.BaseFrozenLLM"], [3, "src.dataset.base.BaseFrozenLLM"]], "basepromptformatter (class in src.dataset.base)": [[0, "src.dataset.base.BasePromptFormatter"], [4, "src.dataset.base.BasePromptFormatter"]], "baserewardsimulator (class in src.dataset.base)": [[0, "src.dataset.base.BaseRewardSimulator"], [5, "src.dataset.base.BaseRewardSimulator"]], "calc_expected_reward() (src.dataset.base.baserewardsimulator method)": [[0, "src.dataset.base.BaseRewardSimulator.calc_expected_reward"], [5, "src.dataset.base.BaseRewardSimulator.calc_expected_reward"]], "format_tokens() (src.dataset.base.basepromptformatter method)": [[0, "src.dataset.base.BasePromptFormatter.format_tokens"], [4, "src.dataset.base.BasePromptFormatter.format_tokens"]], "generate_output_sentence() (src.dataset.base.basefrozenllm method)": [[0, "src.dataset.base.BaseFrozenLLM.generate_output_sentence"], [3, "src.dataset.base.BaseFrozenLLM.generate_output_sentence"]], "module": [[0, "module-src.dataset.base"], [6, "module-src.dataset.benchmark"], [8, "module-src.dataset.frozen_llm"], [9, "module-src.dataset.function"], [12, "module-src.dataset.prompt_formatter"], [13, "module-src.dataset.reward_simulator"], [16, "module-toy.dataset.function"], [33, "module-toy.dataset.synthetic"], [35, "module-src.opl.behavior_cloning"], [36, "module-src.opl.dataset"], [37, "module-src.opl.marginal_learner"], [38, "module-src.opl.policy_learner"], [39, "module-src.opl.reward_learner"], [40, "module-src.policy.base"], [41, "module-src.policy.model"], [42, "module-src.policy.policy"], [43, "module-src.utils"], [44, "module-toy.opl.behavior_cloning"], [46, "module-toy.opl.marginal_learner"], [48, "module-toy.opl.policy_learner"], [51, "module-toy.opl.reward_learner"], [54, "module-toy.policy.base"], [62, "module-toy.policy.model"], [69, "module-toy.policy.policy"], [74, "module-toy.utils"]], "src.dataset.base": [[0, "module-src.dataset.base"]], "add_module() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.add_module"]], "apply() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.apply"]], "bfloat16() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.bfloat16"]], "buffers() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.buffers"]], "children() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.children"]], "compile() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.compile"]], "cpu() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.cpu"]], "cuda() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.cuda"]], "double() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.double"]], "eval() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.eval"]], "extra_repr() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.extra_repr"]], "float() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.float"]], "get_buffer() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.get_buffer"]], "get_extra_state() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.get_extra_state"]], "get_parameter() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.get_parameter"]], "get_submodule() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.get_submodule"]], "half() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.half"]], "ipu() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.ipu"]], "load_state_dict() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.load_state_dict"]], "modules() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.modules"]], "named_buffers() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.named_buffers"]], "named_children() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.named_children"]], "named_modules() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.named_modules"]], "named_parameters() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.named_parameters"]], "parameters() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.parameters"]], "register_backward_hook() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_backward_hook"]], "register_buffer() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_buffer"]], "register_forward_hook() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_forward_hook"]], "register_forward_pre_hook() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_forward_pre_hook"]], "register_full_backward_hook() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_full_backward_hook"]], "register_full_backward_pre_hook() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_load_state_dict_post_hook"]], "register_module() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_module"]], "register_parameter() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_parameter"]], "register_state_dict_pre_hook() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.register_state_dict_pre_hook"]], "requires_grad_() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.requires_grad_"]], "set_extra_state() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.set_extra_state"]], "share_memory() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.share_memory"]], "state_dict() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.state_dict"]], "to() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.to"]], "to_empty() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.to_empty"]], "train() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.train"]], "type() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.type"]], "xpu() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.xpu"]], "zero_grad() (src.dataset.base.baserewardsimulator method)": [[5, "src.dataset.base.BaseRewardSimulator.zero_grad"]], "semisyntheticdataset (class in src.dataset.benchmark)": [[6, "src.dataset.benchmark.SemiSyntheticDataset"], [7, "src.dataset.benchmark.SemiSyntheticDataset"]], "calc_expected_policy_value() (src.dataset.benchmark.semisyntheticdataset method)": [[6, "src.dataset.benchmark.SemiSyntheticDataset.calc_expected_policy_value"], [7, "src.dataset.benchmark.SemiSyntheticDataset.calc_expected_policy_value"]], "sample_dataset() (src.dataset.benchmark.semisyntheticdataset method)": [[6, "src.dataset.benchmark.SemiSyntheticDataset.sample_dataset"], [7, "src.dataset.benchmark.SemiSyntheticDataset.sample_dataset"]], "sample_reward_given_action() (src.dataset.benchmark.semisyntheticdataset method)": [[6, "src.dataset.benchmark.SemiSyntheticDataset.sample_reward_given_action"], [7, "src.dataset.benchmark.SemiSyntheticDataset.sample_reward_given_action"]], "sample_reward_given_output() (src.dataset.benchmark.semisyntheticdataset method)": [[6, "src.dataset.benchmark.SemiSyntheticDataset.sample_reward_given_output"], [7, "src.dataset.benchmark.SemiSyntheticDataset.sample_reward_given_output"]], "src.dataset.benchmark": [[6, "module-src.dataset.benchmark"]], "autofrozenllm (class in src.dataset.frozen_llm)": [[8, "src.dataset.frozen_llm.AutoFrozenLLM"]], "generate_output_sentence() (src.dataset.frozen_llm.autofrozenllm method)": [[8, "src.dataset.frozen_llm.AutoFrozenLLM.generate_output_sentence"]], "src.dataset.frozen_llm": [[8, "module-src.dataset.frozen_llm"]], "defaultcandidateactionsloader (class in src.dataset.function)": [[9, "src.dataset.function.DefaultCandidateActionsLoader"], [10, "src.dataset.function.DefaultCandidateActionsLoader"]], "defaultcontextqueryloader (class in src.dataset.function)": [[9, "src.dataset.function.DefaultContextQueryLoader"], [11, "src.dataset.function.DefaultContextQueryLoader"]], "sample_context_and_query() (src.dataset.function.defaultcontextqueryloader method)": [[9, "src.dataset.function.DefaultContextQueryLoader.sample_context_and_query"], [11, "src.dataset.function.DefaultContextQueryLoader.sample_context_and_query"]], "src.dataset.function": [[9, "module-src.dataset.function"]], "movielenspromptformatter (class in src.dataset.prompt_formatter)": [[12, "src.dataset.prompt_formatter.MovielensPromptFormatter"]], "format_tokens() (src.dataset.prompt_formatter.movielenspromptformatter method)": [[12, "src.dataset.prompt_formatter.MovielensPromptFormatter.format_tokens"]], "src.dataset.prompt_formatter": [[12, "module-src.dataset.prompt_formatter"]], "collaborativefilteringrewardsimulator (class in src.dataset.reward_simulator)": [[13, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator"], [14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator"]], "transformerrewardsimulator (class in src.dataset.reward_simulator)": [[13, "src.dataset.reward_simulator.TransformerRewardSimulator"], [15, "src.dataset.reward_simulator.TransformerRewardSimulator"]], "calc_expected_reward() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[13, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.calc_expected_reward"], [14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.calc_expected_reward"]], "calc_expected_reward() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[13, "src.dataset.reward_simulator.TransformerRewardSimulator.calc_expected_reward"], [15, "src.dataset.reward_simulator.TransformerRewardSimulator.calc_expected_reward"]], "src.dataset.reward_simulator": [[13, "module-src.dataset.reward_simulator"]], "add_module() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.add_module"]], "apply() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.apply"]], "bfloat16() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.bfloat16"]], "buffers() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.buffers"]], "children() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.children"]], "compile() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.compile"]], "cpu() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.cpu"]], "cuda() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.cuda"]], "double() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.double"]], "eval() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.eval"]], "extra_repr() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.extra_repr"]], "float() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.float"]], "get_buffer() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.get_buffer"]], "get_extra_state() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.get_extra_state"]], "get_parameter() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.get_parameter"]], "get_submodule() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.get_submodule"]], "half() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.half"]], "ipu() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.ipu"]], "load_state_dict() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.load_state_dict"]], "modules() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.modules"]], "named_buffers() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.named_buffers"]], "named_children() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.named_children"]], "named_modules() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.named_modules"]], "named_parameters() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.named_parameters"]], "parameters() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.parameters"]], "register_backward_hook() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_backward_hook"]], "register_buffer() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_buffer"]], "register_forward_hook() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_forward_hook"]], "register_forward_pre_hook() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_forward_pre_hook"]], "register_full_backward_hook() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_full_backward_hook"]], "register_full_backward_pre_hook() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_load_state_dict_post_hook"]], "register_module() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_module"]], "register_parameter() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_parameter"]], "register_state_dict_pre_hook() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.register_state_dict_pre_hook"]], "requires_grad_() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.requires_grad_"]], "set_extra_state() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.set_extra_state"]], "share_memory() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.share_memory"]], "state_dict() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.state_dict"]], "to() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.to"]], "to_empty() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.to_empty"]], "train() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.train"]], "type() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.type"]], "xpu() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.xpu"]], "zero_grad() (src.dataset.reward_simulator.collaborativefilteringrewardsimulator method)": [[14, "src.dataset.reward_simulator.CollaborativeFilteringRewardSimulator.zero_grad"]], "add_module() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.add_module"]], "apply() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.apply"]], "bfloat16() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.bfloat16"]], "buffers() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.buffers"]], "children() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.children"]], "compile() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.compile"]], "cpu() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.cpu"]], "cuda() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.cuda"]], "double() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.double"]], "eval() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.eval"]], "extra_repr() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.extra_repr"]], "float() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.float"]], "get_buffer() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.get_buffer"]], "get_extra_state() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.get_extra_state"]], "get_parameter() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.get_parameter"]], "get_submodule() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.get_submodule"]], "half() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.half"]], "ipu() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.ipu"]], "load_state_dict() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.load_state_dict"]], "modules() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.modules"]], "named_buffers() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.named_buffers"]], "named_children() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.named_children"]], "named_modules() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.named_modules"]], "named_parameters() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.named_parameters"]], "parameters() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.parameters"]], "register_backward_hook() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_backward_hook"]], "register_buffer() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_buffer"]], "register_forward_hook() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_forward_hook"]], "register_forward_pre_hook() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_forward_pre_hook"]], "register_full_backward_hook() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_full_backward_hook"]], "register_full_backward_pre_hook() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_load_state_dict_post_hook"]], "register_module() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_module"]], "register_parameter() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_parameter"]], "register_state_dict_pre_hook() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.register_state_dict_pre_hook"]], "requires_grad_() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.requires_grad_"]], "set_extra_state() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.set_extra_state"]], "share_memory() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.share_memory"]], "state_dict() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.state_dict"]], "to() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.to"]], "to_empty() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.to_empty"]], "train() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.train"]], "type() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.type"]], "xpu() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.xpu"]], "zero_grad() (src.dataset.reward_simulator.transformerrewardsimulator method)": [[15, "src.dataset.reward_simulator.TransformerRewardSimulator.zero_grad"]], "auxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.AuxiliaryOutputGenerator"], [17, "toy.dataset.function.AuxiliaryOutputGenerator"]], "candidateactionsgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.CandidateActionsGenerator"], [18, "toy.dataset.function.CandidateActionsGenerator"]], "confoundedauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.ConfoundedAuxiliaryOutputGenerator"], [19, "toy.dataset.function.ConfoundedAuxiliaryOutputGenerator"]], "confoundedexponentialauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.ConfoundedExponentialAuxiliaryOutputGenerator"], [20, "toy.dataset.function.ConfoundedExponentialAuxiliaryOutputGenerator"]], "confoundedpowerauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.ConfoundedPowerAuxiliaryOutputGenerator"], [21, "toy.dataset.function.ConfoundedPowerAuxiliaryOutputGenerator"]], "confoundedrationalauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.ConfoundedRationalAuxiliaryOutputGenerator"], [22, "toy.dataset.function.ConfoundedRationalAuxiliaryOutputGenerator"]], "confoundedtrigonometricauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.ConfoundedTrigonometricAuxiliaryOutputGenerator"], [23, "toy.dataset.function.ConfoundedTrigonometricAuxiliaryOutputGenerator"]], "contextquerygenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.ContextQueryGenerator"], [24, "toy.dataset.function.ContextQueryGenerator"]], "exponentialauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.ExponentialAuxiliaryOutputGenerator"], [25, "toy.dataset.function.ExponentialAuxiliaryOutputGenerator"]], "mixtureofgaussiancandidateactionsgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.MixtureOfGaussianCandidateActionsGenerator"], [26, "toy.dataset.function.MixtureOfGaussianCandidateActionsGenerator"]], "powerauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.PowerAuxiliaryOutputGenerator"], [27, "toy.dataset.function.PowerAuxiliaryOutputGenerator"]], "rationalauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.RationalAuxiliaryOutputGenerator"], [28, "toy.dataset.function.RationalAuxiliaryOutputGenerator"]], "rewardsimulator (class in toy.dataset.function)": [[16, "toy.dataset.function.RewardSimulator"], [29, "toy.dataset.function.RewardSimulator"]], "sigmoidauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.SigmoidAuxiliaryOutputGenerator"], [30, "toy.dataset.function.SigmoidAuxiliaryOutputGenerator"]], "sparserewardsimulator (class in toy.dataset.function)": [[16, "toy.dataset.function.SparseRewardSimulator"], [31, "toy.dataset.function.SparseRewardSimulator"]], "trigonometricauxiliaryoutputgenerator (class in toy.dataset.function)": [[16, "toy.dataset.function.TrigonometricAuxiliaryOutputGenerator"], [32, "toy.dataset.function.TrigonometricAuxiliaryOutputGenerator"]], "calc_expected_reward() (toy.dataset.function.rewardsimulator method)": [[16, "toy.dataset.function.RewardSimulator.calc_expected_reward"], [29, "toy.dataset.function.RewardSimulator.calc_expected_reward"]], "calc_expected_reward() (toy.dataset.function.sparserewardsimulator method)": [[16, "toy.dataset.function.SparseRewardSimulator.calc_expected_reward"], [31, "toy.dataset.function.SparseRewardSimulator.calc_expected_reward"]], "sample_auxiliary_output() (toy.dataset.function.auxiliaryoutputgenerator method)": [[16, "toy.dataset.function.AuxiliaryOutputGenerator.sample_auxiliary_output"], [17, "toy.dataset.function.AuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_context_and_query() (toy.dataset.function.contextquerygenerator method)": [[16, "toy.dataset.function.ContextQueryGenerator.sample_context_and_query"], [24, "toy.dataset.function.ContextQueryGenerator.sample_context_and_query"]], "toy.dataset.function": [[16, "module-toy.dataset.function"]], "sample_auxiliary_output() (toy.dataset.function.confoundedauxiliaryoutputgenerator method)": [[19, "toy.dataset.function.ConfoundedAuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_auxiliary_output() (toy.dataset.function.confoundedexponentialauxiliaryoutputgenerator method)": [[20, "toy.dataset.function.ConfoundedExponentialAuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_auxiliary_output() (toy.dataset.function.confoundedpowerauxiliaryoutputgenerator method)": [[21, "toy.dataset.function.ConfoundedPowerAuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_auxiliary_output() (toy.dataset.function.confoundedrationalauxiliaryoutputgenerator method)": [[22, "toy.dataset.function.ConfoundedRationalAuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_auxiliary_output() (toy.dataset.function.confoundedtrigonometricauxiliaryoutputgenerator method)": [[23, "toy.dataset.function.ConfoundedTrigonometricAuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_auxiliary_output() (toy.dataset.function.exponentialauxiliaryoutputgenerator method)": [[25, "toy.dataset.function.ExponentialAuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_auxiliary_output() (toy.dataset.function.powerauxiliaryoutputgenerator method)": [[27, "toy.dataset.function.PowerAuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_auxiliary_output() (toy.dataset.function.rationalauxiliaryoutputgenerator method)": [[28, "toy.dataset.function.RationalAuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_auxiliary_output() (toy.dataset.function.sigmoidauxiliaryoutputgenerator method)": [[30, "toy.dataset.function.SigmoidAuxiliaryOutputGenerator.sample_auxiliary_output"]], "sample_auxiliary_output() (toy.dataset.function.trigonometricauxiliaryoutputgenerator method)": [[32, "toy.dataset.function.TrigonometricAuxiliaryOutputGenerator.sample_auxiliary_output"]], "syntheticdataset (class in toy.dataset.synthetic)": [[33, "toy.dataset.synthetic.SyntheticDataset"], [34, "toy.dataset.synthetic.SyntheticDataset"]], "calc_expected_policy_value() (toy.dataset.synthetic.syntheticdataset method)": [[33, "toy.dataset.synthetic.SyntheticDataset.calc_expected_policy_value"], [34, "toy.dataset.synthetic.SyntheticDataset.calc_expected_policy_value"]], "calc_expected_reward_for_all_actions() (toy.dataset.synthetic.syntheticdataset method)": [[33, "toy.dataset.synthetic.SyntheticDataset.calc_expected_reward_for_all_actions"], [34, "toy.dataset.synthetic.SyntheticDataset.calc_expected_reward_for_all_actions"]], "calc_expected_reward_given_action() (toy.dataset.synthetic.syntheticdataset method)": [[33, "toy.dataset.synthetic.SyntheticDataset.calc_expected_reward_given_action"], [34, "toy.dataset.synthetic.SyntheticDataset.calc_expected_reward_given_action"]], "sample_dataset() (toy.dataset.synthetic.syntheticdataset method)": [[33, "toy.dataset.synthetic.SyntheticDataset.sample_dataset"], [34, "toy.dataset.synthetic.SyntheticDataset.sample_dataset"]], "sample_reward_given_action() (toy.dataset.synthetic.syntheticdataset method)": [[33, "toy.dataset.synthetic.SyntheticDataset.sample_reward_given_action"], [34, "toy.dataset.synthetic.SyntheticDataset.sample_reward_given_action"]], "sample_reward_given_output() (toy.dataset.synthetic.syntheticdataset method)": [[33, "toy.dataset.synthetic.SyntheticDataset.sample_reward_given_output"], [34, "toy.dataset.synthetic.SyntheticDataset.sample_reward_given_output"]], "toy.dataset.synthetic": [[33, "module-toy.dataset.synthetic"]], "behaviorcloninglearner (class in src.opl.behavior_cloning)": [[35, "src.opl.behavior_cloning.BehaviorCloningLearner"]], "load() (src.opl.behavior_cloning.behaviorcloninglearner method)": [[35, "src.opl.behavior_cloning.BehaviorCloningLearner.load"]], "offline_cloning() (src.opl.behavior_cloning.behaviorcloninglearner method)": [[35, "src.opl.behavior_cloning.BehaviorCloningLearner.offline_cloning"]], "online_cloning() (src.opl.behavior_cloning.behaviorcloninglearner method)": [[35, "src.opl.behavior_cloning.BehaviorCloningLearner.online_cloning"]], "optimizer (src.opl.behavior_cloning.behaviorcloninglearner attribute)": [[35, "src.opl.behavior_cloning.BehaviorCloningLearner.optimizer"]], "random_state (src.opl.behavior_cloning.behaviorcloninglearner attribute)": [[35, "src.opl.behavior_cloning.BehaviorCloningLearner.random_state"]], "save() (src.opl.behavior_cloning.behaviorcloninglearner method)": [[35, "src.opl.behavior_cloning.BehaviorCloningLearner.save"]], "seed() (src.opl.behavior_cloning.behaviorcloninglearner method)": [[35, "src.opl.behavior_cloning.BehaviorCloningLearner.seed"]], "src.opl.behavior_cloning": [[35, "module-src.opl.behavior_cloning"]], "torchloggeddataset (class in src.opl.dataset)": [[36, "src.opl.dataset.TorchLoggedDataset"]], "src.opl.dataset": [[36, "module-src.opl.dataset"]], "marginaldensitylearner (class in src.opl.marginal_learner)": [[37, "src.opl.marginal_learner.MarginalDensityLearner"]], "load() (src.opl.marginal_learner.marginaldensitylearner method)": [[37, "src.opl.marginal_learner.MarginalDensityLearner.load"]], "optimizer (src.opl.marginal_learner.marginaldensitylearner attribute)": [[37, "src.opl.marginal_learner.MarginalDensityLearner.optimizer"]], "random_state (src.opl.marginal_learner.marginaldensitylearner attribute)": [[37, "src.opl.marginal_learner.MarginalDensityLearner.random_state"]], "save() (src.opl.marginal_learner.marginaldensitylearner method)": [[37, "src.opl.marginal_learner.MarginalDensityLearner.save"]], "seed() (src.opl.marginal_learner.marginaldensitylearner method)": [[37, "src.opl.marginal_learner.MarginalDensityLearner.seed"]], "simulation_training() (src.opl.marginal_learner.marginaldensitylearner method)": [[37, "src.opl.marginal_learner.MarginalDensityLearner.simulation_training"]], "src.opl.marginal_learner": [[37, "module-src.opl.marginal_learner"]], "kernelpolicylearner (class in src.opl.policy_learner)": [[38, "src.opl.policy_learner.KernelPolicyLearner"]], "policylearner (class in src.opl.policy_learner)": [[38, "src.opl.policy_learner.PolicyLearner"]], "hybrid_policy_gradient() (src.opl.policy_learner.kernelpolicylearner method)": [[38, "src.opl.policy_learner.KernelPolicyLearner.hybrid_policy_gradient"]], "hybrid_policy_gradient() (src.opl.policy_learner.policylearner method)": [[38, "src.opl.policy_learner.PolicyLearner.hybrid_policy_gradient"]], "importance_sampling_based_policy_gradient() (src.opl.policy_learner.kernelpolicylearner method)": [[38, "src.opl.policy_learner.KernelPolicyLearner.importance_sampling_based_policy_gradient"]], "importance_sampling_based_policy_gradient() (src.opl.policy_learner.policylearner method)": [[38, "src.opl.policy_learner.PolicyLearner.importance_sampling_based_policy_gradient"]], "load() (src.opl.policy_learner.kernelpolicylearner method)": [[38, "src.opl.policy_learner.KernelPolicyLearner.load"]], "load() (src.opl.policy_learner.policylearner method)": [[38, "src.opl.policy_learner.PolicyLearner.load"]], "model_based_policy_gradient() (src.opl.policy_learner.policylearner method)": [[38, "src.opl.policy_learner.PolicyLearner.model_based_policy_gradient"]], "online_policy_gradient() (src.opl.policy_learner.policylearner method)": [[38, "src.opl.policy_learner.PolicyLearner.online_policy_gradient"]], "optimizer (src.opl.policy_learner.kernelpolicylearner attribute)": [[38, "src.opl.policy_learner.KernelPolicyLearner.optimizer"]], "optimizer (src.opl.policy_learner.policylearner attribute)": [[38, "src.opl.policy_learner.PolicyLearner.optimizer"]], "random_state (src.opl.policy_learner.kernelpolicylearner attribute)": [[38, "src.opl.policy_learner.KernelPolicyLearner.random_state"]], "random_state (src.opl.policy_learner.policylearner attribute)": [[38, "src.opl.policy_learner.PolicyLearner.random_state"]], "save() (src.opl.policy_learner.kernelpolicylearner method)": [[38, "src.opl.policy_learner.KernelPolicyLearner.save"]], "save() (src.opl.policy_learner.policylearner method)": [[38, "src.opl.policy_learner.PolicyLearner.save"]], "seed() (src.opl.policy_learner.kernelpolicylearner method)": [[38, "src.opl.policy_learner.KernelPolicyLearner.seed"]], "seed() (src.opl.policy_learner.policylearner method)": [[38, "src.opl.policy_learner.PolicyLearner.seed"]], "src.opl.policy_learner": [[38, "module-src.opl.policy_learner"]], "promptrewardlearner (class in src.opl.reward_learner)": [[39, "src.opl.reward_learner.PromptRewardLearner"]], "sentencerewardlearner (class in src.opl.reward_learner)": [[39, "src.opl.reward_learner.SentenceRewardLearner"]], "load() (src.opl.reward_learner.promptrewardlearner method)": [[39, "src.opl.reward_learner.PromptRewardLearner.load"]], "load() (src.opl.reward_learner.sentencerewardlearner method)": [[39, "src.opl.reward_learner.SentenceRewardLearner.load"]], "offline_cloning() (src.opl.reward_learner.promptrewardlearner method)": [[39, "src.opl.reward_learner.PromptRewardLearner.offline_cloning"]], "offline_training() (src.opl.reward_learner.promptrewardlearner method)": [[39, "src.opl.reward_learner.PromptRewardLearner.offline_training"]], "offline_training() (src.opl.reward_learner.sentencerewardlearner method)": [[39, "src.opl.reward_learner.SentenceRewardLearner.offline_training"]], "online_cloning() (src.opl.reward_learner.promptrewardlearner method)": [[39, "src.opl.reward_learner.PromptRewardLearner.online_cloning"]], "online_training() (src.opl.reward_learner.promptrewardlearner method)": [[39, "src.opl.reward_learner.PromptRewardLearner.online_training"]], "online_training() (src.opl.reward_learner.sentencerewardlearner method)": [[39, "src.opl.reward_learner.SentenceRewardLearner.online_training"]], "optimizer (src.opl.reward_learner.promptrewardlearner attribute)": [[39, "src.opl.reward_learner.PromptRewardLearner.optimizer"]], "optimizer (src.opl.reward_learner.sentencerewardlearner attribute)": [[39, "src.opl.reward_learner.SentenceRewardLearner.optimizer"]], "random_state (src.opl.reward_learner.promptrewardlearner attribute)": [[39, "src.opl.reward_learner.PromptRewardLearner.random_state"]], "random_state (src.opl.reward_learner.sentencerewardlearner attribute)": [[39, "src.opl.reward_learner.SentenceRewardLearner.random_state"]], "save() (src.opl.reward_learner.promptrewardlearner method)": [[39, "src.opl.reward_learner.PromptRewardLearner.save"]], "save() (src.opl.reward_learner.sentencerewardlearner method)": [[39, "src.opl.reward_learner.SentenceRewardLearner.save"]], "seed() (src.opl.reward_learner.promptrewardlearner method)": [[39, "src.opl.reward_learner.PromptRewardLearner.seed"]], "seed() (src.opl.reward_learner.sentencerewardlearner method)": [[39, "src.opl.reward_learner.SentenceRewardLearner.seed"]], "src.opl.reward_learner": [[39, "module-src.opl.reward_learner"]], "baseclusterpolicymodel (class in src.policy.base)": [[40, "src.policy.base.BaseClusterPolicyModel"]], "baseclusteringmodel (class in src.policy.base)": [[40, "src.policy.base.BaseClusteringModel"]], "baseencoder (class in src.policy.base)": [[40, "src.policy.base.BaseEncoder"]], "basekernelmarginaldensitymodel (class in src.policy.base)": [[40, "src.policy.base.BaseKernelMarginalDensityModel"]], "basepolicy (class in src.policy.base)": [[40, "src.policy.base.BasePolicy"]], "basepromptpolicymodel (class in src.policy.base)": [[40, "src.policy.base.BasePromptPolicyModel"]], "basepromptrewardmodel (class in src.policy.base)": [[40, "src.policy.base.BasePromptRewardModel"]], "basesentencerewardmodel (class in src.policy.base)": [[40, "src.policy.base.BaseSentenceRewardModel"]], "calc_action_choice_probability() (src.policy.base.baseclusterpolicymodel method)": [[40, "src.policy.base.BaseClusterPolicyModel.calc_action_choice_probability"]], "calc_action_choice_probability() (src.policy.base.basepolicy method)": [[40, "src.policy.base.BasePolicy.calc_action_choice_probability"]], "calc_action_choice_probability() (src.policy.base.basepromptpolicymodel method)": [[40, "src.policy.base.BasePromptPolicyModel.calc_action_choice_probability"]], "calc_cluster_choice_prob() (src.policy.base.baseclusteringmodel method)": [[40, "src.policy.base.BaseClusteringModel.calc_cluster_choice_prob"]], "calc_cluster_variance() (src.policy.base.baseclusteringmodel method)": [[40, "src.policy.base.BaseClusteringModel.calc_cluster_variance"]], "calc_logits() (src.policy.base.baseclusterpolicymodel method)": [[40, "src.policy.base.BaseClusterPolicyModel.calc_logits"]], "calc_logits() (src.policy.base.basepromptpolicymodel method)": [[40, "src.policy.base.BasePromptPolicyModel.calc_logits"]], "calc_pairwise_distance() (src.policy.base.basekernelmarginaldensitymodel method)": [[40, "src.policy.base.BaseKernelMarginalDensityModel.calc_pairwise_distance"]], "calc_pairwise_weight() (src.policy.base.basekernelmarginaldensitymodel method)": [[40, "src.policy.base.BaseKernelMarginalDensityModel.calc_pairwise_weight"]], "calc_prob_given_action() (src.policy.base.baseclusterpolicymodel method)": [[40, "src.policy.base.BaseClusterPolicyModel.calc_prob_given_action"]], "calc_prob_given_action() (src.policy.base.basepolicy method)": [[40, "src.policy.base.BasePolicy.calc_prob_given_action"]], "calc_prob_given_action() (src.policy.base.basepromptpolicymodel method)": [[40, "src.policy.base.BasePromptPolicyModel.calc_prob_given_action"]], "encode() (src.policy.base.baseencoder method)": [[40, "src.policy.base.BaseEncoder.encode"]], "estimate_marginal_density() (src.policy.base.basekernelmarginaldensitymodel method)": [[40, "src.policy.base.BaseKernelMarginalDensityModel.estimate_marginal_density"]], "predict_policy_value() (src.policy.base.basepolicy method)": [[40, "src.policy.base.BasePolicy.predict_policy_value"]], "predict_policy_value() (src.policy.base.basepromptpolicymodel method)": [[40, "src.policy.base.BasePromptPolicyModel.predict_policy_value"]], "predict_value() (src.policy.base.basepromptrewardmodel method)": [[40, "src.policy.base.BasePromptRewardModel.predict_value"]], "predict_value() (src.policy.base.basesentencerewardmodel method)": [[40, "src.policy.base.BaseSentenceRewardModel.predict_value"]], "predict_values() (src.policy.base.basepromptrewardmodel method)": [[40, "src.policy.base.BasePromptRewardModel.predict_values"]], "predict_values() (src.policy.base.basesentencerewardmodel method)": [[40, "src.policy.base.BaseSentenceRewardModel.predict_values"]], "retrieve_candidate_actions() (src.policy.base.baseclusteringmodel method)": [[40, "src.policy.base.BaseClusteringModel.retrieve_candidate_actions"]], "retrieve_candidate_actions_for_all_clusters() (src.policy.base.baseclusteringmodel method)": [[40, "src.policy.base.BaseClusteringModel.retrieve_candidate_actions_for_all_clusters"]], "retrieve_cluster() (src.policy.base.baseclusteringmodel method)": [[40, "src.policy.base.BaseClusteringModel.retrieve_cluster"]], "retrieve_cluster_center() (src.policy.base.baseclusteringmodel method)": [[40, "src.policy.base.BaseClusteringModel.retrieve_cluster_center"]], "retrieve_cluster_centers() (src.policy.base.baseclusteringmodel method)": [[40, "src.policy.base.BaseClusteringModel.retrieve_cluster_centers"]], "sample_action() (src.policy.base.baseclusterpolicymodel method)": [[40, "src.policy.base.BaseClusterPolicyModel.sample_action"]], "sample_action() (src.policy.base.basepolicy method)": [[40, "src.policy.base.BasePolicy.sample_action"]], "sample_action() (src.policy.base.basepromptpolicymodel method)": [[40, "src.policy.base.BasePromptPolicyModel.sample_action"]], "sample_action_and_output_prob() (src.policy.base.baseclusterpolicymodel method)": [[40, "src.policy.base.BaseClusterPolicyModel.sample_action_and_output_prob"]], "sample_action_and_output_prob() (src.policy.base.basepolicy method)": [[40, "src.policy.base.BasePolicy.sample_action_and_output_prob"]], "sample_action_and_output_prob() (src.policy.base.basepromptpolicymodel method)": [[40, "src.policy.base.BasePromptPolicyModel.sample_action_and_output_prob"]], "sample_clustering() (src.policy.base.baseclusteringmodel method)": [[40, "src.policy.base.BaseClusteringModel.sample_clustering"]], "sample_multiple_actions() (src.policy.base.baseclusterpolicymodel method)": [[40, "src.policy.base.BaseClusterPolicyModel.sample_multiple_actions"]], "sample_multiple_actions() (src.policy.base.basepolicy method)": [[40, "src.policy.base.BasePolicy.sample_multiple_actions"]], "sample_multiple_actions() (src.policy.base.basepromptpolicymodel method)": [[40, "src.policy.base.BasePromptPolicyModel.sample_multiple_actions"]], "src.policy.base": [[40, "module-src.policy.base"]], "clusterpolicy (class in src.policy.model)": [[41, "src.policy.model.ClusterPolicy"]], "kernelmarginaldensityestimator (class in src.policy.model)": [[41, "src.policy.model.KernelMarginalDensityEstimator"]], "kmeanspromptclustering (class in src.policy.model)": [[41, "src.policy.model.KmeansPromptClustering"]], "promptpolicy (class in src.policy.model)": [[41, "src.policy.model.PromptPolicy"]], "promptrewardpredictor (class in src.policy.model)": [[41, "src.policy.model.PromptRewardPredictor"]], "sentencerewardpredictor (class in src.policy.model)": [[41, "src.policy.model.SentenceRewardPredictor"]], "transformerencoder (class in src.policy.model)": [[41, "src.policy.model.TransformerEncoder"]], "calc_pairwise_distance() (src.policy.model.kernelmarginaldensityestimator method)": [[41, "src.policy.model.KernelMarginalDensityEstimator.calc_pairwise_distance"]], "encode() (src.policy.model.transformerencoder method)": [[41, "src.policy.model.TransformerEncoder.encode"]], "fit_pca() (src.policy.model.transformerencoder method)": [[41, "src.policy.model.TransformerEncoder.fit_pca"]], "retrieve_cluster() (src.policy.model.kmeanspromptclustering method)": [[41, "src.policy.model.KmeansPromptClustering.retrieve_cluster"]], "sample_clustering() (src.policy.model.kmeanspromptclustering method)": [[41, "src.policy.model.KmeansPromptClustering.sample_clustering"]], "src.policy.model": [[41, "module-src.policy.model"]], "epsilongreedypolicy (class in src.policy.policy)": [[42, "src.policy.policy.EpsilonGreedyPolicy"]], "softmaxpolicy (class in src.policy.policy)": [[42, "src.policy.policy.SoftmaxPolicy"]], "twostagepolicy (class in src.policy.policy)": [[42, "src.policy.policy.TwoStagePolicy"]], "uniformrandompolicy (class in src.policy.policy)": [[42, "src.policy.policy.UniformRandomPolicy"]], "calc_action_choice_probability() (src.policy.policy.epsilongreedypolicy method)": [[42, "src.policy.policy.EpsilonGreedyPolicy.calc_action_choice_probability"]], "calc_action_choice_probability() (src.policy.policy.softmaxpolicy method)": [[42, "src.policy.policy.SoftmaxPolicy.calc_action_choice_probability"]], "calc_action_choice_probability() (src.policy.policy.twostagepolicy method)": [[42, "src.policy.policy.TwoStagePolicy.calc_action_choice_probability"]], "calc_action_choice_probability() (src.policy.policy.uniformrandompolicy method)": [[42, "src.policy.policy.UniformRandomPolicy.calc_action_choice_probability"]], "calc_prob_given_action() (src.policy.policy.epsilongreedypolicy method)": [[42, "src.policy.policy.EpsilonGreedyPolicy.calc_prob_given_action"]], "calc_prob_given_action() (src.policy.policy.softmaxpolicy method)": [[42, "src.policy.policy.SoftmaxPolicy.calc_prob_given_action"]], "calc_prob_given_action() (src.policy.policy.twostagepolicy method)": [[42, "src.policy.policy.TwoStagePolicy.calc_prob_given_action"]], "calc_prob_given_action() (src.policy.policy.uniformrandompolicy method)": [[42, "src.policy.policy.UniformRandomPolicy.calc_prob_given_action"]], "greedy_action() (src.policy.policy.epsilongreedypolicy method)": [[42, "src.policy.policy.EpsilonGreedyPolicy.greedy_action"]], "predict_policy_value() (src.policy.policy.epsilongreedypolicy method)": [[42, "src.policy.policy.EpsilonGreedyPolicy.predict_policy_value"]], "predict_policy_value() (src.policy.policy.softmaxpolicy method)": [[42, "src.policy.policy.SoftmaxPolicy.predict_policy_value"]], "predict_policy_value() (src.policy.policy.twostagepolicy method)": [[42, "src.policy.policy.TwoStagePolicy.predict_policy_value"]], "predict_policy_value() (src.policy.policy.uniformrandompolicy method)": [[42, "src.policy.policy.UniformRandomPolicy.predict_policy_value"]], "sample_action() (src.policy.policy.epsilongreedypolicy method)": [[42, "src.policy.policy.EpsilonGreedyPolicy.sample_action"]], "sample_action() (src.policy.policy.softmaxpolicy method)": [[42, "src.policy.policy.SoftmaxPolicy.sample_action"]], "sample_action() (src.policy.policy.twostagepolicy method)": [[42, "src.policy.policy.TwoStagePolicy.sample_action"]], "sample_action() (src.policy.policy.uniformrandompolicy method)": [[42, "src.policy.policy.UniformRandomPolicy.sample_action"]], "sample_action_and_output_prob() (src.policy.policy.epsilongreedypolicy method)": [[42, "src.policy.policy.EpsilonGreedyPolicy.sample_action_and_output_prob"]], "sample_action_and_output_prob() (src.policy.policy.softmaxpolicy method)": [[42, "src.policy.policy.SoftmaxPolicy.sample_action_and_output_prob"]], "sample_action_and_output_prob() (src.policy.policy.twostagepolicy method)": [[42, "src.policy.policy.TwoStagePolicy.sample_action_and_output_prob"]], "sample_action_and_output_prob() (src.policy.policy.uniformrandompolicy method)": [[42, "src.policy.policy.UniformRandomPolicy.sample_action_and_output_prob"]], "sample_multiple_actions() (src.policy.policy.epsilongreedypolicy method)": [[42, "src.policy.policy.EpsilonGreedyPolicy.sample_multiple_actions"]], "sample_multiple_actions() (src.policy.policy.softmaxpolicy method)": [[42, "src.policy.policy.SoftmaxPolicy.sample_multiple_actions"]], "sample_multiple_actions() (src.policy.policy.twostagepolicy method)": [[42, "src.policy.policy.TwoStagePolicy.sample_multiple_actions"]], "sample_multiple_actions() (src.policy.policy.uniformrandompolicy method)": [[42, "src.policy.policy.UniformRandomPolicy.sample_multiple_actions"]], "src.policy.policy": [[42, "module-src.policy.policy"]], "frozenllmdataset (class in src.utils)": [[43, "src.utils.FrozenLLMDataset"]], "check_logged_feedback() (in module src.utils)": [[43, "src.utils.check_logged_feedback"]], "check_tensor() (in module src.utils)": [[43, "src.utils.check_tensor"]], "defaultdict_to_dict() (in module src.utils)": [[43, "src.utils.defaultdict_to_dict"]], "gaussian_kernel() (in module src.utils)": [[43, "src.utils.gaussian_kernel"]], "src.utils": [[43, "module-src.utils"]], "to_device() (in module src.utils)": [[43, "src.utils.to_device"]], "tokenize() (in module src.utils)": [[43, "src.utils.tokenize"]], "torch_seed() (in module src.utils)": [[43, "src.utils.torch_seed"]], "uniform_kernel() (in module src.utils)": [[43, "src.utils.uniform_kernel"]], "behaviorcloninglearner (class in toy.opl.behavior_cloning)": [[44, "toy.opl.behavior_cloning.BehaviorCloningLearner"], [45, "toy.opl.behavior_cloning.BehaviorCloningLearner"]], "load() (toy.opl.behavior_cloning.behaviorcloninglearner method)": [[44, "toy.opl.behavior_cloning.BehaviorCloningLearner.load"], [45, "toy.opl.behavior_cloning.BehaviorCloningLearner.load"]], "offline_cloning() (toy.opl.behavior_cloning.behaviorcloninglearner method)": [[44, "toy.opl.behavior_cloning.BehaviorCloningLearner.offline_cloning"], [45, "toy.opl.behavior_cloning.BehaviorCloningLearner.offline_cloning"]], "online_cloning() (toy.opl.behavior_cloning.behaviorcloninglearner method)": [[44, "toy.opl.behavior_cloning.BehaviorCloningLearner.online_cloning"], [45, "toy.opl.behavior_cloning.BehaviorCloningLearner.online_cloning"]], "optimizer (toy.opl.behavior_cloning.behaviorcloninglearner attribute)": [[44, "toy.opl.behavior_cloning.BehaviorCloningLearner.optimizer"], [45, "toy.opl.behavior_cloning.BehaviorCloningLearner.optimizer"]], "random_state (toy.opl.behavior_cloning.behaviorcloninglearner attribute)": [[44, "toy.opl.behavior_cloning.BehaviorCloningLearner.random_state"], [45, "toy.opl.behavior_cloning.BehaviorCloningLearner.random_state"]], "save() (toy.opl.behavior_cloning.behaviorcloninglearner method)": [[44, "toy.opl.behavior_cloning.BehaviorCloningLearner.save"], [45, "toy.opl.behavior_cloning.BehaviorCloningLearner.save"]], "seed() (toy.opl.behavior_cloning.behaviorcloninglearner method)": [[44, "toy.opl.behavior_cloning.BehaviorCloningLearner.seed"], [45, "toy.opl.behavior_cloning.BehaviorCloningLearner.seed"]], "toy.opl.behavior_cloning": [[44, "module-toy.opl.behavior_cloning"]], "marginaldensitylearner (class in toy.opl.marginal_learner)": [[46, "toy.opl.marginal_learner.MarginalDensityLearner"], [47, "toy.opl.marginal_learner.MarginalDensityLearner"]], "load() (toy.opl.marginal_learner.marginaldensitylearner method)": [[46, "toy.opl.marginal_learner.MarginalDensityLearner.load"], [47, "toy.opl.marginal_learner.MarginalDensityLearner.load"]], "optimizer (toy.opl.marginal_learner.marginaldensitylearner attribute)": [[46, "toy.opl.marginal_learner.MarginalDensityLearner.optimizer"], [47, "toy.opl.marginal_learner.MarginalDensityLearner.optimizer"]], "random_state (toy.opl.marginal_learner.marginaldensitylearner attribute)": [[46, "toy.opl.marginal_learner.MarginalDensityLearner.random_state"], [47, "toy.opl.marginal_learner.MarginalDensityLearner.random_state"]], "save() (toy.opl.marginal_learner.marginaldensitylearner method)": [[46, "toy.opl.marginal_learner.MarginalDensityLearner.save"], [47, "toy.opl.marginal_learner.MarginalDensityLearner.save"]], "seed() (toy.opl.marginal_learner.marginaldensitylearner method)": [[46, "toy.opl.marginal_learner.MarginalDensityLearner.seed"], [47, "toy.opl.marginal_learner.MarginalDensityLearner.seed"]], "simulation_training() (toy.opl.marginal_learner.marginaldensitylearner method)": [[46, "toy.opl.marginal_learner.MarginalDensityLearner.simulation_training"], [47, "toy.opl.marginal_learner.MarginalDensityLearner.simulation_training"]], "toy.opl.marginal_learner": [[46, "module-toy.opl.marginal_learner"]], "kernelpolicylearner (class in toy.opl.policy_learner)": [[48, "toy.opl.policy_learner.KernelPolicyLearner"], [49, "toy.opl.policy_learner.KernelPolicyLearner"]], "policylearner (class in toy.opl.policy_learner)": [[48, "toy.opl.policy_learner.PolicyLearner"], [50, "toy.opl.policy_learner.PolicyLearner"]], "hybrid_policy_gradient() (toy.opl.policy_learner.kernelpolicylearner method)": [[48, "toy.opl.policy_learner.KernelPolicyLearner.hybrid_policy_gradient"], [49, "toy.opl.policy_learner.KernelPolicyLearner.hybrid_policy_gradient"]], "hybrid_policy_gradient() (toy.opl.policy_learner.policylearner method)": [[48, "toy.opl.policy_learner.PolicyLearner.hybrid_policy_gradient"], [50, "toy.opl.policy_learner.PolicyLearner.hybrid_policy_gradient"]], "importance_sampling_based_policy_gradient() (toy.opl.policy_learner.kernelpolicylearner method)": [[48, "toy.opl.policy_learner.KernelPolicyLearner.importance_sampling_based_policy_gradient"], [49, "toy.opl.policy_learner.KernelPolicyLearner.importance_sampling_based_policy_gradient"]], "importance_sampling_based_policy_gradient() (toy.opl.policy_learner.policylearner method)": [[48, "toy.opl.policy_learner.PolicyLearner.importance_sampling_based_policy_gradient"], [50, "toy.opl.policy_learner.PolicyLearner.importance_sampling_based_policy_gradient"]], "load() (toy.opl.policy_learner.kernelpolicylearner method)": [[48, "toy.opl.policy_learner.KernelPolicyLearner.load"], [49, "toy.opl.policy_learner.KernelPolicyLearner.load"]], "load() (toy.opl.policy_learner.policylearner method)": [[48, "toy.opl.policy_learner.PolicyLearner.load"], [50, "toy.opl.policy_learner.PolicyLearner.load"]], "model_based_policy_gradient() (toy.opl.policy_learner.policylearner method)": [[48, "toy.opl.policy_learner.PolicyLearner.model_based_policy_gradient"], [50, "toy.opl.policy_learner.PolicyLearner.model_based_policy_gradient"]], "online_policy_gradient() (toy.opl.policy_learner.policylearner method)": [[48, "toy.opl.policy_learner.PolicyLearner.online_policy_gradient"], [50, "toy.opl.policy_learner.PolicyLearner.online_policy_gradient"]], "optimizer (toy.opl.policy_learner.kernelpolicylearner attribute)": [[48, "toy.opl.policy_learner.KernelPolicyLearner.optimizer"], [49, "toy.opl.policy_learner.KernelPolicyLearner.optimizer"]], "optimizer (toy.opl.policy_learner.policylearner attribute)": [[48, "toy.opl.policy_learner.PolicyLearner.optimizer"], [50, "toy.opl.policy_learner.PolicyLearner.optimizer"]], "random_state (toy.opl.policy_learner.kernelpolicylearner attribute)": [[48, "toy.opl.policy_learner.KernelPolicyLearner.random_state"], [49, "toy.opl.policy_learner.KernelPolicyLearner.random_state"]], "random_state (toy.opl.policy_learner.policylearner attribute)": [[48, "toy.opl.policy_learner.PolicyLearner.random_state"], [50, "toy.opl.policy_learner.PolicyLearner.random_state"]], "save() (toy.opl.policy_learner.kernelpolicylearner method)": [[48, "toy.opl.policy_learner.KernelPolicyLearner.save"], [49, "toy.opl.policy_learner.KernelPolicyLearner.save"]], "save() (toy.opl.policy_learner.policylearner method)": [[48, "toy.opl.policy_learner.PolicyLearner.save"], [50, "toy.opl.policy_learner.PolicyLearner.save"]], "seed() (toy.opl.policy_learner.kernelpolicylearner method)": [[48, "toy.opl.policy_learner.KernelPolicyLearner.seed"], [49, "toy.opl.policy_learner.KernelPolicyLearner.seed"]], "seed() (toy.opl.policy_learner.policylearner method)": [[48, "toy.opl.policy_learner.PolicyLearner.seed"], [50, "toy.opl.policy_learner.PolicyLearner.seed"]], "toy.opl.policy_learner": [[48, "module-toy.opl.policy_learner"]], "actionrewardlearner (class in toy.opl.reward_learner)": [[51, "toy.opl.reward_learner.ActionRewardLearner"], [52, "toy.opl.reward_learner.ActionRewardLearner"]], "outputrewardlearner (class in toy.opl.reward_learner)": [[51, "toy.opl.reward_learner.OutputRewardLearner"], [53, "toy.opl.reward_learner.OutputRewardLearner"]], "load() (toy.opl.reward_learner.actionrewardlearner method)": [[51, "toy.opl.reward_learner.ActionRewardLearner.load"], [52, "toy.opl.reward_learner.ActionRewardLearner.load"]], "load() (toy.opl.reward_learner.outputrewardlearner method)": [[51, "toy.opl.reward_learner.OutputRewardLearner.load"], [53, "toy.opl.reward_learner.OutputRewardLearner.load"]], "offline_cloning() (toy.opl.reward_learner.actionrewardlearner method)": [[51, "toy.opl.reward_learner.ActionRewardLearner.offline_cloning"], [52, "toy.opl.reward_learner.ActionRewardLearner.offline_cloning"]], "offline_training() (toy.opl.reward_learner.actionrewardlearner method)": [[51, "toy.opl.reward_learner.ActionRewardLearner.offline_training"], [52, "toy.opl.reward_learner.ActionRewardLearner.offline_training"]], "offline_training() (toy.opl.reward_learner.outputrewardlearner method)": [[51, "toy.opl.reward_learner.OutputRewardLearner.offline_training"], [53, "toy.opl.reward_learner.OutputRewardLearner.offline_training"]], "online_cloning() (toy.opl.reward_learner.actionrewardlearner method)": [[51, "toy.opl.reward_learner.ActionRewardLearner.online_cloning"], [52, "toy.opl.reward_learner.ActionRewardLearner.online_cloning"]], "online_training() (toy.opl.reward_learner.actionrewardlearner method)": [[51, "toy.opl.reward_learner.ActionRewardLearner.online_training"], [52, "toy.opl.reward_learner.ActionRewardLearner.online_training"]], "online_training() (toy.opl.reward_learner.outputrewardlearner method)": [[51, "toy.opl.reward_learner.OutputRewardLearner.online_training"], [53, "toy.opl.reward_learner.OutputRewardLearner.online_training"]], "optimizer (toy.opl.reward_learner.actionrewardlearner attribute)": [[51, "toy.opl.reward_learner.ActionRewardLearner.optimizer"], [52, "toy.opl.reward_learner.ActionRewardLearner.optimizer"]], "optimizer (toy.opl.reward_learner.outputrewardlearner attribute)": [[51, "toy.opl.reward_learner.OutputRewardLearner.optimizer"], [53, "toy.opl.reward_learner.OutputRewardLearner.optimizer"]], "random_state (toy.opl.reward_learner.actionrewardlearner attribute)": [[51, "toy.opl.reward_learner.ActionRewardLearner.random_state"], [52, "toy.opl.reward_learner.ActionRewardLearner.random_state"]], "random_state (toy.opl.reward_learner.outputrewardlearner attribute)": [[51, "toy.opl.reward_learner.OutputRewardLearner.random_state"], [53, "toy.opl.reward_learner.OutputRewardLearner.random_state"]], "save() (toy.opl.reward_learner.actionrewardlearner method)": [[51, "toy.opl.reward_learner.ActionRewardLearner.save"], [52, "toy.opl.reward_learner.ActionRewardLearner.save"]], "save() (toy.opl.reward_learner.outputrewardlearner method)": [[51, "toy.opl.reward_learner.OutputRewardLearner.save"], [53, "toy.opl.reward_learner.OutputRewardLearner.save"]], "seed() (toy.opl.reward_learner.actionrewardlearner method)": [[51, "toy.opl.reward_learner.ActionRewardLearner.seed"], [52, "toy.opl.reward_learner.ActionRewardLearner.seed"]], "seed() (toy.opl.reward_learner.outputrewardlearner method)": [[51, "toy.opl.reward_learner.OutputRewardLearner.seed"], [53, "toy.opl.reward_learner.OutputRewardLearner.seed"]], "toy.opl.reward_learner": [[51, "module-toy.opl.reward_learner"]], "baseactionpolicymodel (class in toy.policy.base)": [[54, "toy.policy.base.BaseActionPolicyModel"], [55, "toy.policy.base.BaseActionPolicyModel"]], "baseactionrewardmodel (class in toy.policy.base)": [[54, "toy.policy.base.BaseActionRewardModel"], [56, "toy.policy.base.BaseActionRewardModel"]], "baseclusterpolicymodel (class in toy.policy.base)": [[54, "toy.policy.base.BaseClusterPolicyModel"], [57, "toy.policy.base.BaseClusterPolicyModel"]], "baseclusteringmodel (class in toy.policy.base)": [[54, "toy.policy.base.BaseClusteringModel"], [58, "toy.policy.base.BaseClusteringModel"]], "basekernelmarginaldensitymodel (class in toy.policy.base)": [[54, "toy.policy.base.BaseKernelMarginalDensityModel"], [59, "toy.policy.base.BaseKernelMarginalDensityModel"]], "baseoutputrewardmodel (class in toy.policy.base)": [[54, "toy.policy.base.BaseOutputRewardModel"], [60, "toy.policy.base.BaseOutputRewardModel"]], "basepolicy (class in toy.policy.base)": [[54, "toy.policy.base.BasePolicy"], [61, "toy.policy.base.BasePolicy"]], "calc_action_choice_probability() (toy.policy.base.baseactionpolicymodel method)": [[54, "toy.policy.base.BaseActionPolicyModel.calc_action_choice_probability"], [55, "toy.policy.base.BaseActionPolicyModel.calc_action_choice_probability"]], "calc_action_choice_probability() (toy.policy.base.baseclusterpolicymodel method)": [[54, "toy.policy.base.BaseClusterPolicyModel.calc_action_choice_probability"], [57, "toy.policy.base.BaseClusterPolicyModel.calc_action_choice_probability"]], "calc_action_choice_probability() (toy.policy.base.basepolicy method)": [[54, "toy.policy.base.BasePolicy.calc_action_choice_probability"], [61, "toy.policy.base.BasePolicy.calc_action_choice_probability"]], "calc_cluster_choice_prob() (toy.policy.base.baseclusteringmodel method)": [[54, "toy.policy.base.BaseClusteringModel.calc_cluster_choice_prob"], [58, "toy.policy.base.BaseClusteringModel.calc_cluster_choice_prob"]], "calc_cluster_variance() (toy.policy.base.baseclusteringmodel method)": [[54, "toy.policy.base.BaseClusteringModel.calc_cluster_variance"], [58, "toy.policy.base.BaseClusteringModel.calc_cluster_variance"]], "calc_logits() (toy.policy.base.baseactionpolicymodel method)": [[54, "toy.policy.base.BaseActionPolicyModel.calc_logits"], [55, "toy.policy.base.BaseActionPolicyModel.calc_logits"]], "calc_logits() (toy.policy.base.baseclusterpolicymodel method)": [[54, "toy.policy.base.BaseClusterPolicyModel.calc_logits"], [57, "toy.policy.base.BaseClusterPolicyModel.calc_logits"]], "calc_pairwise_distance() (toy.policy.base.basekernelmarginaldensitymodel method)": [[54, "toy.policy.base.BaseKernelMarginalDensityModel.calc_pairwise_distance"], [59, "toy.policy.base.BaseKernelMarginalDensityModel.calc_pairwise_distance"]], "calc_pairwise_weight() (toy.policy.base.basekernelmarginaldensitymodel method)": [[54, "toy.policy.base.BaseKernelMarginalDensityModel.calc_pairwise_weight"], [59, "toy.policy.base.BaseKernelMarginalDensityModel.calc_pairwise_weight"]], "calc_prob_given_action() (toy.policy.base.baseactionpolicymodel method)": [[54, "toy.policy.base.BaseActionPolicyModel.calc_prob_given_action"], [55, "toy.policy.base.BaseActionPolicyModel.calc_prob_given_action"]], "calc_prob_given_action() (toy.policy.base.baseclusterpolicymodel method)": [[54, "toy.policy.base.BaseClusterPolicyModel.calc_prob_given_action"], [57, "toy.policy.base.BaseClusterPolicyModel.calc_prob_given_action"]], "calc_prob_given_action() (toy.policy.base.basepolicy method)": [[54, "toy.policy.base.BasePolicy.calc_prob_given_action"], [61, "toy.policy.base.BasePolicy.calc_prob_given_action"]], "estimate_marginal_density() (toy.policy.base.basekernelmarginaldensitymodel method)": [[54, "toy.policy.base.BaseKernelMarginalDensityModel.estimate_marginal_density"], [59, "toy.policy.base.BaseKernelMarginalDensityModel.estimate_marginal_density"]], "predict_policy_value() (toy.policy.base.baseactionpolicymodel method)": [[54, "toy.policy.base.BaseActionPolicyModel.predict_policy_value"], [55, "toy.policy.base.BaseActionPolicyModel.predict_policy_value"]], "predict_policy_value() (toy.policy.base.basepolicy method)": [[54, "toy.policy.base.BasePolicy.predict_policy_value"], [61, "toy.policy.base.BasePolicy.predict_policy_value"]], "predict_value() (toy.policy.base.baseactionrewardmodel method)": [[54, "toy.policy.base.BaseActionRewardModel.predict_value"], [56, "toy.policy.base.BaseActionRewardModel.predict_value"]], "predict_value() (toy.policy.base.baseoutputrewardmodel method)": [[54, "toy.policy.base.BaseOutputRewardModel.predict_value"], [60, "toy.policy.base.BaseOutputRewardModel.predict_value"]], "predict_values() (toy.policy.base.baseactionrewardmodel method)": [[54, "toy.policy.base.BaseActionRewardModel.predict_values"], [56, "toy.policy.base.BaseActionRewardModel.predict_values"]], "predict_values() (toy.policy.base.baseoutputrewardmodel method)": [[54, "toy.policy.base.BaseOutputRewardModel.predict_values"], [60, "toy.policy.base.BaseOutputRewardModel.predict_values"]], "retrieve_candidate_actions() (toy.policy.base.baseclusteringmodel method)": [[54, "toy.policy.base.BaseClusteringModel.retrieve_candidate_actions"], [58, "toy.policy.base.BaseClusteringModel.retrieve_candidate_actions"]], "retrieve_candidate_actions_for_all_clusters() (toy.policy.base.baseclusteringmodel method)": [[54, "toy.policy.base.BaseClusteringModel.retrieve_candidate_actions_for_all_clusters"], [58, "toy.policy.base.BaseClusteringModel.retrieve_candidate_actions_for_all_clusters"]], "retrieve_cluster() (toy.policy.base.baseclusteringmodel method)": [[54, "toy.policy.base.BaseClusteringModel.retrieve_cluster"], [58, "toy.policy.base.BaseClusteringModel.retrieve_cluster"]], "retrieve_cluster_center() (toy.policy.base.baseclusteringmodel method)": [[54, "toy.policy.base.BaseClusteringModel.retrieve_cluster_center"], [58, "toy.policy.base.BaseClusteringModel.retrieve_cluster_center"]], "retrieve_cluster_centers() (toy.policy.base.baseclusteringmodel method)": [[54, "toy.policy.base.BaseClusteringModel.retrieve_cluster_centers"], [58, "toy.policy.base.BaseClusteringModel.retrieve_cluster_centers"]], "sample_action() (toy.policy.base.baseactionpolicymodel method)": [[54, "toy.policy.base.BaseActionPolicyModel.sample_action"], [55, "toy.policy.base.BaseActionPolicyModel.sample_action"]], "sample_action() (toy.policy.base.baseclusterpolicymodel method)": [[54, "toy.policy.base.BaseClusterPolicyModel.sample_action"], [57, "toy.policy.base.BaseClusterPolicyModel.sample_action"]], "sample_action() (toy.policy.base.basepolicy method)": [[54, "toy.policy.base.BasePolicy.sample_action"], [61, "toy.policy.base.BasePolicy.sample_action"]], "sample_action_and_output_prob() (toy.policy.base.baseactionpolicymodel method)": [[54, "toy.policy.base.BaseActionPolicyModel.sample_action_and_output_prob"], [55, "toy.policy.base.BaseActionPolicyModel.sample_action_and_output_prob"]], "sample_action_and_output_prob() (toy.policy.base.baseclusterpolicymodel method)": [[54, "toy.policy.base.BaseClusterPolicyModel.sample_action_and_output_prob"], [57, "toy.policy.base.BaseClusterPolicyModel.sample_action_and_output_prob"]], "sample_action_and_output_prob() (toy.policy.base.basepolicy method)": [[54, "toy.policy.base.BasePolicy.sample_action_and_output_prob"], [61, "toy.policy.base.BasePolicy.sample_action_and_output_prob"]], "sample_clustering() (toy.policy.base.baseclusteringmodel method)": [[54, "toy.policy.base.BaseClusteringModel.sample_clustering"], [58, "toy.policy.base.BaseClusteringModel.sample_clustering"]], "sample_multiple_actions() (toy.policy.base.baseactionpolicymodel method)": [[54, "toy.policy.base.BaseActionPolicyModel.sample_multiple_actions"], [55, "toy.policy.base.BaseActionPolicyModel.sample_multiple_actions"]], "sample_multiple_actions() (toy.policy.base.baseclusterpolicymodel method)": [[54, "toy.policy.base.BaseClusterPolicyModel.sample_multiple_actions"], [57, "toy.policy.base.BaseClusterPolicyModel.sample_multiple_actions"]], "sample_multiple_actions() (toy.policy.base.basepolicy method)": [[54, "toy.policy.base.BasePolicy.sample_multiple_actions"], [61, "toy.policy.base.BasePolicy.sample_multiple_actions"]], "toy.policy.base": [[54, "module-toy.policy.base"]], "add_module() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.add_module"]], "apply() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.apply"]], "bfloat16() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.bfloat16"]], "buffers() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.buffers"]], "children() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.children"]], "compile() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.compile"]], "cpu() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.cpu"]], "cuda() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.cuda"]], "double() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.double"]], "eval() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.eval"]], "extra_repr() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.extra_repr"]], "float() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.float"]], "get_buffer() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.get_buffer"]], "get_extra_state() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.get_extra_state"]], "get_parameter() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.get_parameter"]], "get_submodule() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.get_submodule"]], "half() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.half"]], "ipu() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.ipu"]], "load_state_dict() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.load_state_dict"]], "modules() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.modules"]], "named_buffers() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.named_buffers"]], "named_children() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.named_children"]], "named_modules() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.named_modules"]], "named_parameters() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.named_parameters"]], "parameters() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.parameters"]], "register_backward_hook() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_backward_hook"]], "register_buffer() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_buffer"]], "register_forward_hook() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_load_state_dict_post_hook"]], "register_module() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_module"]], "register_parameter() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.requires_grad_"]], "set_extra_state() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.set_extra_state"]], "share_memory() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.share_memory"]], "state_dict() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.state_dict"]], "to() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.to"]], "to_empty() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.to_empty"]], "train() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.train"]], "type() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.type"]], "xpu() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.xpu"]], "zero_grad() (toy.policy.base.baseactionpolicymodel method)": [[55, "toy.policy.base.BaseActionPolicyModel.zero_grad"]], "add_module() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.add_module"]], "apply() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.apply"]], "bfloat16() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.bfloat16"]], "buffers() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.buffers"]], "children() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.children"]], "compile() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.compile"]], "cpu() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.cpu"]], "cuda() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.cuda"]], "double() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.double"]], "eval() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.eval"]], "extra_repr() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.extra_repr"]], "float() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.float"]], "get_buffer() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.get_buffer"]], "get_extra_state() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.get_extra_state"]], "get_parameter() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.get_parameter"]], "get_submodule() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.get_submodule"]], "half() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.half"]], "ipu() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.ipu"]], "load_state_dict() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.load_state_dict"]], "modules() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.modules"]], "named_buffers() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.named_buffers"]], "named_children() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.named_children"]], "named_modules() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.named_modules"]], "named_parameters() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.named_parameters"]], "parameters() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.parameters"]], "register_backward_hook() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_backward_hook"]], "register_buffer() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_buffer"]], "register_forward_hook() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_load_state_dict_post_hook"]], "register_module() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_module"]], "register_parameter() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.requires_grad_"]], "set_extra_state() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.set_extra_state"]], "share_memory() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.share_memory"]], "state_dict() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.state_dict"]], "to() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.to"]], "to_empty() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.to_empty"]], "train() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.train"]], "type() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.type"]], "xpu() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.xpu"]], "zero_grad() (toy.policy.base.baseactionrewardmodel method)": [[56, "toy.policy.base.BaseActionRewardModel.zero_grad"]], "add_module() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.add_module"]], "apply() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.apply"]], "bfloat16() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.bfloat16"]], "buffers() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.buffers"]], "children() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.children"]], "compile() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.compile"]], "cpu() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.cpu"]], "cuda() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.cuda"]], "double() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.double"]], "eval() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.eval"]], "extra_repr() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.extra_repr"]], "float() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.float"]], "get_buffer() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.get_buffer"]], "get_extra_state() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.get_extra_state"]], "get_parameter() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.get_parameter"]], "get_submodule() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.get_submodule"]], "half() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.half"]], "ipu() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.ipu"]], "load_state_dict() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.load_state_dict"]], "modules() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.modules"]], "named_buffers() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.named_buffers"]], "named_children() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.named_children"]], "named_modules() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.named_modules"]], "named_parameters() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.named_parameters"]], "parameters() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.parameters"]], "register_backward_hook() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_backward_hook"]], "register_buffer() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_buffer"]], "register_forward_hook() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_load_state_dict_post_hook"]], "register_module() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_module"]], "register_parameter() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.requires_grad_"]], "set_extra_state() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.set_extra_state"]], "share_memory() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.share_memory"]], "state_dict() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.state_dict"]], "to() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.to"]], "to_empty() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.to_empty"]], "train() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.train"]], "type() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.type"]], "xpu() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.xpu"]], "zero_grad() (toy.policy.base.baseclusterpolicymodel method)": [[57, "toy.policy.base.BaseClusterPolicyModel.zero_grad"]], "add_module() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.add_module"]], "apply() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.apply"]], "bfloat16() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.bfloat16"]], "buffers() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.buffers"]], "children() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.children"]], "compile() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.compile"]], "cpu() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.cpu"]], "cuda() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.cuda"]], "double() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.double"]], "eval() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.eval"]], "extra_repr() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.extra_repr"]], "float() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.float"]], "get_buffer() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.get_buffer"]], "get_extra_state() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.get_extra_state"]], "get_parameter() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.get_parameter"]], "get_submodule() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.get_submodule"]], "half() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.half"]], "ipu() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.ipu"]], "load_state_dict() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.load_state_dict"]], "modules() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.modules"]], "named_buffers() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.named_buffers"]], "named_children() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.named_children"]], "named_modules() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.named_modules"]], "named_parameters() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.named_parameters"]], "parameters() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.parameters"]], "register_backward_hook() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_backward_hook"]], "register_buffer() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_buffer"]], "register_forward_hook() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_load_state_dict_post_hook"]], "register_module() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_module"]], "register_parameter() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.requires_grad_"]], "set_extra_state() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.set_extra_state"]], "share_memory() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.share_memory"]], "state_dict() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.state_dict"]], "to() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.to"]], "to_empty() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.to_empty"]], "train() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.train"]], "type() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.type"]], "xpu() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.xpu"]], "zero_grad() (toy.policy.base.baseclusteringmodel method)": [[58, "toy.policy.base.BaseClusteringModel.zero_grad"]], "add_module() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.add_module"]], "apply() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.apply"]], "bfloat16() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.bfloat16"]], "buffers() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.buffers"]], "children() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.children"]], "compile() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.compile"]], "cpu() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.cpu"]], "cuda() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.cuda"]], "double() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.double"]], "eval() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.eval"]], "extra_repr() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.extra_repr"]], "float() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.float"]], "get_buffer() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.get_buffer"]], "get_extra_state() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.get_extra_state"]], "get_parameter() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.get_parameter"]], "get_submodule() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.get_submodule"]], "half() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.half"]], "ipu() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.ipu"]], "load_state_dict() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.load_state_dict"]], "modules() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.modules"]], "named_buffers() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.named_buffers"]], "named_children() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.named_children"]], "named_modules() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.named_modules"]], "named_parameters() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.named_parameters"]], "parameters() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.parameters"]], "register_backward_hook() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_backward_hook"]], "register_buffer() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_buffer"]], "register_forward_hook() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_load_state_dict_post_hook"]], "register_module() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_module"]], "register_parameter() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.requires_grad_"]], "set_extra_state() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.set_extra_state"]], "share_memory() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.share_memory"]], "state_dict() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.state_dict"]], "to() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.to"]], "to_empty() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.to_empty"]], "train() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.train"]], "type() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.type"]], "xpu() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.xpu"]], "zero_grad() (toy.policy.base.basekernelmarginaldensitymodel method)": [[59, "toy.policy.base.BaseKernelMarginalDensityModel.zero_grad"]], "add_module() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.add_module"]], "apply() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.apply"]], "bfloat16() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.bfloat16"]], "buffers() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.buffers"]], "children() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.children"]], "compile() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.compile"]], "cpu() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.cpu"]], "cuda() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.cuda"]], "double() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.double"]], "eval() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.eval"]], "extra_repr() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.extra_repr"]], "float() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.float"]], "get_buffer() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.get_buffer"]], "get_extra_state() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.get_extra_state"]], "get_parameter() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.get_parameter"]], "get_submodule() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.get_submodule"]], "half() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.half"]], "ipu() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.ipu"]], "load_state_dict() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.load_state_dict"]], "modules() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.modules"]], "named_buffers() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.named_buffers"]], "named_children() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.named_children"]], "named_modules() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.named_modules"]], "named_parameters() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.named_parameters"]], "parameters() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.parameters"]], "register_backward_hook() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_backward_hook"]], "register_buffer() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_buffer"]], "register_forward_hook() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_load_state_dict_post_hook"]], "register_module() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_module"]], "register_parameter() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.requires_grad_"]], "set_extra_state() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.set_extra_state"]], "share_memory() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.share_memory"]], "state_dict() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.state_dict"]], "to() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.to"]], "to_empty() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.to_empty"]], "train() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.train"]], "type() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.type"]], "xpu() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.xpu"]], "zero_grad() (toy.policy.base.baseoutputrewardmodel method)": [[60, "toy.policy.base.BaseOutputRewardModel.zero_grad"]], "kmeansactionclustering (class in toy.policy.model)": [[62, "toy.policy.model.KmeansActionClustering"], [63, "toy.policy.model.KmeansActionClustering"]], "neuralactionpolicy (class in toy.policy.model)": [[62, "toy.policy.model.NeuralActionPolicy"], [64, "toy.policy.model.NeuralActionPolicy"]], "neuralactionrewardpredictor (class in toy.policy.model)": [[62, "toy.policy.model.NeuralActionRewardPredictor"], [65, "toy.policy.model.NeuralActionRewardPredictor"]], "neuralclusterpolicy (class in toy.policy.model)": [[62, "toy.policy.model.NeuralClusterPolicy"], [66, "toy.policy.model.NeuralClusterPolicy"]], "neuralmarginaldensityestimator (class in toy.policy.model)": [[62, "toy.policy.model.NeuralMarginalDensityEstimator"], [67, "toy.policy.model.NeuralMarginalDensityEstimator"]], "neuraloutputrewardpredictor (class in toy.policy.model)": [[62, "toy.policy.model.NeuralOutputRewardPredictor"], [68, "toy.policy.model.NeuralOutputRewardPredictor"]], "calc_pairwise_distance() (toy.policy.model.neuralmarginaldensityestimator method)": [[62, "toy.policy.model.NeuralMarginalDensityEstimator.calc_pairwise_distance"], [67, "toy.policy.model.NeuralMarginalDensityEstimator.calc_pairwise_distance"]], "retrieve_cluster() (toy.policy.model.kmeansactionclustering method)": [[62, "toy.policy.model.KmeansActionClustering.retrieve_cluster"], [63, "toy.policy.model.KmeansActionClustering.retrieve_cluster"]], "sample_clustering() (toy.policy.model.kmeansactionclustering method)": [[62, "toy.policy.model.KmeansActionClustering.sample_clustering"], [63, "toy.policy.model.KmeansActionClustering.sample_clustering"]], "toy.policy.model": [[62, "module-toy.policy.model"]], "add_module() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.add_module"]], "apply() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.apply"]], "bfloat16() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.bfloat16"]], "buffers() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.buffers"]], "calc_cluster_choice_prob() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.calc_cluster_choice_prob"]], "calc_cluster_variance() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.calc_cluster_variance"]], "children() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.children"]], "compile() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.compile"]], "cpu() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.cpu"]], "cuda() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.cuda"]], "double() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.double"]], "eval() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.eval"]], "extra_repr() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.extra_repr"]], "float() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.float"]], "get_buffer() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.get_buffer"]], "get_extra_state() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.get_extra_state"]], "get_parameter() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.get_parameter"]], "get_submodule() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.get_submodule"]], "half() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.half"]], "ipu() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.ipu"]], "load_state_dict() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.load_state_dict"]], "modules() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.modules"]], "named_buffers() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.named_buffers"]], "named_children() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.named_children"]], "named_modules() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.named_modules"]], "named_parameters() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.named_parameters"]], "parameters() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.parameters"]], "register_backward_hook() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_backward_hook"]], "register_buffer() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_buffer"]], "register_forward_hook() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_load_state_dict_post_hook"]], "register_module() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_module"]], "register_parameter() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.requires_grad_"]], "retrieve_candidate_actions() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.retrieve_candidate_actions"]], "retrieve_candidate_actions_for_all_clusters() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.retrieve_candidate_actions_for_all_clusters"]], "retrieve_cluster_center() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.retrieve_cluster_center"]], "retrieve_cluster_centers() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.retrieve_cluster_centers"]], "set_extra_state() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.set_extra_state"]], "share_memory() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.share_memory"]], "state_dict() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.state_dict"]], "to() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.to"]], "to_empty() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.to_empty"]], "train() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.train"]], "type() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.type"]], "xpu() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.xpu"]], "zero_grad() (toy.policy.model.kmeansactionclustering method)": [[63, "toy.policy.model.KmeansActionClustering.zero_grad"]], "add_module() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.add_module"]], "apply() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.apply"]], "bfloat16() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.bfloat16"]], "buffers() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.buffers"]], "calc_action_choice_probability() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.calc_action_choice_probability"]], "calc_logits() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.calc_logits"]], "calc_prob_given_action() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.calc_prob_given_action"]], "children() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.children"]], "compile() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.compile"]], "cpu() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.cpu"]], "cuda() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.cuda"]], "double() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.double"]], "eval() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.eval"]], "extra_repr() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.extra_repr"]], "float() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.float"]], "get_buffer() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.get_buffer"]], "get_extra_state() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.get_extra_state"]], "get_parameter() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.get_parameter"]], "get_submodule() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.get_submodule"]], "half() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.half"]], "ipu() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.ipu"]], "load_state_dict() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.load_state_dict"]], "modules() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.modules"]], "named_buffers() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.named_buffers"]], "named_children() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.named_children"]], "named_modules() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.named_modules"]], "named_parameters() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.named_parameters"]], "parameters() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.parameters"]], "predict_policy_value() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.predict_policy_value"]], "register_backward_hook() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_backward_hook"]], "register_buffer() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_buffer"]], "register_forward_hook() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_load_state_dict_post_hook"]], "register_module() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_module"]], "register_parameter() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.requires_grad_"]], "sample_action() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.sample_action"]], "sample_action_and_output_prob() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.sample_action_and_output_prob"]], "sample_multiple_actions() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.sample_multiple_actions"]], "set_extra_state() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.set_extra_state"]], "share_memory() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.share_memory"]], "state_dict() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.state_dict"]], "to() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.to"]], "to_empty() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.to_empty"]], "train() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.train"]], "type() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.type"]], "xpu() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.xpu"]], "zero_grad() (toy.policy.model.neuralactionpolicy method)": [[64, "toy.policy.model.NeuralActionPolicy.zero_grad"]], "add_module() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.add_module"]], "apply() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.apply"]], "bfloat16() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.bfloat16"]], "buffers() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.buffers"]], "children() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.children"]], "compile() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.compile"]], "cpu() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.cpu"]], "cuda() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.cuda"]], "double() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.double"]], "eval() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.eval"]], "extra_repr() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.extra_repr"]], "float() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.float"]], "get_buffer() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.get_buffer"]], "get_extra_state() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.get_extra_state"]], "get_parameter() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.get_parameter"]], "get_submodule() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.get_submodule"]], "half() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.half"]], "ipu() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.ipu"]], "load_state_dict() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.load_state_dict"]], "modules() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.modules"]], "named_buffers() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.named_buffers"]], "named_children() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.named_children"]], "named_modules() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.named_modules"]], "named_parameters() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.named_parameters"]], "parameters() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.parameters"]], "predict_value() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.predict_value"]], "predict_values() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.predict_values"]], "register_backward_hook() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_backward_hook"]], "register_buffer() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_buffer"]], "register_forward_hook() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_load_state_dict_post_hook"]], "register_module() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_module"]], "register_parameter() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.requires_grad_"]], "set_extra_state() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.set_extra_state"]], "share_memory() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.share_memory"]], "state_dict() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.state_dict"]], "to() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.to"]], "to_empty() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.to_empty"]], "train() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.train"]], "type() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.type"]], "xpu() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.xpu"]], "zero_grad() (toy.policy.model.neuralactionrewardpredictor method)": [[65, "toy.policy.model.NeuralActionRewardPredictor.zero_grad"]], "add_module() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.add_module"]], "apply() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.apply"]], "bfloat16() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.bfloat16"]], "buffers() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.buffers"]], "calc_action_choice_probability() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.calc_action_choice_probability"]], "calc_logits() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.calc_logits"]], "calc_prob_given_action() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.calc_prob_given_action"]], "children() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.children"]], "compile() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.compile"]], "cpu() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.cpu"]], "cuda() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.cuda"]], "double() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.double"]], "eval() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.eval"]], "extra_repr() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.extra_repr"]], "float() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.float"]], "get_buffer() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.get_buffer"]], "get_extra_state() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.get_extra_state"]], "get_parameter() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.get_parameter"]], "get_submodule() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.get_submodule"]], "half() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.half"]], "ipu() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.ipu"]], "load_state_dict() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.load_state_dict"]], "modules() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.modules"]], "named_buffers() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.named_buffers"]], "named_children() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.named_children"]], "named_modules() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.named_modules"]], "named_parameters() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.named_parameters"]], "parameters() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.parameters"]], "register_backward_hook() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_backward_hook"]], "register_buffer() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_buffer"]], "register_forward_hook() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_load_state_dict_post_hook"]], "register_module() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_module"]], "register_parameter() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.requires_grad_"]], "sample_action() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.sample_action"]], "sample_action_and_output_prob() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.sample_action_and_output_prob"]], "sample_multiple_actions() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.sample_multiple_actions"]], "set_extra_state() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.set_extra_state"]], "share_memory() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.share_memory"]], "state_dict() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.state_dict"]], "to() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.to"]], "to_empty() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.to_empty"]], "train() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.train"]], "type() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.type"]], "xpu() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.xpu"]], "zero_grad() (toy.policy.model.neuralclusterpolicy method)": [[66, "toy.policy.model.NeuralClusterPolicy.zero_grad"]], "add_module() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.add_module"]], "apply() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.apply"]], "bfloat16() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.bfloat16"]], "buffers() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.buffers"]], "calc_pairwise_weight() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.calc_pairwise_weight"]], "children() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.children"]], "compile() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.compile"]], "cpu() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.cpu"]], "cuda() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.cuda"]], "double() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.double"]], "estimate_marginal_density() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.estimate_marginal_density"]], "eval() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.eval"]], "extra_repr() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.extra_repr"]], "float() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.float"]], "get_buffer() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.get_buffer"]], "get_extra_state() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.get_extra_state"]], "get_parameter() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.get_parameter"]], "get_submodule() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.get_submodule"]], "half() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.half"]], "ipu() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.ipu"]], "load_state_dict() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.load_state_dict"]], "modules() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.modules"]], "named_buffers() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.named_buffers"]], "named_children() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.named_children"]], "named_modules() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.named_modules"]], "named_parameters() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.named_parameters"]], "parameters() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.parameters"]], "register_backward_hook() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_backward_hook"]], "register_buffer() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_buffer"]], "register_forward_hook() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_load_state_dict_post_hook"]], "register_module() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_module"]], "register_parameter() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.requires_grad_"]], "set_extra_state() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.set_extra_state"]], "share_memory() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.share_memory"]], "state_dict() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.state_dict"]], "to() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.to"]], "to_empty() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.to_empty"]], "train() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.train"]], "type() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.type"]], "xpu() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.xpu"]], "zero_grad() (toy.policy.model.neuralmarginaldensityestimator method)": [[67, "toy.policy.model.NeuralMarginalDensityEstimator.zero_grad"]], "add_module() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.add_module"]], "apply() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.apply"]], "bfloat16() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.bfloat16"]], "buffers() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.buffers"]], "children() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.children"]], "compile() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.compile"]], "cpu() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.cpu"]], "cuda() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.cuda"]], "double() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.double"]], "eval() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.eval"]], "extra_repr() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.extra_repr"]], "float() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.float"]], "get_buffer() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.get_buffer"]], "get_extra_state() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.get_extra_state"]], "get_parameter() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.get_parameter"]], "get_submodule() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.get_submodule"]], "half() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.half"]], "ipu() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.ipu"]], "load_state_dict() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.load_state_dict"]], "modules() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.modules"]], "named_buffers() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.named_buffers"]], "named_children() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.named_children"]], "named_modules() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.named_modules"]], "named_parameters() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.named_parameters"]], "parameters() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.parameters"]], "predict_value() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.predict_value"]], "predict_values() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.predict_values"]], "register_backward_hook() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_backward_hook"]], "register_buffer() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_buffer"]], "register_forward_hook() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_forward_hook"]], "register_forward_pre_hook() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_forward_pre_hook"]], "register_full_backward_hook() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_full_backward_hook"]], "register_full_backward_pre_hook() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_full_backward_pre_hook"]], "register_load_state_dict_post_hook() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_load_state_dict_post_hook"]], "register_module() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_module"]], "register_parameter() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_parameter"]], "register_state_dict_pre_hook() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.register_state_dict_pre_hook"]], "requires_grad_() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.requires_grad_"]], "set_extra_state() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.set_extra_state"]], "share_memory() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.share_memory"]], "state_dict() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.state_dict"]], "to() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.to"]], "to_empty() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.to_empty"]], "train() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.train"]], "type() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.type"]], "xpu() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.xpu"]], "zero_grad() (toy.policy.model.neuraloutputrewardpredictor method)": [[68, "toy.policy.model.NeuralOutputRewardPredictor.zero_grad"]], "epsilongreedypolicy (class in toy.policy.policy)": [[69, "toy.policy.policy.EpsilonGreedyPolicy"], [70, "toy.policy.policy.EpsilonGreedyPolicy"]], "softmaxpolicy (class in toy.policy.policy)": [[69, "toy.policy.policy.SoftmaxPolicy"], [71, "toy.policy.policy.SoftmaxPolicy"]], "twostagepolicy (class in toy.policy.policy)": [[69, "toy.policy.policy.TwoStagePolicy"], [72, "toy.policy.policy.TwoStagePolicy"]], "uniformrandompolicy (class in toy.policy.policy)": [[69, "toy.policy.policy.UniformRandomPolicy"], [73, "toy.policy.policy.UniformRandomPolicy"]], "calc_action_choice_probability() (toy.policy.policy.epsilongreedypolicy method)": [[69, "toy.policy.policy.EpsilonGreedyPolicy.calc_action_choice_probability"], [70, "toy.policy.policy.EpsilonGreedyPolicy.calc_action_choice_probability"]], "calc_action_choice_probability() (toy.policy.policy.softmaxpolicy method)": [[69, "toy.policy.policy.SoftmaxPolicy.calc_action_choice_probability"], [71, "toy.policy.policy.SoftmaxPolicy.calc_action_choice_probability"]], "calc_action_choice_probability() (toy.policy.policy.twostagepolicy method)": [[69, "toy.policy.policy.TwoStagePolicy.calc_action_choice_probability"], [72, "toy.policy.policy.TwoStagePolicy.calc_action_choice_probability"]], "calc_action_choice_probability() (toy.policy.policy.uniformrandompolicy method)": [[69, "toy.policy.policy.UniformRandomPolicy.calc_action_choice_probability"], [73, "toy.policy.policy.UniformRandomPolicy.calc_action_choice_probability"]], "calc_prob_given_action() (toy.policy.policy.epsilongreedypolicy method)": [[69, "toy.policy.policy.EpsilonGreedyPolicy.calc_prob_given_action"], [70, "toy.policy.policy.EpsilonGreedyPolicy.calc_prob_given_action"]], "calc_prob_given_action() (toy.policy.policy.softmaxpolicy method)": [[69, "toy.policy.policy.SoftmaxPolicy.calc_prob_given_action"], [71, "toy.policy.policy.SoftmaxPolicy.calc_prob_given_action"]], "calc_prob_given_action() (toy.policy.policy.twostagepolicy method)": [[69, "toy.policy.policy.TwoStagePolicy.calc_prob_given_action"], [72, "toy.policy.policy.TwoStagePolicy.calc_prob_given_action"]], "calc_prob_given_action() (toy.policy.policy.uniformrandompolicy method)": [[69, "toy.policy.policy.UniformRandomPolicy.calc_prob_given_action"], [73, "toy.policy.policy.UniformRandomPolicy.calc_prob_given_action"]], "greedy_action() (toy.policy.policy.epsilongreedypolicy method)": [[69, "toy.policy.policy.EpsilonGreedyPolicy.greedy_action"], [70, "toy.policy.policy.EpsilonGreedyPolicy.greedy_action"]], "predict_policy_value() (toy.policy.policy.epsilongreedypolicy method)": [[69, "toy.policy.policy.EpsilonGreedyPolicy.predict_policy_value"], [70, "toy.policy.policy.EpsilonGreedyPolicy.predict_policy_value"]], "predict_policy_value() (toy.policy.policy.softmaxpolicy method)": [[69, "toy.policy.policy.SoftmaxPolicy.predict_policy_value"], [71, "toy.policy.policy.SoftmaxPolicy.predict_policy_value"]], "predict_policy_value() (toy.policy.policy.twostagepolicy method)": [[69, "toy.policy.policy.TwoStagePolicy.predict_policy_value"], [72, "toy.policy.policy.TwoStagePolicy.predict_policy_value"]], "predict_policy_value() (toy.policy.policy.uniformrandompolicy method)": [[69, "toy.policy.policy.UniformRandomPolicy.predict_policy_value"], [73, "toy.policy.policy.UniformRandomPolicy.predict_policy_value"]], "sample_action() (toy.policy.policy.epsilongreedypolicy method)": [[69, "toy.policy.policy.EpsilonGreedyPolicy.sample_action"], [70, "toy.policy.policy.EpsilonGreedyPolicy.sample_action"]], "sample_action() (toy.policy.policy.softmaxpolicy method)": [[69, "toy.policy.policy.SoftmaxPolicy.sample_action"], [71, "toy.policy.policy.SoftmaxPolicy.sample_action"]], "sample_action() (toy.policy.policy.twostagepolicy method)": [[69, "toy.policy.policy.TwoStagePolicy.sample_action"], [72, "toy.policy.policy.TwoStagePolicy.sample_action"]], "sample_action() (toy.policy.policy.uniformrandompolicy method)": [[69, "toy.policy.policy.UniformRandomPolicy.sample_action"], [73, "toy.policy.policy.UniformRandomPolicy.sample_action"]], "sample_action_and_output_prob() (toy.policy.policy.epsilongreedypolicy method)": [[69, "toy.policy.policy.EpsilonGreedyPolicy.sample_action_and_output_prob"], [70, "toy.policy.policy.EpsilonGreedyPolicy.sample_action_and_output_prob"]], "sample_action_and_output_prob() (toy.policy.policy.softmaxpolicy method)": [[69, "toy.policy.policy.SoftmaxPolicy.sample_action_and_output_prob"], [71, "toy.policy.policy.SoftmaxPolicy.sample_action_and_output_prob"]], "sample_action_and_output_prob() (toy.policy.policy.twostagepolicy method)": [[69, "toy.policy.policy.TwoStagePolicy.sample_action_and_output_prob"], [72, "toy.policy.policy.TwoStagePolicy.sample_action_and_output_prob"]], "sample_action_and_output_prob() (toy.policy.policy.uniformrandompolicy method)": [[69, "toy.policy.policy.UniformRandomPolicy.sample_action_and_output_prob"], [73, "toy.policy.policy.UniformRandomPolicy.sample_action_and_output_prob"]], "sample_multiple_actions() (toy.policy.policy.epsilongreedypolicy method)": [[69, "toy.policy.policy.EpsilonGreedyPolicy.sample_multiple_actions"], [70, "toy.policy.policy.EpsilonGreedyPolicy.sample_multiple_actions"]], "sample_multiple_actions() (toy.policy.policy.softmaxpolicy method)": [[69, "toy.policy.policy.SoftmaxPolicy.sample_multiple_actions"], [71, "toy.policy.policy.SoftmaxPolicy.sample_multiple_actions"]], "sample_multiple_actions() (toy.policy.policy.twostagepolicy method)": [[69, "toy.policy.policy.TwoStagePolicy.sample_multiple_actions"], [72, "toy.policy.policy.TwoStagePolicy.sample_multiple_actions"]], "sample_multiple_actions() (toy.policy.policy.uniformrandompolicy method)": [[69, "toy.policy.policy.UniformRandomPolicy.sample_multiple_actions"], [73, "toy.policy.policy.UniformRandomPolicy.sample_multiple_actions"]], "toy.policy.policy": [[69, "module-toy.policy.policy"]], "check_logged_feedback() (in module toy.utils)": [[74, "toy.utils.check_logged_feedback"], [75, "toy.utils.check_logged_feedback"]], "check_tensor() (in module toy.utils)": [[74, "toy.utils.check_tensor"], [76, "toy.utils.check_tensor"]], "defaultdict_to_dict() (in module toy.utils)": [[74, "toy.utils.defaultdict_to_dict"], [77, "toy.utils.defaultdict_to_dict"]], "gaussian_kernel() (in module toy.utils)": [[74, "toy.utils.gaussian_kernel"], [78, "toy.utils.gaussian_kernel"]], "torch_seed() (in module toy.utils)": [[74, "toy.utils.torch_seed"], [79, "toy.utils.torch_seed"]], "toy.utils": [[74, "module-toy.utils"]], "uniform_kernel() (in module toy.utils)": [[74, "toy.utils.uniform_kernel"], [80, "toy.utils.uniform_kernel"]]}})