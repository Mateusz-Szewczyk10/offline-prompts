# OfflinePrompts: Benchmark Suites for Prompt-guided Text Personalization from Logged Data (Python)

<div align="center"><img src="https://raw.githubusercontent.com/aiueola/offline-prompts/main/images/logo.png" width="100%"/></div>

[![pypi](https://img.shields.io/pypi/v/offline-prompts.svg)](https://pypi.python.org/pypi/offline-prompts)
[![Python](https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11-blue)](https://www.python.org)
[![Downloads](https://pepy.tech/badge/offline-prompts)](https://pepy.tech/project/offline-prompts)
[![GitHub commit activity](https://img.shields.io/github/commit-activity/m/aiueola/offline-prompts)](https://github.com/aiueola/offline-prompts/graphs/contributors)
[![GitHub last commit](https://img.shields.io/github/last-commit/aiueola/offline-prompts)](https://github.com/aiueola/offline-prompts/graphs/commit-activity)
[![Documentation Status](https://readthedocs.org/projects/offline-prompts/badge/?version=latest)](https://offline-prompts.readthedocs.io/en/latest/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![arXiv](https://img.shields.io/badge/arXiv-2504.02646-b31b1b.svg)](https://arxiv.org/abs/2504.02646)
[![arXiv](https://img.shields.io/badge/arXiv-xxxx.xxxxx-b31b1b.svg)](https://arxiv.org/abs/xxxx.xxxxx)

<details>
<summary><strong>Table of Contents </strong>(click to expand)</summary>

- [OfflinePrompts: Benchmark Suites for Prompt-guided Text Personalization from Logged Data (Python)](#offlineprompts-benchmark-suites-for-prompt-guided-text-personalization-from-logged-data-python)
- [Overview](#overview)
- [Installation](#installation)
- [Module](#module)
- [Usage](#usage)
- [Citation](#citation)
- [Project Team](#project-team)
- [License](#license)
- [Contact](#contact)
- [Reference](#reference)

</details>

**Pretrained simulator and datasets are available [here](https://drive.google.com/drive/folders/1gH9MvoNwOC1GfT1Dxb5v4cFsnw5mt1KT?usp=sharing).**

**Documentation is available [here]() (debugging is in progress).**

**Stable versions are available at [PyPI]().**

**Slides are available [here](https://speakerdeck.com/aiueola/opl-prompt).**

**日本語は[こちら](README_ja.md)。**

## Overview

Personalizing sentence generation is crucial in interactive platforms including recommender systems, online ads, and educational apps. For example, consider a situation where we recommend the movie "Wall-E (2008)" with some short descriptions or slogans of the movie, as shown in the following figure. If the user is a sci-fi lover, the description focusing on the "sci-fi" aspect of the movie would gain more attention than that focusing on the "romance" aspect, and vice versa. Such personalization is crucial for maximizing some business metrics and users' welfare.

<div align="center"><img src="https://raw.githubusercontent.com/aiueola/offline-prompts/main/images/motivative_example.png" width="100%"/></div>
<figcaption>
<p align="center">
  Motivative example of for generating personalized sentence in movie recommendation.
</p>
</figcaption>

This package, called **OfflinePrompt**, aims to provide tools for the above personalization focusing on the prompt-policy learning from logged bandit data. Specifically, we consider the following workflow -- (i) For each coming user, a policy chooses which prompt to use to generate sentences with a frozen LLM. (ii) Each user observes only the sentence generated by the chosen prompt and provides the reward for the corresponding sentence. -- The following figure illustrates the interaction process. Through daily operations in the platform, a (logging) policy naturally collects user responses to the sentence generated by the logging policy. Our goal is to use such informative logged data to learn or evaluate a new policy **(Off-policy evaluation and learning; OPE/OPL)** for better personalization in the future. 

<div align="center"><img src="https://raw.githubusercontent.com/aiueola/offline-prompts/main/images/personalized_sentence_generation.png" width="100%"/></div>
<figcaption>
<p align="center">
  Prompt-guided sentence personalization as a off-policy learning of contextual bandits. 
</p>
</figcaption>

To facilitate the research and practical applications of OPL of prompt policies, OfflinePrompts contributes to the followings; 

- To implement representative OPL methods for prompt optimization. 
- To provide two benchmark environments, including synthetic and full-LLM.
- To streamline the workflow to connect OPL and sentence generation modules for smooth experimentation. 

In particular, the provided full-LLM benchmark models realistic users' responses to the presented sentence in the movie recommendation settings, by training a semi-synthetic reward simulator with the sentence-augmented [MovieLens](https://grouplens.org/datasets/movielens/) dataset. The benchmark design is a remarkable contribution to the research community, as we currently lack a realistic simulator on personalized sentence generation tasks that satisfies the following four key qualifications:

- LLMs have knowledge about items (e.g., movies) so that they can generate descriptions.
- Items have more than two aspects (e.g., sci-fi and romance) so that choosing a prompt makes the difference in expected rewards.
- The above \textit{prompt effects} can be different among individual contexts (i.e., users).
- The prompt effects are learnable from datasets, so we can learn a realistic simulator (e.g., MovieLens enables us to learn affinity between user preference and movie features).

We hope this work will be an important cornerstone to the real-world applications of OPL for prompt-guided language personalization.

<details>
<summary><strong>Click here to show the implementation details</strong></summary>

<div align="center"><img src="https://raw.githubusercontent.com/aiueola/offline-prompts/main/images/workflow.png" width="100%"/></div>
<figcaption>
<p align="center">
  Off-policy learning (OPL) workflow and OfflinePrompts modules.
</p>
</figcaption>

OfflinePrompts streamlines the implementation with three modules: dataset, OPL, and policy, as shown in the above figure. All implementations are based on [PyTorch](https://pytorch.org/get-started/locally/). Roughly, the dataset module provides the benchmarks, the policy module provides the base implementations of the prompt-policy, and the OPL module provide learner classes for training the prompt-policy using logged dataset.


Both *synthetic* and *full-LLM* benchmarks provide a standardized setting and configurable submodules to control the data generation process. 
Specifically, as illustrated in the following figure, each benchmark consists of four submodules: 

<div align="center"><img src="https://raw.githubusercontent.com/aiueola/offline-prompts/main/images/dataset_module.png" width="100%"/></div>
<figcaption>
<p align="center">
  Two benchmarks (*synthetic* and *full-LLM*) with four configurable submodules.
</p>
</figcaption>

- `ContextQueryModule`: Loading the original dataset and user and item features.
- `CandidateActionsModule`: Loading and handling candidate prompts.
- `AuxiliaryOutputGenerator`/`FrozenLLM`: Handling sentence generation process given prompts as inputs.
- `RewardSimulator`: Simuating user responses given sentence generated by frozen LLMs.

Compared to the existing OPE/OPL benchmarks such as [obp](https://github.com/st-tech/zr-obp) and [scope-rl](https://github.com/hakuhodo-technologies/scope-rl), our benchmark is distinctive in modeling `AuxiliaryOutputGenerator`/`FrozenLLM`, which enable us to simulate language generation tasks as contextual bandits with auxiliary outputs. 
Moreover, since our `FrozenLLM` and `RewardSimulator` modules are compatible with [Huggingface](https://huggingface.co/models), users can easily employ various language models in the full-LLM experiments. The full-LLM (semi-synthetic) dataset module can also load custom dataset in a manner similar to the movie description benchmark. Please also refer to [this page](./src/dataset/assets/README.md) for the detailed procedure for using a custom dataset or environment. 

</details>

**Further details are available in the [preprint]().**


## Installation

You can install OfflinePrompts using Python's package manager `pip`.
```
pip install offline-prompts
```

You can also install OfflinePrompts from the source.
```bash
git clone https://github.com/aiueola/offline-prompts
cd offline-prompts
python setup.py install
```

OfflinePrompts supports Python 3.9 or newer. See [requirements.txt](./requirements.txt) for other requirements. 

## Usage

Please refer to the [quickstart examples in the doc]().

## Citation

If you use this simulator in your project or find this resource useful, please cite the following papers.

(benchmarking experiment)

Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims.<br>
**An Off-Policy Learning Approach for Steering Sentence Generation towards Personalization**<br>
[link](https://arxiv.org/abs/2504.02646)

```
@inproceedings{kiyohara2025off,
  title = {An Off-Policy Learning Approach for Steering Sentence Generation towards Personalization},
  author = {Kiyohara, Haruka and Cao, Daniel Yiming and Saito, Yuta and Joachims, Thorsten},
  booktitle = {Proceedings of the 19th ACM Conference on Recommender Systems},
  pages = {41--50},
  year = {2025},
}
```

(package and documentation)

Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims.<br>
**OfflinePrompts: Benchmark Suites for Prompt-guided Text Personalization from Logged Data**<br>
[link]()

```
@article{kiyohara2025offline,
  title = {OfflinePrompts: Benchmark Suites for Prompt-guided Text Personalization from Logged Data},
  author = {Kiyohara, Haruka and Cao, Daniel Yiming and Saito, Yuta and Joachims, Thorsten},
  journal = {arXiv preprint arXiv:},
  year = {2025},
}
```

## Project Team

- [Haruka Kiyohara](https://sites.google.com/view/harukakiyohara) (**Main Contributor**; Cornell University)
- Daniel Yiming Cao (Cornell University)
- [Yuta Saito](https://usait0.com/en/) (Cornell University)
- [Thorsten Joachims](https://www.cs.cornell.edu/people/tj/) (Cornell University) 

This research was supported in part by NSF Awards IIS-2312865 and OAC-2311521. Haruka Kiyohara and Yuta Saito are supported by [Funai Overseas Scholarship](https://funaifoundation.jp/scholarship/en/scholarship_guidelines_phd.html).

## Licence

This project is licensed under Apache 2.0 license - see [LICENSE](LICENSE) file for details.

## Contact

If you have any questions, please contact hk844 [at] cornell.edu.

## Reference

<details>
<summary><strong>Implemented papers </strong>(click to expand)</summary>

- Ronald J. Whilliams. "Simple Statistical Gradient-following Algorithms for Connectionist Reinforcement Learning." 1992.

- Vijay Konda and John Tsitsiklis. "Actor-critic algorithms." 1999.

- Alina Beygelzimer and John Langford. "The offset tree for learning with partial labels." 2009.

- Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. "Learning from logged implicit exploration data." 2010.

- Miroslav Dudík, John Langford, and Lihong Li. "Doubly robust policy evaluation and learning." 2011.

- Yuta Saito, Qingyang Ren, and Thorsten Joachims. "Off-policy evaluation for large action spaces via conjunct effect modeling." 2023.

- Adith Swaminathan and Thorsten Joachims. "Batch learning from logged bandit feedback through counterfactual risk minimization." 2015.

- Miroslav Dudík, John Langford, and Lihong Li. "Doubly robust policy evaluation and learning." 2011.

- Yuta Saito, Jihan Yao, and Thorsten Joachims. "POTEC: Off-policy learning for large action spaces via two-stage policy decomposition." 2024.

- Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, and Thorsten Joachims. "An Off-Policy Learning Approach for Steering Sentence Generation towards Personalization". 2025.

</details>

<details>
<summary><strong>Other reference papers </strong>(click to expand)</summary>

- Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. "RLPrompt: Optimizing discrete text prompts with reinforcement learning." 2022.

- Yuta Saito and Thorsten Joachims. "Off-policy evaluation for large action spaces via embedding." 2022.

- Noveen Sachdeva, Yi Su, and Thorsten Joachims. "Off-Policy Bandits with Deficient Support." 2020.

- Maxwell F. Harper and Joseph A. Konstan. "The MovieLens Datasets: History and Context." 2015.

- Yuta Saito, Shunsuke Aihara, Megumi Matsutani, and Yusuke Narita. "Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation." 2021.

- Haruka Kiyohara, Ren Kishimoto, Kosuke Kawakami, Ken Kobayashi, Kazuhide Nakata, Yuta Saito. "SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation." 2023.

- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. "Huggingface's Transformers: State-of-the-art Natural Language Processing." 2019.

- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. "Language Models are Few-Shot Learners." 2020.

- Albert Q. Jiang, Alexandre Sablayrolles, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, William El Sayed. "Mistral-7B." 2023.

</details>

<details>
<summary><strong>Relevant projects </strong>(click to expand)</summary>

- **Open Bandit Pipeline**  -- a pipeline implementation of OPE in contextual bandits: [[github](https://github.com/st-tech/zr-obp)] [[documentation](https://zr-obp.readthedocs.io/en/latest/)] [[paper](https://arxiv.org/abs/2008.07146)]
- **scope-rl** -- a pipeline implementation of OPE in reinforcement learning: [[github](https://github.com/hakuhodo-technologies/scope-rl)] [[documentation](https://scope-rl.readthedocs.io/en/latest/)] [[paper](https://arxiv.org/abs/2311.18206)]
- **MovieLens** -- a dataset that collects large-scale user-item rating data in movie recommendation: [[documentation](https://grouplens.org/datasets/movielens/)] [[paper](https://dl.acm.org/doi/10.1145/2827872)]

</details>

