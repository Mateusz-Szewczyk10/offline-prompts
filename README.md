# OfflinePrompts: Benchmark Suites for Prompt-guided Language Personalization (Python)

<details>
<summary><strong>Table of Contents </strong>(click to expand)</summary>

- [OfflinePrompts: Benchmark Suites for Prompt-guided Language Personalization (Python)](#offlineprompts-benchmark-suites-for-prompt-guided-language-personalization-python)
- [Overview](#overview)
- [Installation](#installation)
- [Module](#module)
- [Usage](#usage)
  - [xxx]()
- [Citation](#citation)
- [Project Team](#project-team)
- [License](#license)
- [Contact](#contact)
- [Reference](#reference)

</details>

**Pretrained simulator and datasets are available [here]().**

**Documentation is available [here]().**

**Stable versions are available at [PyPI]().**

**Slides are available [here]().**

**日本語は[こちら](README_ja.md)。**

## Overview

Personalizing sentence generation is crucial in interactive platforms including recommender systems, online ads, and educational apps. For example, consider a situation where we recommend the movie "Wall-E (2008)" with some short descriptions or slogans of the movie, as shown in Figure xxx. If the user is a sci-fi lover, the description focusing on the "sci-fi" aspect of the movie would gain more attention than that focusing on the "romance" aspect, and vice versa. Such personalization is crucial for maximizing some business metrics and users' welfare.

<div align="center"><img src="https://raw.githubusercontent.com/aiueola/offline-prompts/main/images/motivative_example.png" width="100%"/></div>
<figcaption>
<p align="center">
  Motivative example of for generating personalized sentence in movie recommendation.
</p>
</figcaption>

This package, called **OfflinePrompt**, aims to provide tools for the above personalization focusing on the prompt-policy learning from logged bandit data. Specifically, we consider the following workflow -- (i) For each coming user, a policy chooses which prompt to use to generate sentences with a frozen LLM. (ii) Each user observes only the sentence generated by the chosen prompt and provides the reward for the corresponding sentence. -- Figure xxx illustrates the interaction process. Through daily operations in the platform, a (logging) policy naturally collects user responses to the sentence generated by the logging policy. Our goal is to use such informative logged data to learn or evaluate a new policy **(Off-policy evaluation and learning; OPE/OPL)** for better personalization in the future. 

<div align="center"><img src="https://raw.githubusercontent.com/aiueola/offline-prompts/main/images/personalized_sentence_generation.png" width="100%"/></div>
<figcaption>
<p align="center">
  Prompt-based sentence personalization as a off-policy learning of contextual bandits. 
</p>
</figcaption>

To facilitate the research and practical applications of OPL of prompt policies, OfflinePrompts contributes to the followings; 

- To implement representative OPL methods for prompt optimization. 
- To provide two benchmark environments, including synthetic and full-LLM.
- To streamline the workflow to connect OPL and sentence generation modules for smooth experimentation. 

In particular, the provided full-LLM benchmark models realistic users' responses to the presented sentence in the movie recommendation settings, by training a semi-synthetic reward simulator with the sentence-augmented [MovieLens](https://grouplens.org/datasets/movielens/) dataset. The benchmark design is a remarkable contribution to the research community, as we currently lack a realistic simulator on personalized sentence generation tasks that satisfies the following four key qualifications:

- LLMs have knowledge about items (e.g., movies) so that they can generate descriptions.
- Items have more than two aspects (e.g., sci-fi and romance) so that choosing a prompt makes the difference in expected rewards.
- The above \textit{prompt effects} can be different among individual contexts (i.e., users).
- The prompt effects are learnable from datasets, so we can learn a realistic simulator (e.g., MovieLens enables us to learn affinity between user preference and movie features).

We hope this work will be an important cornerstone to the real-world applications of OPL for prompt-guided language personalization.

<details>
<summary><strong>Click here to show the implementation details</strong></summary>

<div align="center"><img src="https://raw.githubusercontent.com/aiueola/offline-prompts/main/images/workflow.png" width="100%"/></div>
<figcaption>
<p align="center">
  Off-policy learning (OPL) workflow and OfflinePrompts modules.
</p>
</figcaption>

OfflinePrompts streamlines the implementation with three modules: dataset, OPL, and policy, as shown in Figure xxx. All implementations are based on [PyTorch](https://pytorch.org/get-started/locally/). We elaborate on the details of each feature below.

<div align="center"><img src="https://raw.githubusercontent.com/aiueola/offline-prompts/main/images/dataset_module.png" width="100%"/></div>
<figcaption>
<p align="center">
  Two benchmarks (*synthetic* and *full-LLM*) with four configurable submodules.
</p>
</figcaption>

Both *synthetic* and *full-LLM* benchmarks provide a standardized setting and configurable submodules to control the data generation process. 
Specifically, as illustrated in Figure xxx, each benchmark consists of four submodules: 

- `ContextQueryModule`: Loading the original dataset and user and item features.
- `CandidateActionsModule`: Loading and handling candidate prompts.
- `AuxiliaryOutputGenerator`/`FrozenLLM`: Handling sentence generation process given prompts as inputs.
- `RewardSimulator`: Simuating user responses given sentence generated by frozen LLMs.

Compared to the existing OPE/OPL benchmarks such as [obp](https://github.com/st-tech/zr-obp) and [scope-rl](https://github.com/hakuhodo-technologies/scope-rl), our benchmark is distinctive in modeling `AuxiliaryOutputGenerator`/`FrozenLLM`, which enable us to simulate language generation tasks as contextual bandits with auxiliary outputs. 
Moreover, since our `FrozenLLM` and `RewardSimulator` modules are compatible with [Huggingface](https://huggingface.co/models), users can easily employ various language models in the full-LLM experiments. The full-LLM (semi-synthetic) dataset module can also load custom dataset in a manner similar to the movie description benchmark. Please also refer to [this page](./src/dataset/assets/README.md) for the detailed procedure for using a custom dataset or environment. 

</details>

**Further details are available in the [preprint]().**


## Installation

You can install OfflinePrompts using Python's package manager `pip`.
```
pip install offline-prompts
```

You can also install SCOPE-RL from the source.
```bash
git clone https://github.com/aiueola/offline-prompts
cd offline-prompts
python setup.py install
```

OfflinePrompts supports Python 3.9 or newer. See [requirements.txt](./requirements.txt) for other requirements. 

## Usage

## Citation

If you use this simulator in your project or find this resource useful, please cite the following papers.

(benchmarking experiment)

Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims.<br>
**Off-Policy Learning for Prompt-guided Sentence Personalization Using Logged Bandit Data**<br>
[link]()

```
@article{kiyohara2025off,
  title = {Off-Policy Learning for Prompt-guided Sentence Personalization Using Logged Bandit Data},
  author = {Kiyohara, Haruka and Cao, Daniel Yiming and Saito, Yuta and Joachims, Thorsten},
  journal = {xxx},
  pages = {xxx--xxx},
  year = {2025},
}
```

(package and documentation)

Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims.<br>
**OfflinePrompts: Benchmark Suites for Prompt-guided Language Personalization**<br>
[link]()

```
@article{kiyohara2025offline,
  title = {OfflinePrompts: Benchmark Suites for Prompt-guided Language Personalization},
  author = {Kiyohara, Haruka and Cao, Daniel Yiming and Saito, Yuta and Joachims, Thorsten},
  journal = {xxx},
  pages = {xxx--xxx},
  year = {2025},
}
```

## Project Team

- [Haruka Kiyohara](https://sites.google.com/view/harukakiyohara) (**Main Contributor**; Cornell University)
- Daniel Yiming Cao (Cornell University)
- [Yuta Saito](https://usait0.com/en/) (Cornell University)
- [Thorsten Joachims](https://www.cs.cornell.edu/people/tj/) (Cornell University) 

This research was supported in part by NSF Awards IIS-2312865 and OAC-2311521. Haruka Kiyohara and Yuta Saito are supported by [Funai Overseas Scholarship](https://funaifoundation.jp/scholarship/en/scholarship_guidelines_phd.html).

## Licence

This project is licensed under Apache 2.0 license - see [LICENSE](LICENSE) file for details.

## Contact

If you have any questions, please contact hk844 [at] cornell.edu.

## Reference

<details>
<summary><strong>Implemented papers </strong>(click to expand)</summary>

- Richard S Sutton and Andrew G Barto. "Reinforcement learning: An introduction." 2018.

- Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. "RLPrompt: Optimizing discrete text prompts with reinforcement learning." 2022.

- Vijay Konda and John Tsitsiklis. "Actor-critic algorithms." 1999.

- Alina Beygelzimer and John Langford. "The offset tree for learning with partial labels." 2009.

- Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. "Learning from logged implicit exploration data." 2010.

- Miroslav Dudík, John Langford, and Lihong Li. "Doubly robust policy evaluation and learning." 2011.

- Yuta Saito, Qingyang Ren, and Thorsten Joachims. "Off-policy evaluation for large action spaces via conjunct effect modeling." 2023.

- Adith Swaminathan and Thorsten Joachims. "Batch learning from logged bandit feedback through counterfactual risk minimization." 2015.

- Miroslav Dudík, John Langford, and Lihong Li. "Doubly robust policy evaluation and learning." 2011.

- Yuta Saito, Jihan Yao, and Thorsten Joachims. "POTEC: Off-policy learning for large action spaces via two-stage policy decomposition." 2024.

- Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, and Thorsten Joachims. "Off-policy learning for prompt-guided text personalization using logged bandit data". 2025.

</details>

<details>
<summary><strong>Relevant projects </strong>(click to expand)</summary>

- **Open Bandit Pipeline**  -- a pipeline implementation of OPE in contextual bandits: [[github](https://github.com/st-tech/zr-obp)] [[documentation](https://zr-obp.readthedocs.io/en/latest/)] [[paper](https://arxiv.org/abs/2008.07146)]
- **scope-rl** -- a pipeline implementation of OPE in reinforcement learning: [[github](https://github.com/hakuhodo-technologies/scope-rl)] [[documentation](https://scope-rl.readthedocs.io/en/latest/)] [[paper](https://arxiv.org/abs/2311.18206)]
- **MovieLens** -- a dataset that collects large-scale user-item rating data in movie recommendation: [[documentation](https://grouplens.org/datasets/movielens/)] [[paper](https://dl.acm.org/doi/10.1145/2827872)]

</details>

