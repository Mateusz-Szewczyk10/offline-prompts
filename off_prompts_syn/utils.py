"""Useful tools."""
import math
from collections import defaultdict
from typing import DefaultDict, Dict, Union, Optional, Any

import torch

from .types import LoggedDataset


def gaussian_kernel(distance: torch.Tensor, tau: float = 1.0):
    """Gaussian kernel."""
    if tau <= 0:
        raise ValueError("tau should be a positive value, but found False.")

    return torch.exp(-(distance**2) / (2 * tau**2)) / math.sqrt(
        2 * math.pi * tau**2
    )


def uniform_kernel(distance: torch.Tensor, tau: float = 1.0):
    """Uniform kernel."""
    if tau <= 0:
        raise ValueError("tau should be a positive value, but found False.")

    return (distance <= tau) / (2 * tau)


def torch_seed(random_state: int, device=str):
    """Set seeds of pytorch."""
    if device == "cuda:0":
        torch.cuda.manual_seed(random_state)

    torch.manual_seed(random_state)


def defaultdict_to_dict(dict_: Union[Dict[Any, Any], DefaultDict[Any, Any]]):
    """Transform a defaultdict into a corresponding dict."""
    if isinstance(dict_, defaultdict):
        dict_ = {key: defaultdict_to_dict(value) for key, value in dict_.items()}
    return dict_


def check_tensor(
    tensor: torch.Tensor,
    name: str,
    expected_dim: int = 1,
    expected_dtype: Optional[type] = None,
    min_val: Optional[float] = None,
    max_val: Optional[float] = None,
) -> ValueError:
    """Input validation on tensor.

    Parameters
    -------
    tensor: object
        Input tensor to check.

    name: str
        Name of the input tensor.

    expected_dim: int, default=1
        Expected dimension of the input tensor.

    expected_dtype: {type, tuple of type}, default=None
        Expected dtype of the input tensor.

    min_val: float, default=None
        Minimum value allowed in the input tensor.

    max_val: float, default=None
        Maximum value allowed in the input tensor.

    """
    if not isinstance(tensor, torch.Tensor):
        raise ValueError(
            f"{name} must be {expected_dim}D tensor, but got {type(tensor)}"
        )
    if tensor.ndim != expected_dim:
        raise ValueError(
            f"{name} must be {expected_dim}D tensor, but got {tensor.ndim}D tensor"
        )
    if expected_dtype is not None:
        if tensor.dtype != expected_dtype:
            raise ValueError(
                f"The elements of {name} must be {expected_dtype}, but got {tensor.dtype}"
            )
    if min_val is not None:
        if tensor.min() < min_val:
            raise ValueError(
                f"The elements of {name} must be larger than {min_val}, but got minimum value {tensor.min()}"
            )
    if max_val is not None:
        if tensor.max() > max_val:
            raise ValueError(
                f"The elements of {name} must be smaller than {max_val}, but got maximum value {tensor.max()}"
            )


def check_logged_feedback(logged_feedback: LoggedDataset):
    """Check keys of logged feedback.

    Parameters
    -------
    logged_feedback: LoggedDataset
        Logged data, which contains the following keys.

        .. code-block:: python

                key: [
                    context,
                    query,
                    action,
                    sentence,
                    expected_reward,
                    reward,
                    logging_policy,
                ]

            context: torch.Tensor, shape (n_samples, dim_context)
                Feature vector of each user.

            query: Sentence, shape (n_samples, )
                Propmpts or embeddings of the prompts specified by users.

            action: torch.Tensor, shape (n_samples, ) or (n_samples, dim_action)
                Discrete or continuous (soft) prompts chosen by the given policy.

            sentence: Sentence, shape (n_samples, )
                Sentence generated by some frozen LLMs using user-specified query and prompts chosen by a policy.

            expected_reward: torch.Tensor, shape (n_samples, )
                Expected reward predicted by some frozen LLMs (i.e., reward simulator).

            reward: torch.Tensor, shape (n_samples, )
                Either binary or continuous reward.

            logging_policy: BasePolicy
                Policy that chooses either discrete or continuous (soft) prompt as action.

    """
    for key in ["context", "query", "action", "sentence", "reward"]:
        if key not in logged_feedback.keys():
            raise ValueError(
                f"logged_logged_feedback must contrain {key}, but {key} is not found"
            )

    context = logged_feedback["context"]
    query = logged_feedback["query"]
    action = logged_feedback["action"]
    sentence = logged_feedback["sentence"]
    reward = logged_feedback["reward"]

    if not (len(context) == len(query) == len(action) == len(sentence) == len(reward)):
        raise ValueError(
            "content, query, action, sentence, reward must have the same data size, but found False."
        )
